'''
Author: zdytim zdytim@foxmail.com
Date: 2026-01-05 22:33:38
LastEditors: zdytim zdytim@foxmail.com
LastEditTime: 2026-01-05 22:33:39
FilePath: /NavRL/isaac-training/training/scripts/train_ppo_with_wandb.py
Description: è¿™æ˜¯é»˜è®¤è®¾ç½®,è¯·è®¾ç½®`customMade`, æ‰“å¼€koroFileHeaderæŸ¥çœ‹é…ç½® è¿›è¡Œè®¾ç½®: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE
'''
"""
NavRL è®­ç»ƒè„šæœ¬ - é›†æˆ wandb æ¨¡å‹ç®¡ç†
====================================

åœ¨åŸæœ‰çš„ train_ppo.py åŸºç¡€ä¸Šé›†æˆ wandb æ¨¡å‹ä¸Šä¼ åŠŸèƒ½ã€‚
ä¸»è¦å¢åŠ ï¼š
1. è‡ªåŠ¨æ¨¡å‹ç‰ˆæœ¬ç®¡ç†
2. åŸºäºæ€§èƒ½çš„æœ€ä½³æ¨¡å‹ä¿å­˜
3. æ¨¡å‹å…ƒæ•°æ®è·Ÿè¸ª
4. ä¾¿æ·çš„æ¨¡å‹æ¢å¤åŠŸèƒ½
"""

import hydra
from omegaconf import DictConfig, OmegaConf
import torch
import os
import datetime
import wandb

# å¯¼å…¥åŸæœ‰æ¨¡å—
from env import NavigationEnv
from ppo_vit_v3 import PPOVIT  # æˆ–è€…ä½¿ç”¨æ–°çš„æ¨¡å‹ç®¡ç†å™¨
from utils import make_eval_env

# å¯¼å…¥ wandb æ¨¡å‹å·¥å…·
from wandb_model_utils import (
    upload_model_to_wandb, 
    save_and_upload_best_model,
    download_model_from_wandb,
    log_model_metrics
)


@hydra.main(version_base=None, config_path="../cfg", config_name="train_ppo_vit")
def main(cfg: DictConfig):
    """
    ä¸»è®­ç»ƒå‡½æ•° - é›†æˆ wandb æ¨¡å‹ç®¡ç†
    """
    print("ğŸš€ NavRL Training with wandb Model Management")
    print("=" * 60)
    
    # === 1. ç¯å¢ƒå’Œè®¾å¤‡è®¾ç½® ===
    device = torch.device(cfg.device if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”§ Using device: {device}")
    
    # === 2. wandb åˆå§‹åŒ– ===
    wandb_config = OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True)
    
    if cfg.wandb.run_id is None:
        run = wandb.init(
            project=cfg.wandb.project,
            name=cfg.wandb.name + "/" + datetime.datetime.now().strftime('%m-%d_%H-%M'),
            config=wandb_config,
            mode=cfg.wandb.mode,
            id=wandb.util.generate_id(),
            tags=['navrl', 'ppo-vit', 'model-management']  # æ·»åŠ æ ‡ç­¾
        )
    else:
        run = wandb.init(
            project=cfg.wandb.project,
            name=cfg.wandb.name + "/" + datetime.datetime.now().strftime('%m-%d_%H-%M'),
            config=wandb_config,
            mode=cfg.wandb.mode,
            id=cfg.wandb.run_id,
            resume="must"
        )
    
    print(f"ğŸ”— wandb run: {run.name}")
    
    # === 3. ç¯å¢ƒåˆ›å»º ===
    print("ğŸ—ï¸  Creating environments...")
    env = NavigationEnv(cfg)
    eval_env = make_eval_env(cfg)
    
    # === 4. æ¨¡å‹åˆ›å»º ===
    print("ğŸ§  Creating model...")
    observation_spec = env.observation_spec
    action_spec = env.action_spec
    
    policy = PPOVIT(cfg, observation_spec, action_spec, device)
    
    # è®°å½•æ¨¡å‹æ¶æ„ä¿¡æ¯åˆ° wandb
    model_info = {
        'total_parameters': sum(p.numel() for p in policy.parameters()),
        'trainable_parameters': sum(p.numel() for p in policy.parameters() if p.requires_grad),
        'architecture': 'PPO-ViT',
        'action_dim': policy.action_dim
    }
    
    wandb.config.update({'model_info': model_info})
    print(f"ğŸ“Š Model parameters: {model_info['total_parameters']:,}")
    
    # === 5. æ¨¡å‹æ¢å¤ï¼ˆå¯é€‰ï¼‰===
    resume_checkpoint = getattr(cfg, 'resume_checkpoint', None)
    if resume_checkpoint:
        if ":" in resume_checkpoint and "/" in resume_checkpoint:
            # ä» wandb artifact æ¢å¤
            print(f"ğŸ“¥ Resuming from wandb artifact: {resume_checkpoint}")
            model_path = download_model_from_wandb(resume_checkpoint)
            if model_path:
                policy.load_state_dict(torch.load(model_path, map_location=device))
                print("âœ… Model restored from wandb")
        else:
            # ä»æœ¬åœ°æ–‡ä»¶æ¢å¤
            print(f"ğŸ“¥ Resuming from local checkpoint: {resume_checkpoint}")
            if os.path.exists(resume_checkpoint):
                policy.load_state_dict(torch.load(resume_checkpoint, map_location=device))
                print("âœ… Model restored from local file")
    
    # === 6. æœ€ä½³æ¨¡å‹è·Ÿè¸ªå™¨ ===
    best_model_tracker = {
        'best_value': float('-inf'),
        'best_step': 0,
        'threshold_metric': 'mean_reward'
    }
    
    # === 7. ä¸»è®­ç»ƒå¾ªç¯ ===
    print("ğŸƒ Starting training loop...")
    
    collector = make_collector(env, policy, cfg, device)  # å‡è®¾è¿™ä¸ªå‡½æ•°å­˜åœ¨
    
    for i, data in enumerate(collector):
        current_frames = data.numel()
        
        print(f"\\nStep {i+1} - Frames: {current_frames}")
        
        # è®­ç»ƒæ›´æ–°
        with torch.cuda.amp.autocast(enabled=getattr(cfg, 'use_amp', True)):
            train_info = policy.train(data)
        
        # åŸºç¡€æŒ‡æ ‡è®°å½•
        info = {f'train/{k}': v for k, v in train_info.items()}
        
        # === å®šæœŸè¯„ä¼° ===
        if i % cfg.eval_interval == 0:
            print(f"ğŸ” Evaluating at step {i}...")
            
            eval_info = evaluate_model(eval_env, policy, cfg)
            info.update({f'eval/{k}': v for k, v in eval_info.items()})
            
            # è®°å½•æ¨¡å‹ç›¸å…³æŒ‡æ ‡
            log_model_metrics(eval_info, step=i)
            
            # æ£€æŸ¥å¹¶ä¿å­˜æœ€ä½³æ¨¡å‹
            is_new_best = save_and_upload_best_model(
                model_state_dict=policy.state_dict(),
                step=i,
                eval_metrics=eval_info,
                threshold_metric=best_model_tracker['threshold_metric'],
                best_value_tracker=best_model_tracker
            )
            
            if is_new_best:
                print(f"ğŸ† New best model saved! {best_model_tracker['threshold_metric']}: {best_model_tracker['best_value']:.4f}")
            
            print(f"ğŸ“Š Evaluation completed.")
        
        # è®°å½•æ‰€æœ‰æŒ‡æ ‡åˆ° wandb
        run.log(info, step=i)
        
        # === å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ ===
        if i % cfg.save_interval == 0:
            # æœ¬åœ°ä¿å­˜
            ckpt_path = os.path.join(run.dir, f"checkpoint_{i}.pt")
            torch.save(policy.state_dict(), ckpt_path)
            print(f"ğŸ’¾ Checkpoint saved locally: checkpoint_{i}.pt")
            
            # ä¸Šä¼ åˆ° wandbï¼ˆæ¯5æ¬¡ä¿å­˜ä¸Šä¼ ä¸€æ¬¡ï¼Œé¿å…è¿‡äºé¢‘ç¹ï¼‰
            if i % (cfg.save_interval * 5) == 0:
                upload_model_to_wandb(
                    model_state_dict=policy.state_dict(),
                    step=i,
                    eval_metrics=info.get('eval', {}),
                    model_alias="latest",
                    model_type="checkpoint"
                )
        
        # === è®­ç»ƒæ‘˜è¦æ‰“å° ===
        if i % 100 == 0:
            print(f"\\n[Step {i}] Training Summary:")
            for k, v in info.items():
                if isinstance(v, (float, int)):
                    print(f"  {k:<20}: {v:.4f}")
            print("-" * 40)
        
        # æ£€æŸ¥è®­ç»ƒç»“æŸæ¡ä»¶
        if i >= cfg.total_training_steps:
            break
    
    # === 8. è®­ç»ƒç»“æŸå¤„ç† ===
    print("ğŸ¯ Training completed! Saving final model...")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_ckpt_path = os.path.join(run.dir, "checkpoint_final.pt")
    torch.save(policy.state_dict(), final_ckpt_path)
    
    # ä¸Šä¼ æœ€ç»ˆæ¨¡å‹åˆ° wandbï¼Œå¸¦æœ‰ç‰¹æ®Šæ ‡è®°
    final_eval_metrics = evaluate_model(eval_env, policy, cfg)
    upload_model_to_wandb(
        model_state_dict=policy.state_dict(),
        step=i,
        eval_metrics=final_eval_metrics,
        model_alias="final",
        model_type="final"
    )
    
    # åˆ›å»ºè®­ç»ƒæ‘˜è¦
    training_summary = {
        'total_steps': i,
        'best_model_step': best_model_tracker['best_step'],
        'best_model_reward': best_model_tracker['best_value'],
        'final_eval_reward': final_eval_metrics.get('mean_reward', 0),
        'total_parameters': model_info['total_parameters']
    }
    
    # è®°å½•è®­ç»ƒæ‘˜è¦
    wandb.log({"training_summary": training_summary})
    
    print("ğŸ‰ Training session completed successfully!")
    print(f"   ğŸ“Š Best model: step {best_model_tracker['best_step']}, reward {best_model_tracker['best_value']:.2f}")
    print(f"   ğŸ“ˆ Final reward: {final_eval_metrics.get('mean_reward', 0):.2f}")
    
    # ç»“æŸ wandb è¿è¡Œ
    wandb.finish()
    
    # å…³é—­ç¯å¢ƒ
    if hasattr(env, 'close'):
        env.close()
    if hasattr(eval_env, 'close'):
        eval_env.close()


def evaluate_model(eval_env, policy, cfg, num_episodes=10):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    
    Args:
        eval_env: è¯„ä¼°ç¯å¢ƒ
        policy: ç­–ç•¥æ¨¡å‹
        cfg: é…ç½®
        num_episodes: è¯„ä¼°å›åˆæ•°
        
    Returns:
        è¯„ä¼°æŒ‡æ ‡å­—å…¸
    """
    print(f"   ğŸ” Running {num_episodes} evaluation episodes...")
    
    policy.eval()
    rewards = []
    episode_lengths = []
    success_count = 0
    
    for episode in range(num_episodes):
        td = eval_env.reset()
        episode_reward = 0
        episode_length = 0
        
        with torch.no_grad():
            for step in range(cfg.max_episode_length):
                td = policy(td)
                td = eval_env.step(td)
                
                reward = td["next", "agents", "reward"].sum().item()
                episode_reward += reward
                episode_length += 1
                
                # æ£€æŸ¥æˆåŠŸæ¡ä»¶ï¼ˆæ ¹æ®ä½ çš„ä»»åŠ¡å®šä¹‰ï¼‰
                if reward > cfg.success_reward_threshold:  # å‡è®¾é…ç½®ä¸­æœ‰è¿™ä¸ªé˜ˆå€¼
                    success_count += 1
                
                if td["next", "terminated"].any():
                    break
        
        rewards.append(episode_reward)
        episode_lengths.append(episode_length)
    
    policy.train()
    
    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    eval_metrics = {
        'mean_reward': sum(rewards) / len(rewards),
        'std_reward': torch.tensor(rewards).std().item(),
        'min_reward': min(rewards),
        'max_reward': max(rewards),
        'mean_episode_length': sum(episode_lengths) / len(episode_lengths),
        'success_rate': success_count / num_episodes
    }
    
    return eval_metrics


def make_collector(env, policy, cfg, device):
    """
    åˆ›å»ºæ•°æ®æ”¶é›†å™¨
    è¿™é‡Œéœ€è¦æ ¹æ®ä½ çš„å…·ä½“å®ç°æ¥è°ƒæ•´
    """
    # è¿™æ˜¯ä¸€ä¸ªå ä½å‡½æ•°ï¼Œéœ€è¦æ ¹æ®ä½ çš„å®é™…æ”¶é›†å™¨å®ç°æ¥å¡«å†™
    from torchrl.collectors import SyncDataCollector
    
    collector = SyncDataCollector(
        env,
        policy,
        frames_per_batch=cfg.frames_per_batch,
        total_frames=cfg.total_frames,
        device=device,
        storing_device=device,
    )
    
    return collector


# === é…ç½®ç¤ºä¾‹ ===
"""
ä½¿ç”¨ç¤ºä¾‹é…ç½® (cfg/train_ppo_vit.yaml):

```yaml
# åŸºç¡€é…ç½®
device: "cuda:0"
total_training_steps: 10000
eval_interval: 100
save_interval: 200

# è¯„ä¼°é…ç½®
max_episode_length: 1000
success_reward_threshold: 50.0

# wandb é…ç½®
wandb:
  project: "navrl-models"
  name: "ppo-vit-experiment"
  mode: "online"

# æ¨¡å‹æ¢å¤ (å¯é€‰)
# resume_checkpoint: "path/to/checkpoint.pt"  # æœ¬åœ°æ–‡ä»¶
# resume_checkpoint: "username/project/model:best"  # wandb artifact

# å…¶ä»–åŸæœ‰é…ç½®...
```

è¿è¡Œå‘½ä»¤ï¼š
```bash
# åŸºç¡€è®­ç»ƒ
python train_ppo_with_wandb.py

# ä» wandb artifact æ¢å¤
python train_ppo_with_wandb.py resume_checkpoint="user/project/model:best"

# ç¦»çº¿æ¨¡å¼
python train_ppo_with_wandb.py wandb.mode=offline
```
"""


if __name__ == "__main__":
    main()