'''
Author: zdytim zdytim@foxmail.com
Date: 2026-01-05 22:31:48
LastEditors: zdytim zdytim@foxmail.com
LastEditTime: 2026-01-05 22:31:50
FilePath: /NavRL/isaac-training/training/scripts/train_with_wandb.py
Description: è¿™æ˜¯é»˜è®¤è®¾ç½®,è¯·è®¾ç½®`customMade`, æ‰“å¼€koroFileHeaderæŸ¥çœ‹é…ç½® è¿›è¡Œè®¾ç½®: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE
'''
"""
NavRL + wandb é›†æˆç¤ºä¾‹
======================

å±•ç¤ºå¦‚ä½•åœ¨ NavRL è®­ç»ƒä¸­é›†æˆ wandb çš„æ¨¡å‹ç®¡ç†åŠŸèƒ½ã€‚

åŠŸèƒ½ç‰¹æ€§ï¼š
1. è‡ªåŠ¨æ¨¡å‹ç‰ˆæœ¬ç®¡ç†
2. è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¨¡å‹ä¸Šä¼ 
3. æ¨¡å‹æ³¨å†Œè¡¨ç®¡ç†
4. ä¸°å¯Œçš„æ¨¡å‹å…ƒæ•°æ®
"""

import torch
import wandb
import hydra
from omegaconf import DictConfig
from tensordict import TensorDict
from torchrl.envs import ParallelEnv

# å¯¼å…¥æ¨¡å‹ç®¡ç†å™¨
from models import create_navrl_model, load_pretrained_model

# å¯¼å…¥å…¶ä»–å¿…è¦æ¨¡å—  
from env import NavigationEnv
from utils import make_batch


class WandbNavRLTrainer:
    """
    é›†æˆ wandb çš„ NavRL è®­ç»ƒå™¨
    
    ç‰¹è‰²åŠŸèƒ½ï¼š
    1. è‡ªåŠ¨æ¨¡å‹ç‰ˆæœ¬ç®¡ç†
    2. åŸºäºæ€§èƒ½çš„æ¨¡å‹ä¸Šä¼ 
    3. è®­ç»ƒæŒ‡æ ‡å’Œæ¨¡å‹åŒæ­¥è·Ÿè¸ª
    4. æ¨¡å‹æ³¨å†Œè¡¨é›†æˆ
    """
    
    def __init__(self, cfg: DictConfig):
        self.cfg = cfg
        self.device = torch.device(cfg.device)
        
        # åˆå§‹åŒ– wandb
        self._init_wandb()
        
        # åˆ›å»ºç¯å¢ƒå’Œæ¨¡å‹
        self.env = self._create_environment()
        self.model_manager = self._create_model()
        
        # ä¸Šä¼ æ¨¡å‹æ¶æ„åˆ° wandb
        self._log_model_architecture()
        
        # è·Ÿè¸ªæœ€ä½³æ€§èƒ½
        self.best_reward = float('-inf')
        self.best_model_step = 0
        
        print("ğŸ‰ WandbNavRLTrainer initialized successfully!")
    
    def _init_wandb(self):
        """åˆå§‹åŒ–wandb"""
        wandb_config = {
            'architecture': 'PPO-ViT',
            'framework': 'NavRL',
            'env_config': dict(self.cfg.env),
            'training_config': dict(self.cfg.actor),
        }
        
        wandb.init(
            project=self.cfg.wandb.project,
            name=self.cfg.wandb.name,
            config=wandb_config,
            mode=self.cfg.wandb.mode,
            tags=['navrl', 'ppo-vit', 'model-management']
        )
        
        print(f"ğŸ”— wandb initialized: {wandb.run.name}")
    
    def _create_environment(self):
        """åˆ›å»ºè®­ç»ƒç¯å¢ƒ"""
        print("ğŸ—ï¸  Creating environment...")
        
        env_fn = lambda: NavigationEnv(self.cfg)
        env = ParallelEnv(
            num_workers=self.cfg.env.num_envs,
            create_env_fn=env_fn,
            device=self.device
        )
        
        # è®°å½•ç¯å¢ƒä¿¡æ¯åˆ°wandb
        wandb.config.update({
            'num_envs': self.cfg.env.num_envs,
            'env_type': 'NavigationEnv'
        })
        
        return env
    
    def _create_model(self):
        """åˆ›å»ºæˆ–åŠ è½½æ¨¡å‹"""
        checkpoint_path = getattr(self.cfg, 'resume_checkpoint', None)
        
        if checkpoint_path and checkpoint_path != "":
            # æ£€æŸ¥æ˜¯å¦ä¸ºwandb artifactè·¯å¾„
            if ":" in checkpoint_path and "/" in checkpoint_path:
                print(f"ğŸ”„ Loading from wandb artifact: {checkpoint_path}")
                model_manager = create_navrl_model(
                    cfg=self.cfg,
                    observation_spec=self.env.observation_spec,
                    action_spec=self.env.action_spec,
                    device=self.device
                )
                model_manager.load_from_wandb(checkpoint_path)
            else:
                print(f"ğŸ”„ Loading from local checkpoint: {checkpoint_path}")
                model_manager = load_pretrained_model(
                    checkpoint_path=checkpoint_path,
                    cfg=self.cfg,
                    observation_spec=self.env.observation_spec,
                    action_spec=self.env.action_spec,
                    device=self.device
                )
        else:
            print("ğŸ†• Creating new model...")
            model_manager = create_navrl_model(
                cfg=self.cfg,
                observation_spec=self.env.observation_spec,
                action_spec=self.env.action_spec,
                device=self.device
            )
        
        return model_manager
    
    def _log_model_architecture(self):
        """è®°å½•æ¨¡å‹æ¶æ„ä¿¡æ¯åˆ°wandb"""
        model_info = self.model_manager.get_model().get_model_info()
        
        # æ›´æ–°wandbé…ç½®
        wandb.config.update({
            'model_info': model_info,
            'total_parameters': model_info['total_parameters'],
            'trainable_parameters': model_info['trainable_parameters']
        })
        
        # è®°å½•æ¨¡å‹æ‘˜è¦
        wandb.log({
            'model/total_parameters': model_info['total_parameters'],
            'model/trainable_parameters': model_info['trainable_parameters'],
            'model/frozen_parameters': model_info['frozen_parameters']
        })
        
        print(f"ğŸ“Š Model info logged to wandb")
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print("ğŸš€ Starting training with wandb integration...")
        
        model = self.model_manager.get_model()
        step = 0
        
        # æ¨¡æ‹Ÿè®­ç»ƒå¾ªç¯
        for episode in range(self.cfg.total_episodes):
            # 1. ç¯å¢ƒäº¤äº’å’Œæ•°æ®æ”¶é›†
            rollout_data = self._collect_rollout(model, episode)
            
            # 2. æ¨¡å‹è®­ç»ƒ
            train_info = self._train_step(model, rollout_data)
            step += 1
            
            # 3. è®°å½•è®­ç»ƒä¿¡æ¯
            wandb.log({
                'train/episode': episode,
                'train/step': step,
                **{f'train/{k}': v for k, v in train_info.items()}
            })
            
            # 4. å®šæœŸè¯„ä¼°å’Œæ¨¡å‹ä¿å­˜
            if episode % self.cfg.eval_interval == 0:
                eval_metrics = self._evaluate(model, episode)
                
                # è®°å½•è¯„ä¼°æŒ‡æ ‡
                wandb.log({
                    'eval/episode': episode,
                    **{f'eval/{k}': v for k, v in eval_metrics.items()}
                })
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜æœ€ä½³æ¨¡å‹
                current_reward = eval_metrics['mean_reward']
                if current_reward > self.best_reward:
                    self.best_reward = current_reward
                    self.best_model_step = step
                    
                    print(f"ğŸ† New best model! Reward: {current_reward:.2f}")
                    self._save_best_model(step, eval_metrics)
            
            # 5. å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if episode % self.cfg.save_interval == 0:
                self._save_checkpoint(step, train_info)
        
        # 6. è®­ç»ƒç»“æŸï¼Œä¸Šä¼ æœ€ç»ˆæ¨¡å‹åˆ°æ³¨å†Œè¡¨
        self._upload_final_model(step)
        
        print("ğŸ‰ Training completed!")
    
    def _collect_rollout(self, model, episode):
        """æ”¶é›†rolloutæ•°æ®"""
        # æ¨¡æ‹Ÿæ•°æ®æ”¶é›†
        td = self.env.reset()
        rollout_data = []
        
        for _ in range(self.cfg.rollout_length):
            td = model(td)
            td = self.env.step(td)
            rollout_data.append(td.clone())
        
        return rollout_data
    
    def _train_step(self, model, rollout_data):
        """æ‰§è¡Œè®­ç»ƒæ­¥éª¤"""
        # å°†æ•°æ®è½¬æ¢ä¸ºbatch
        batch_td = make_batch(rollout_data, self.device)
        
        # è®­ç»ƒæ¨¡å‹
        train_info = model.train(batch_td)
        
        return train_info
    
    def _evaluate(self, model, episode):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print(f"ğŸ” Evaluating model at episode {episode}...")
        
        self.model_manager.set_training_mode(False)
        
        rewards = []
        success_rates = []
        
        # è¿è¡Œå¤šä¸ªè¯„ä¼°episode
        for eval_ep in range(10):
            td = self.env.reset()
            episode_reward = 0
            success = False
            
            for _ in range(200):  # æœ€å¤§æ­¥æ•°
                with torch.no_grad():
                    td = model(td)
                    td = self.env.step(td)
                    
                    reward = td["next", "agents", "reward"].sum().item()
                    episode_reward += reward
                    
                    # æ£€æŸ¥æ˜¯å¦æˆåŠŸï¼ˆå¯æ ¹æ®ä»»åŠ¡å®šä¹‰ï¼‰
                    if reward > 10:  # ç¤ºä¾‹æˆåŠŸæ¡ä»¶
                        success = True
                    
                    if td["next", "terminated"].any():
                        break
            
            rewards.append(episode_reward)
            success_rates.append(1.0 if success else 0.0)
        
        # æ¢å¤è®­ç»ƒæ¨¡å¼
        self.model_manager.set_training_mode(True)
        
        eval_metrics = {
            'mean_reward': sum(rewards) / len(rewards),
            'std_reward': torch.tensor(rewards).std().item(),
            'min_reward': min(rewards),
            'max_reward': max(rewards),
            'success_rate': sum(success_rates) / len(success_rates)
        }
        
        print(f"   ğŸ“Š Evaluation results: {eval_metrics}")
        return eval_metrics
    
    def _save_best_model(self, step, eval_metrics):
        """ä¿å­˜æœ€ä½³æ¨¡å‹"""
        save_path = f"models/best_model_step_{step}.pt"
        
        # ä¿å­˜å¹¶ä¸Šä¼ åˆ°wandbï¼Œå¸¦æœ‰'best'åˆ«å
        self.model_manager.save_checkpoint(
            filepath=save_path,
            step=step,
            additional_info={
                'eval_metrics': eval_metrics,
                'best_reward': self.best_reward,
                'model_type': 'best'
            },
            upload_to_wandb=True,
            wandb_alias='best'
        )
        
        print(f"ğŸ’ Best model saved and uploaded: {save_path}")
    
    def _save_checkpoint(self, step, train_info):
        """ä¿å­˜å¸¸è§„æ£€æŸ¥ç‚¹"""
        save_path = f"models/checkpoint_step_{step}.pt"
        
        # ä¿å­˜å¹¶å¯é€‰ä¸Šä¼ åˆ°wandb
        upload_to_wandb = step % (self.cfg.save_interval * 5) == 0  # æ¯5æ¬¡ä¿å­˜ä¸Šä¼ ä¸€æ¬¡
        
        self.model_manager.save_checkpoint(
            filepath=save_path,
            step=step,
            additional_info={
                'train_info': train_info,
                'model_type': 'checkpoint'
            },
            upload_to_wandb=upload_to_wandb,
            wandb_alias='latest' if upload_to_wandb else None
        )
        
        if upload_to_wandb:
            print(f"â˜ï¸  Checkpoint uploaded to wandb: step {step}")
    
    def _upload_final_model(self, step):
        """ä¸Šä¼ æœ€ç»ˆæ¨¡å‹åˆ°æ³¨å†Œè¡¨"""
        print("ğŸ¯ Uploading final model to wandb registry...")
        
        self.model_manager.upload_model_to_registry(
            model_name=f"navrl-final-{wandb.run.id}",
            description=f"Final NavRL model trained for {step} steps with best reward {self.best_reward:.2f}",
            tags=['final', 'production-ready', f'reward-{self.best_reward:.1f}'],
            step=step
        )


# === ä½¿ç”¨ç¤ºä¾‹ ===

@hydra.main(version_base=None, config_path="../cfg", config_name="train_ppo_vit")
def main(cfg: DictConfig):
    """
    ä¸»è®­ç»ƒå‡½æ•° - ä½¿ç”¨wandbæ¨¡å‹ç®¡ç†
    
    ç¤ºä¾‹é…ç½®ï¼š
    ```yaml
    wandb:
      project: "navrl-models"
      name: "ppo-vit-experiment"
      mode: "online"  # æˆ– "offline", "disabled"
    
    # ä»wandb artifactæ¢å¤è®­ç»ƒ
    resume_checkpoint: "username/navrl-models/navrl-model-step-1000:best"
    
    # æˆ–ä»æœ¬åœ°æ£€æŸ¥ç‚¹æ¢å¤
    # resume_checkpoint: "path/to/checkpoint.pt"
    ```
    """
    print("ğŸ¯ NavRL Training with wandb Model Management")
    print("=" * 60)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = WandbNavRLTrainer(cfg)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()
    
    # ç»“æŸwandbè¿è¡Œ
    wandb.finish()
    print("ğŸ‰ Training session completed!")


# === é«˜çº§ç”¨æ³•ç¤ºä¾‹ ===

def download_and_evaluate_model():
    """
    ä¸‹è½½wandbæ¨¡å‹å¹¶è¿›è¡Œè¯„ä¼°çš„ç¤ºä¾‹
    """
    print("ğŸ“¥ Downloading model from wandb for evaluation...")
    
    # åˆ›å»ºæ¨¡å‹ç®¡ç†å™¨
    model_manager = create_navrl_model(cfg, obs_spec, act_spec, device)
    
    # ä»wandbä¸‹è½½æ¨¡å‹
    success = model_manager.load_from_wandb("username/project/model-name:best")
    
    if success:
        # è¿›è¡Œè¯„ä¼°
        model = model_manager.get_model()
        # ... è¯„ä¼°ä»£ç  ...
        print("âœ… Model evaluation completed")
    else:
        print("âŒ Failed to load model from wandb")


def compare_model_versions():
    """
    æ¯”è¾ƒä¸åŒç‰ˆæœ¬æ¨¡å‹æ€§èƒ½çš„ç¤ºä¾‹
    """
    print("ğŸ” Comparing model versions...")
    
    versions = ["v1", "v2", "best", "latest"]
    results = {}
    
    for version in versions:
        model_manager = create_navrl_model(cfg, obs_spec, act_spec, device)
        success = model_manager.load_from_wandb(f"project/model-name:{version}")
        
        if success:
            # è¯„ä¼°æ¨¡å‹
            eval_result = evaluate_model(model_manager.get_model())
            results[version] = eval_result
            print(f"   {version}: {eval_result['mean_reward']:.2f}")
    
    # æ‰¾åˆ°æœ€ä½³ç‰ˆæœ¬
    best_version = max(results.keys(), key=lambda v: results[v]['mean_reward'])
    print(f"ğŸ† Best version: {best_version}")


if __name__ == "__main__":
    main()


# === wandb æ¨¡å‹ç®¡ç†åŠŸèƒ½æ€»ç»“ ===
"""
ğŸ¯ NavRL + wandb æ¨¡å‹ç®¡ç†åŠŸèƒ½ï¼š

1. **è‡ªåŠ¨æ¨¡å‹ä¸Šä¼ **ï¼š
   - è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨ä¿å­˜å’Œä¸Šä¼ æ¨¡å‹
   - æ”¯æŒç‰ˆæœ¬åˆ«åï¼ˆbest, latest, v1, v2ç­‰ï¼‰
   - ä¸°å¯Œçš„æ¨¡å‹å…ƒæ•°æ®å’Œæ¨¡å‹å¡ç‰‡

2. **æ¨¡å‹æ³¨å†Œè¡¨**ï¼š
   - ç”Ÿäº§çº§æ¨¡å‹ç‰ˆæœ¬ç®¡ç†
   - æ¨¡å‹ç”Ÿå‘½å‘¨æœŸè·Ÿè¸ª
   - æ ‡ç­¾å’Œæè¿°ç®¡ç†

3. **æ€§èƒ½è¿½è¸ª**ï¼š
   - è®­ç»ƒæŒ‡æ ‡å’Œæ¨¡å‹ç‰ˆæœ¬å…³è”
   - æœ€ä½³æ¨¡å‹è‡ªåŠ¨è¯†åˆ«å’Œä¿å­˜
   - è¯„ä¼°æŒ‡æ ‡æŒç»­è·Ÿè¸ª

4. **ä¾¿æ·åŠ è½½**ï¼š
   - æ”¯æŒä»wandb artifactç›´æ¥åŠ è½½æ¨¡å‹
   - ç‰ˆæœ¬æ¯”è¾ƒå’Œæ€§èƒ½å¯¹æ¯”
   - è·¨å®éªŒæ¨¡å‹å…±äº«

ä½¿ç”¨å‘½ä»¤ï¼š
```bash
# åŸºç¡€è®­ç»ƒ
python train_with_wandb.py

# ä»wandb artifactæ¢å¤è®­ç»ƒ
python train_with_wandb.py resume_checkpoint="user/project/model:best"

# ç¦»çº¿æ¨¡å¼è®­ç»ƒ
python train_with_wandb.py wandb.mode=offline
```

ä¼˜åŠ¿ï¼š
âœ… å®Œæ•´çš„æ¨¡å‹ç”Ÿå‘½å‘¨æœŸç®¡ç†
âœ… è‡ªåŠ¨ç‰ˆæœ¬æ§åˆ¶å’Œå…ƒæ•°æ®è·Ÿè¸ª  
âœ… ä¾¿æ·çš„æ¨¡å‹åˆ†äº«å’Œåä½œ
âœ… é›†æˆè®­ç»ƒæŒ‡æ ‡å’Œæ¨¡å‹æ€§èƒ½
âœ… ç”Ÿäº§ç¯å¢ƒæ¨¡å‹éƒ¨ç½²æ”¯æŒ
"""