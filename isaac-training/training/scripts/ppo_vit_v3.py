
import torch
import torch.nn as nn
import torch.nn.functional as F
from tensordict.tensordict import TensorDict
from tensordict.nn import TensorDictModuleBase, TensorDictSequential, TensorDictModule
from einops.layers.torch import Rearrange
from torchrl.modules import ProbabilisticActor
from torchrl.envs.transforms import CatTensors
from utils import ValueNorm, make_mlp, IndependentNormal, Actor, GAE, make_batch, IndependentBeta, BetaActor, vec_to_world
from VIT import VIT
from torch.cuda.amp import autocast, GradScaler  # âœ… æ··åˆç²¾åº¦è®­ç»ƒ


# ==========================================
# 2. å…±äº«ç‰¹å¾æå–å™¨ (çº¯å‡€æ¨¡å‹ï¼Œä¸å«ç»´åº¦å¤„ç†)
# ==========================================
class SharedFeatureExtractor(nn.Module):
    def __init__(self, device, pretrained_checkpoint_path=None):
        super().__init__()
        self.device = device
        
        # --- A. ViT Backbone åˆå§‹åŒ– ---
        print("Loading ViT backbone")
        self.vit = VIT().to(device)
        
        # === å‚æ•°ç®¡ç†ï¼šåŠ è½½ã€å†»ç»“ã€è§£å†» ===
        self._load_vit_weights(pretrained_checkpoint_path)
        self._setup_parameter_training()
        
        # --- B. å…¶ä»–ç»„ä»¶åˆå§‹åŒ– ---
        self._init_other_modules()
    def forward(self, camera, dynamic_obstacle, state):
        """
        Input shapes are assumed to be flattened: [Batch, ...] 
        No dimension checks inside the model.
        """
        camera = torch.nan_to_num(camera, nan=10.0, posinf=10.0, neginf=0.0)
        camera = camera.clamp(0.0, 10.0)
        
        # éªŒè¯è¾“å…¥æ˜¯å•é€šé“ç°åº¦å›¾ [Batch, 1, H, W]
        assert camera.dim() == 4, f"Camera input should be 4D [B,C,H,W], got {camera.shape}"
        assert camera.shape[1] == 1, f"Camera input should be grayscale (1 channel), got {camera.shape[1]} channels"
        
        # 1. Image Processing
        x = (camera / 10 if camera.max() > 1.1 else camera)
        
        # [FIX] è½¬æ¢ RGB ä¸ºå•é€šé“ (Gray)ï¼Œå› ä¸º VIT æ¨¡å‹å®šä¹‰ä¸º 1 é€šé“è¾“å…¥
        # x = (x - self.mean) / self.std
        
        # with torch.no_grad():
            # ViT forward
        v_feat = self.vit(x) # [Batch, 512]
        
        # 2. Dynamic Obstacle Processing
        d_feat = self.dyn_ext(dynamic_obstacle) # [Batch, 64]
        
        # 3. State Processing (NEW: encode state for dimension balance)
        s_feat = self.state_ext(state) # [Batch, 8] -> [Batch, 64]
        
        # 4. Concatenation & Fusion (584 = 512+64+8)
        combined = torch.cat([v_feat, d_feat, state], dim=-1) # [Batch, 584]
        latent = self.fusion_mlp(combined) # [Batch, 128]
        
        return latent
    
    def _load_vit_weights(self, checkpoint_path=None):
        if(checkpoint_path is not None):
            print("ğŸ”„ Loading default ViT weights...")
            full_model_checkpoint = torch.load(checkpoint_path, map_location=self.device)
            # full_model_checkpoint = torch.load("/home/u20/NavRL/isaac-training/training/scripts/ViTLSTM_model.pth")
            
            # åªåŠ è½½ ViT ç›¸å…³çš„æƒé‡ï¼ˆè¿‡æ»¤æ‰ LSTM å’Œ fc2ï¼‰
            vit_state_dict = {}
            ignored_keys = []
            for k, v in full_model_checkpoint.items():
                # ä¿ç•™ encoder_blocksã€decoder ä»¥åŠè¾…åŠ©å±‚çš„æƒé‡
                if k.startswith('encoder_blocks') or k.startswith('decoder') or \
                   k.startswith('up_sample') or k.startswith('pxShuffle') or k.startswith('down_sample'):
                    vit_state_dict[k] = v
                else:
                    ignored_keys.append(k)
            
            # åŠ è½½æƒé‡å¹¶è®°å½•ç»“æœ
            missing_keys, unexpected_keys = self.vit.load_state_dict(vit_state_dict, strict=False)
            
            print(f" ViT weights loaded successfully!")
            print(f"   - Loaded keys: {len(vit_state_dict)}")
            print(f"   - Ignored keys (LSTM/FC2): {len(ignored_keys)}")
            if missing_keys:
                print(f"     Missing keys (will use random init): {len(missing_keys)}")
            if unexpected_keys:
                print(f"     Unexpected keys: {len(unexpected_keys)}")

        else:
            print(f" Failed to load ViT weights")
            print("   - Using random initialization")
            # self._init_vit_weights()

    def _setup_parameter_training(self):
        """è®¾ç½®å‚æ•°çš„è®­ç»ƒçŠ¶æ€ï¼šå†»ç»“å’Œè§£å†»"""
        # ç¬¬ä¸€æ­¥ï¼šå…¨éƒ¨å†»ç»“
        self._freeze_all_vit_parameters()
        
        # ç¬¬äºŒæ­¥ï¼šé€‰æ‹©æ€§è§£å†»
        self._unfreeze_decoder_modules()
    
    def _freeze_all_vit_parameters(self):
        """å†»ç»“æ‰€æœ‰ViTå‚æ•°"""
        frozen_params = 0
        for param in self.vit.parameters():
            param.requires_grad = False
            frozen_params += param.numel()
        
        print(f"â„ï¸  Frozen all ViT parameters: {frozen_params:,}")
    
    def _unfreeze_decoder_modules(self):
        """è§£å†»æŒ‡å®šçš„decoderæ¨¡å—è¿›è¡Œfine-tuning"""
        # decoder_modules = ['encoder_blocks','decoder', 'up_sample', 'pxShuffle']
        decoder_modules = ['decoder', 'up_sample', 'pxShuffle']
        unfrozen_params = 0
        
        print("ğŸ”“ Unfreezing ViT decoder modules for fine-tuning:")
        for module_name in decoder_modules:
            if hasattr(self.vit, module_name):
                module = getattr(self.vit, module_name)
                for param in module.parameters():
                    param.requires_grad = True
                    unfrozen_params += param.numel()
                print(f"   - {module_name}: âœ… unfrozen")
            else:
                print(f"   - {module_name}: âš ï¸ not found in model")
        
        print(f"   - Total decoder parameters: {unfrozen_params:,}")
        print(f"   - Decoder learning rate: 10x lower than base rate")
    
    def _init_other_modules(self):
        """åˆå§‹åŒ–å…¶ä»–ç½‘ç»œæ¨¡å—"""
        # --- B. åŠ¨æ€éšœç¢ç‰©æå–å™¨ ---
        self.dyn_ext = nn.Sequential(
            Rearrange("n c w h -> n (c w h)"),
            nn.Linear(50, 128), nn.LeakyReLU(), nn.LayerNorm(128),
            nn.Linear(128, 64), nn.LeakyReLU(), nn.LayerNorm(64),
        ).to(self.device)

        # --- C. State ç‰¹å¾æå–å™¨ï¼ˆæ–°å¢ï¼‰---
        # å°† 8 ç»´åŸå§‹ state ç¼–ç ä¸º 64 ç»´ç‰¹å¾ï¼Œä¸ Dyn Obs ç»´åº¦å¯¹é½
        self.state_ext = nn.Sequential(
            nn.Linear(8, 64), nn.LeakyReLU(), nn.LayerNorm(64),
            nn.Linear(64, 64), nn.LeakyReLU(), nn.LayerNorm(64),
        ).to(self.device)

        # --- D. èåˆ MLP (æ¼æ–—ç»“æ„) ---
        # Input: 512(ViT) + 64(Dyn) + 8(State) = 584
        self.fusion_mlp = nn.Sequential(
            nn.Linear(584, 512), nn.LeakyReLU(), nn.LayerNorm(512),
            nn.Linear(512, 256), nn.LeakyReLU(), nn.LayerNorm(256),
            nn.Linear(256, 128), nn.LeakyReLU(), nn.LayerNorm(128)
        ).to(self.device)
    
    # === é«˜çº§å‚æ•°ç®¡ç†æ¥å£ ===
    def freeze_vit_encoder(self):
        """å†»ç»“ViT encoderï¼ˆä¿æŒdecoderè§£å†»çŠ¶æ€ï¼‰"""
        encoder_modules = ['encoder_blocks']
        frozen_params = 0
        
        for module_name in encoder_modules:
            if hasattr(self.vit, module_name):
                module = getattr(self.vit, module_name)
                for param in module.parameters():
                    param.requires_grad = False
                    frozen_params += param.numel()
        
        print(f"â„ï¸  Frozen encoder: {frozen_params:,} parameters")
    
    def unfreeze_all_vit(self):
        """è§£å†»æ‰€æœ‰ViTå‚æ•°ï¼ˆç”¨äºå®Œå…¨fine-tuningï¼‰"""
        unfrozen_params = 0
        for param in self.vit.parameters():
            param.requires_grad = True
            unfrozen_params += param.numel()
        
        print(f"ğŸ”“ Unfrozen all ViT: {unfrozen_params:,} parameters")
    
    def get_trainable_parameter_groups(self):
        """
        è·å–å¯è®­ç»ƒå‚æ•°ç»„ï¼Œç”¨äºåˆ›å»ºä¼˜åŒ–å™¨
        
        Returns:
            dict: åŒ…å«ä¸åŒå‚æ•°ç»„çš„å­—å…¸
        """
        vit_decoder_params = [p for p in self.vit.parameters() if p.requires_grad]
        other_params = []
        
        # æ”¶é›†å…¶ä»–æ¨¡å—çš„å‚æ•°
        for module in [self.dyn_ext, self.state_ext, self.fusion_mlp]:
            other_params.extend([p for p in module.parameters() if p.requires_grad])
        
        return {
            'vit_decoder': vit_decoder_params,
            'other_modules': other_params,
            'vit_decoder_count': sum(p.numel() for p in vit_decoder_params),
            'other_modules_count': sum(p.numel() for p in other_params)
        }



# ==========================================
# 3. PPO ä¸»ç±»
# ==========================================
class PPOVIT(TensorDictModuleBase):
    def __init__(self, cfg, observation_spec, action_spec, device):
        super().__init__()
        self.cfg = cfg
        self.device = device
        
        # å¤„ç† action_spec å…¼å®¹æ€§
        if hasattr(action_spec, "shape"):
            shape = tuple(action_spec.shape)
            self.action_dim = int(shape[-1]) if len(shape) > 0 else int(shape[0])
        else:
            self.action_dim = int(action_spec)

        # --- åˆå§‹åŒ–ç½‘ç»œç»„ä»¶ ---
        pretrained_checkpoint_path = getattr(cfg.feature_extractor, 'pretrained_checkpoint_path', None)
        self.shared_features = SharedFeatureExtractor(device, pretrained_checkpoint_path=pretrained_checkpoint_path)
        
        # Actor Head (Input: _latent -> Output: alpha, beta)
        self.actor_head = ProbabilisticActor(
            TensorDictModule(
                BetaActor(self.action_dim), 
                in_keys=["_latent"], 
                out_keys=["alpha", "beta"]
            ),
            in_keys=["alpha", "beta"],
            out_keys=[("agents", "action_normalized")],
            distribution_class=IndependentBeta,
            return_log_prob=True
        ).to(device)

        # Critic Head (Input: _latent -> Output: state_value)
        self.critic_head = nn.Linear(128, 1).to(device)


        # 1. æ„é€ ä¸€ä¸ª Dummy Input (å‡æ•°æ®)
        # ä» observation_spec ä¸­ç”Ÿæˆå…¨0æ•°æ®ï¼Œå¹¶æ·»åŠ  Batch ç»´åº¦ [1, ...]
        dummy_tensordict = observation_spec.zero().unsqueeze(0).to(device).reshape(-1)
    
        with torch.no_grad():
            latent = self.shared_features(
                dummy_tensordict["agents", "observation", "camera"],
                dummy_tensordict["agents", "observation", "dynamic_obstacle"],
                dummy_tensordict["agents", "observation", "state"]
            )
            dummy_tensordict.set("_latent", latent)
            # è¿è¡Œä¸€æ¬¡ actor_headï¼Œè§¦å‘ LazyLinear åˆå§‹åŒ–
            self.actor_head(dummy_tensordict)

        # ----------------------------------------------------------------
        # [åŸæœ‰é€»è¾‘] ç°åœ¨çš„æƒé‡å·²ç»å®ä¾‹åŒ–äº†ï¼Œå¯ä»¥å®‰å…¨åˆå§‹åŒ–äº†
        # ----------------------------------------------------------------
        # --- ä¼˜åŒ–å™¨ä¸å·¥å…· ---
        # åˆ†ç»„ä¼˜åŒ–å™¨ï¼šViT decoderä½¿ç”¨ä½å­¦ä¹ ç‡ï¼Œå…¶ä»–æ¨¡å—ä½¿ç”¨æ­£å¸¸å­¦ä¹ ç‡
        param_groups = []
        
        # 1. ViT decoderå¯è®­ç»ƒå‚æ•°ï¼ˆä½å­¦ä¹ ç‡ - 10å€é™ä½ï¼‰
        vit_decoder_params = [p for p in self.shared_features.vit.parameters() if p.requires_grad]
        if vit_decoder_params:
            decoder_lr = cfg.actor.learning_rate * 0.1  # 10å€ä½çš„å­¦ä¹ ç‡
            param_groups.append({
                'params': vit_decoder_params, 
                'lr': decoder_lr,
                'name': 'vit_decoder'
            })
            print(f"ğŸ“š ViT decoder params: {sum(p.numel() for p in vit_decoder_params):,} @ lr={decoder_lr}")
        
        # 2. å…¶ä»–ç½‘ç»œå‚æ•°ï¼ˆæ­£å¸¸å­¦ä¹ ç‡ï¼‰
        other_modules = [self.shared_features.dyn_ext, self.shared_features.state_ext, 
                        self.shared_features.fusion_mlp, self.actor_head, self.critic_head]
        other_params = []
        for module in other_modules:
            other_params.extend([p for p in module.parameters() if p.requires_grad])
        
        param_groups.append({
            'params': other_params, 
            'lr': cfg.actor.learning_rate,
            'name': 'task_specific'
        })
        
        print(f"ğŸ¯ Task-specific params: {sum(p.numel() for p in other_params):,} @ lr={cfg.actor.learning_rate}")
        print(f"ğŸ“Š Total trainable parameters: {sum(p.numel() for p in vit_decoder_params + other_params):,}")
        
        # ä½¿ç”¨å‚æ•°ç»„åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = torch.optim.Adam(param_groups)
        self.gae = GAE(0.99, 0.95)
        self.value_norm = ValueNorm(1).to(device)
        self.critic_loss_fn = nn.HuberLoss(delta=10)
        
        # æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16ï¼‰- å¯èŠ‚çœ 30-50% æ˜¾å­˜
        self.use_amp = getattr(cfg, 'use_amp', True)  # é»˜è®¤å¯ç”¨
        self.scaler = GradScaler() if self.use_amp else None
        if self.use_amp:
            print("âœ… Mixed Precision (AMP) enabled - saving 30-50% GPU memory")

        # ç°åœ¨è°ƒç”¨åˆå§‹åŒ–ä¸ä¼šæŠ¥é”™äº†
        self._init_weights(skip_initialization=False)

    def _init_weights(self, skip_initialization=False):
        """
        åˆå§‹åŒ–Actorå’ŒCriticæƒé‡
        
        Args:
            skip_initialization: è·³è¿‡åˆå§‹åŒ–ï¼ˆå½“ä»å®Œæ•´checkpointåŠ è½½æ—¶ä½¿ç”¨ï¼‰
        """
        if skip_initialization:
            print("âš ï¸  Skipping Actor/Critic initialization - loaded from checkpoint")
            return
        
        def init_(m):
            if isinstance(m, nn.Linear):
                weight = getattr(m, "weight", None)
                if isinstance(weight, torch.nn.parameter.UninitializedParameter):
                    return
                nn.init.orthogonal_(weight, 0.01)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.)
        
        print("ğŸ”„ Initializing Actor and Critic weights...")
        self.actor_head.apply(init_)
        self.critic_head.apply(init_)
        print("   - Actor head: âœ… randomly initialized")
        print("   - Critic head: âœ… randomly initialized")

    def load_full_checkpoint(self, checkpoint_path):
        """
        åŠ è½½å®Œæ•´çš„PPO-ViTæ¨¡å‹checkpoint
        è¿™ä¸ªæ–¹æ³•ä¼šåŠ è½½æ‰€æœ‰ç»„ä»¶çš„æƒé‡ï¼šSharedFeatureExtractor + Actor + Critic
        
        Args:
            checkpoint_path: å®Œæ•´PPOæ¨¡å‹checkpointè·¯å¾„
        """
        try:
            print(f"ğŸ”„ Loading FULL PPO-ViT checkpoint from: {checkpoint_path}")
            
            # åŠ è½½checkpoint
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            # æ£€æŸ¥checkpointç»“æ„
            if 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
            elif 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict'] 
            else:
                state_dict = checkpoint
            
            # ç»Ÿè®¡åŠ è½½çš„ç»„ä»¶
            loaded_stats = {
                'shared_features_vit': 0,      # ViTéƒ¨åˆ†
                'shared_features_other': 0,    # å…¶ä»–å…±äº«ç‰¹å¾ï¼ˆdyn_ext, state_ext, fusion_mlpï¼‰
                'actor_head': 0, 
                'critic_head': 0,
                'skipped': 0
            }
            
            # å½“å‰æ¨¡å‹çš„state dict
            current_state = self.state_dict()
            matched_params = {}
            
            for name, param in state_dict.items():
                if name in current_state:
                    # æ£€æŸ¥å½¢çŠ¶æ˜¯å¦åŒ¹é…
                    if current_state[name].shape == param.shape:
                        matched_params[name] = param
                        
                        # æ›´ç²¾ç¡®çš„ç»„ä»¶åˆ†ç±»
                        if name.startswith('shared_features.vit.'):
                            loaded_stats['shared_features_vit'] += 1
                        elif name.startswith('shared_features.'):
                            loaded_stats['shared_features_other'] += 1
                        elif name.startswith('actor_head.'):
                            loaded_stats['actor_head'] += 1
                        elif name.startswith('critic_head.'):
                            loaded_stats['critic_head'] += 1
                    else:
                        print(f"   âš ï¸  Shape mismatch {name}: {current_state[name].shape} vs {param.shape}")
                        loaded_stats['skipped'] += 1
                else:
                    print(f"   âš ï¸  Key not found: {name}")
                    loaded_stats['skipped'] += 1
            
            # åŠ è½½åŒ¹é…çš„æƒé‡
            self.load_state_dict(matched_params, strict=False)
            
            # æŠ¥å‘ŠåŠ è½½ç»“æœ
            total_loaded = sum(v for k, v in loaded_stats.items() if k != 'skipped')
            print(f"âœ… Loaded {total_loaded} parameters from checkpoint:")
            
            # æ›´æ¸…æ™°çš„æŠ¥å‘Šæ ¼å¼
            if loaded_stats['shared_features_vit'] > 0:
                print(f"   ğŸ§  ViT backbone: {loaded_stats['shared_features_vit']} parameters")
            if loaded_stats['shared_features_other'] > 0:
                print(f"   âš™ï¸  Other features: {loaded_stats['shared_features_other']} parameters")
            if loaded_stats['actor_head'] > 0:
                print(f"   ğŸ¯ Actor head: {loaded_stats['actor_head']} parameters")
            if loaded_stats['critic_head'] > 0:
                print(f"   ğŸ¯ Critic head: {loaded_stats['critic_head']} parameters")
            if loaded_stats['skipped'] > 0:
                print(f"   âš ï¸  Skipped: {loaded_stats['skipped']} parameters (shape mismatch or not found)")
            
            # åŠ è½½è®­ç»ƒçŠ¶æ€ï¼ˆå¯é€‰ï¼‰
            if 'optimizer_state_dict' in checkpoint:
                try:
                    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                    print("   ğŸ“ˆ Optimizer state: âœ… loaded")
                except Exception as e:
                    print(f"   ğŸ“ˆ Optimizer state: âŒ failed ({e})")
            
            if 'value_norm_state' in checkpoint:
                try:
                    self.value_norm.load_state_dict(checkpoint['value_norm_state'])
                    print("   ğŸ“Š Value normalization: âœ… loaded")
                except Exception as e:
                    print(f"   ğŸ“Š Value normalization: âŒ failed ({e})")
            
            return True
                    
        except Exception as e:
            print(f"âŒ Failed to load checkpoint: {e}")
            print("   - Using current initialization")
            return False

    def __call__(self, tensordict):
        """æ¨ç†æ¨¡å¼ï¼šé€šå¸¸ç”¨äºç¯å¢ƒäº¤äº’"""
        # 1. æå–ç‰¹å¾
        latent = self.shared_features(
            tensordict["agents", "observation", "camera"],
            tensordict["agents", "observation", "dynamic_obstacle"],
            tensordict["agents", "observation", "state"]
        )
        tensordict.set("_latent", latent)
        
        # 2. ç­–ç•¥é‡‡æ ·
        self.actor_head(tensordict)
        
        # 3. ä»·å€¼ä¼°è®¡
        value = self.critic_head(latent)
        tensordict.set("state_value", value)

        # 4. åæ ‡è½¬æ¢ (Local -> World)
        actions = (2 * tensordict["agents", "action_normalized"] * self.cfg.actor.action_limit) - self.cfg.actor.action_limit
        actions_world = vec_to_world(actions, tensordict["agents", "observation", "direction"])
        tensordict["agents", "action"] = actions_world
        
        # âœ… æ¸…ç†ä¸­é—´å˜é‡ï¼ˆå¯é€‰ï¼Œæå‡ collector æ€§èƒ½ï¼‰
        # æ³¨é‡Šæ‰è¿™è¡Œå¦‚æœéœ€è¦è°ƒè¯•ä¸­é—´å˜é‡
        # for key in ["_latent", "alpha", "beta"]:
        #     if key in tensordict.keys():
        #         tensordict.exclude(key, inplace=True)
        
        return tensordict

    def train(self, tensordict):
        """
        è®­ç»ƒå‡½æ•°ï¼š
        åœ¨å¤–éƒ¨å¤„ç†æ‰€æœ‰ç»´åº¦é—®é¢˜ï¼Œæ¨¡æ‹Ÿ SAC çš„ buffer.sample() æ•ˆæœã€‚
        """
        # 1. è·å–ç»´åº¦ä¿¡æ¯ [Batch, Time]
        # å³ä½¿ T=1ï¼ŒTensordict ä¾ç„¶ä¿ç•™è¿™ä¸ªç»“æ„
        B, T = tensordict.shape 
        
        # step 1: å±•å¹³ Batch ç»´åº¦ä¾›ç½‘ç»œæ¨ç†
        td_flat = tensordict.reshape(-1)
        next_td_flat = tensordict["next"].reshape(-1)

        with torch.no_grad():
            # -----------------------------------------------------------------
            # [æ–¹æ¡ˆ1] ä½¿ç”¨é‡‡æ ·æ—¶å·²ä¿å­˜çš„valuesï¼ˆæ ‡å‡†PPOåšæ³•ï¼‰
            # -----------------------------------------------------------------
            # 1. ä»tensordictè¯»å–é‡‡æ ·æ—¶ä¿å­˜çš„values
            values_flat = tensordict["state_value"].reshape(-1, 1)  # [B*T, 1]
            
            # 2. åªéœ€è®¡ç®—next_values
            next_latent = self.shared_features(
                next_td_flat["agents", "observation", "camera"],
                next_td_flat["agents", "observation", "dynamic_obstacle"],
                next_td_flat["agents", "observation", "state"]
            )
            next_values_flat = self.critic_head(next_latent)  # [B*T, 1]
            
            # 3. åå½’ä¸€åŒ–
            values_flat = self.value_norm.denormalize(values_flat)
            next_values_flat = self.value_norm.denormalize(next_values_flat)

            # 4. è¿˜åŸä¸º [B, T] ä¾› GAE ä½¿ç”¨
            values = values_flat.squeeze(-1).view(B, T)
            next_values = next_values_flat.squeeze(-1).view(B, T)
            # -----------------------------------------------------------------

        # step 3: å‡†å¤‡ GAE æ‰€éœ€æ•°æ®
        # åŸå§‹æ•°æ®æ˜¯ [B, T, 1]ï¼Œsqueeze æ‰æœ€åä¸€ç»´å˜æˆ [B, T]
        rewards = tensordict["next", "agents", "reward"].squeeze(-1) 
        dones = tensordict["next", "terminated"].float().squeeze(-1)

        # step 4: è®¡ç®— GAE
        # è¾“å…¥å…¨éƒ¨ä¸º [B, T]ï¼Œvalues å’Œ next_values æ˜¯åå½’ä¸€åŒ–åçš„çœŸå®å€¼
        # GAE è¿”å›çš„ adv å’Œ ret ä¹Ÿéƒ½æ˜¯çœŸå®å°ºåº¦
        adv, ret = self.gae(rewards, dones, values, next_values)
        
        # step 5: ValueNorm æ›´æ–°å’Œå½’ä¸€åŒ–
        # 5.1 ç”¨çœŸå® Return æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        ret_flat = ret.reshape(-1, 1)  # [B*T, 1]
        self.value_norm.update(ret_flat)
        
        # 5.2 å½’ä¸€åŒ– Return ç”¨äº Critic Loss Target
        ret_normalized_flat = self.value_norm.normalize(ret_flat)  # [B*T, 1]
        
        # 5.3 è¿˜åŸå½¢çŠ¶
        ret_normalized = ret_normalized_flat.view(B, T, 1)
        
        # step 6: å°†è®¡ç®—ç»“æœå­˜å› tensordict
        # æ ‡å‡†åŒ– Advantageï¼ˆå…¨å±€å½’ä¸€åŒ–ï¼‰
        adv_normalized = (adv - adv.mean()) / adv.std().clip(1e-7)
        
        # å½’ä¸€åŒ–é‡‡æ ·æ—¶çš„ valueï¼ˆç”¨äº critic loss clippingï¼‰
        values_normalized = self.value_norm.normalize(values.unsqueeze(-1).reshape(-1, 1))
        values_normalized = values_normalized.view(B, T, 1)
        
        tensordict.set("adv", adv_normalized.unsqueeze(-1))  # [B, T, 1] - å½’ä¸€åŒ–åçš„
        tensordict.set("ret", ret_normalized)     # [B, T, 1] - å½’ä¸€åŒ–åçš„
        tensordict.set("state_value", values_normalized)  # [B, T, 1] - å½’ä¸€åŒ–åçš„ï¼ˆç”¨äº clippingï¼‰
        
        # step 5: æœ€ç»ˆå±•å¹³ï¼Œå‡†å¤‡è®­ç»ƒ
        # æ­¤æ—¶æ•°æ®å®Œå…¨æ‰“æ•£ï¼Œä¸å†æœ‰æ—¶åºæ¦‚å¿µï¼Œç­‰åŒäº SAC Buffer
        td_flat = tensordict.reshape(-1)

        infos = []
        for epoch in range(self.cfg.training_epoch_num):
            # å…¨é‡æ›´æ–°ï¼ˆä¸ä½¿ç”¨ Minibatchï¼‰
            update_result = self._update(td_flat)
            infos.append(update_result)
        
        if len(infos) == 0: return {}
        infos = torch.stack(infos).to_tensordict()
        infos = infos.apply(torch.mean, batch_size=[])
        return {k: v.item() for k, v in infos.items()}

    def _update(self, batch):
        """
        æ›´æ–°å‡½æ•°ï¼š
        è¾“å…¥ batch æ˜¯ä¸€ç»´çš„ [Minibatch_Size]ï¼Œå®Œå…¨å¯¹ç…§ SAC é£æ ¼ã€‚
        """
        # 1. é‡æ–°æå–ç‰¹å¾ (å¸¦æ¢¯åº¦)
        # è¾“å…¥æ•°æ®å·²ç»æ˜¯ squeeze è¿‡çš„æ‰å¹³æ•°æ®
        latent = self.shared_features(
            batch["agents", "observation", "camera"],
            batch["agents", "observation", "dynamic_obstacle"],
            batch["agents", "observation", "state"]
        )
        # å°†ç‰¹å¾æ³¨å…¥ batchï¼Œä¾› actor_head ä½¿ç”¨
        batch.set("_latent", latent)

        # 2. Actor Update
        # è·å–åŠ¨ä½œåˆ†å¸ƒ
        action_dist = self.actor_head.get_dist(batch)
        log_probs = action_dist.log_prob(batch["agents", "action_normalized"])
        entropy_per_dim = action_dist.base_dist.entropy()  # [B, 3] æ¯ä¸ªåŠ¨ä½œç»´åº¦çš„ç†µ
        action_entropy = entropy_per_dim.mean()  # å¹³å‡è€Œéæ±‚å’Œ

        # PPO Loss Calculation
        # adv å·²ç»æ˜¯ [B, 1]ï¼Œsqueeze æˆ [B] è¿›è¡Œè®¡ç®—
        # æ³¨æ„ï¼šadvå·²ç»åœ¨train()ä¸­å…¨å±€å½’ä¸€åŒ–ï¼Œè¿™é‡Œç›´æ¥ä½¿ç”¨
        advantage = batch["adv"].squeeze(-1)
        
        ratio = torch.exp(log_probs - batch["sample_log_prob"])
        surr1 = advantage * ratio
        surr2 = advantage * ratio.clamp(1.-self.cfg.actor.clip_ratio, 1.+self.cfg.actor.clip_ratio)
        actor_loss = -torch.min(surr1, surr2).mean() - self.cfg.entropy_loss_coefficient * action_entropy

        # 3. Critic Update (with Value Clipping)
        # è·å–é‡‡æ ·æ—¶çš„æ—§ valueï¼ˆå½’ä¸€åŒ–åçš„ï¼‰
        b_value = batch["state_value"].squeeze(-1)  # [B]
        
        # è®¡ç®—å½“å‰ç­–ç•¥çš„æ–° value
        value = self.critic_head(latent).squeeze(-1)  # [B]
        
        # Value Clipping: é™åˆ¶ value æ›´æ–°å¹…åº¦
        value_clipped = b_value + (value - b_value).clamp(
            -self.cfg.critic.clip_ratio, 
            self.cfg.critic.clip_ratio
        )
        
        # Target: å½’ä¸€åŒ–åçš„ return
        target = batch["ret"].squeeze(-1)  # [B]
        
        # è®¡ç®—ä¸¤ç§ Critic Lossï¼Œå–æœ€å¤§å€¼ï¼ˆæ›´ä¿å®ˆçš„æ›´æ–°ï¼‰
        critic_loss_clipped = self.critic_loss_fn(value_clipped, target)
        critic_loss_original = self.critic_loss_fn(value, target)
        critic_loss = torch.max(critic_loss_clipped, critic_loss_original)

        # 4. Total Loss & Optimization
        total_loss = actor_loss + critic_loss

        self.optimizer.zero_grad()
        total_loss.backward()
        
        # æ¢¯åº¦è£å‰ªï¼šåˆ†åˆ«è£å‰ªå’Œç›‘æ§ä¸åŒå‚æ•°ç»„
        actor_params = [p for p in self.actor_head.parameters() if p.requires_grad]
        critic_params = [p for p in self.critic_head.parameters() if p.requires_grad]
        shared_params = [p for p in self.shared_features.parameters() if p.requires_grad and 'vit' not in str(p)]
        vit_decoder_params = [p for p in self.shared_features.vit.parameters() if p.requires_grad]
        
        actor_grad_norm = torch.nn.utils.clip_grad_norm_(actor_params, max_norm=5.0)
        critic_grad_norm = torch.nn.utils.clip_grad_norm_(critic_params, max_norm=5.0)
        shared_grad_norm = torch.nn.utils.clip_grad_norm_(shared_params, max_norm=5.0)
        vit_decoder_grad_norm = torch.nn.utils.clip_grad_norm_(vit_decoder_params, max_norm=1.0) if vit_decoder_params else torch.tensor(0.0)
        
        self.optimizer.step()

        # è®¡ç®— Explained Varianceï¼ˆè¡¡é‡ Critic é¢„æµ‹è´¨é‡ï¼‰
        explained_var = 1 - F.mse_loss(value, target) / target.var()

        return TensorDict({
            "actor_loss": actor_loss.detach(),
            "critic_loss": critic_loss.detach(),
            "entropy": action_entropy.detach(),
            "total_loss": total_loss.detach(),
            "actor_grad_norm": actor_grad_norm.detach(),
            "critic_grad_norm": critic_grad_norm.detach(),
            "shared_grad_norm": shared_grad_norm.detach(),
            "vit_decoder_grad_norm": vit_decoder_grad_norm.detach(),
            "explained_var": explained_var.detach(),
        }, [])