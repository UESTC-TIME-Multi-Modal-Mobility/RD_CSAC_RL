'''
Author: zdytim zdytim@foxmail.com
Date: 2026-01-06 11:20:29
LastEditors: zdytim zdytim@foxmail.com
LastEditTime: 2026-01-06 12:40:13
FilePath: /NavRL/isaac-training/training/scripts/models/sac_model.py
Description: è¿™æ˜¯é»˜è®¤è®¾ç½®,è¯·è®¾ç½®`customMade`, æ‰“å¼€koroFileHeaderæŸ¥çœ‹é…ç½® è¿›è¡Œè®¾ç½®: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE
'''
"""
SAC Model Manager
=================
æŠ½è±¡çš„ SAC æ¨¡å‹ç®¡ç†æ¨¡å—ï¼Œå°† SAC æ¨¡å‹çš„æ ¸å¿ƒé€»è¾‘ä»è®­ç»ƒè„šæœ¬ä¸­åˆ†ç¦»å‡ºæ¥ã€‚
æä¾›ç»Ÿä¸€çš„æ¨¡å‹åˆ›å»ºã€åŠ è½½ã€ä¿å­˜å’Œé…ç½®ç®¡ç†æ¥å£ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. SACFeatureExtractor: åŸºäº CNN çš„ç‰¹å¾æå–å™¨ï¼Œç”¨äº lidar å’ŒåŠ¨æ€éšœç¢ç‰©
2. ActorNetwork: SAC Actor ç½‘ç»œï¼Œè¾“å‡º TanhNormal åˆ†å¸ƒ
3. CriticNetwork: SAC Critic ç½‘ç»œï¼ˆQå‡½æ•°ï¼‰
4. SACModel: å®Œæ•´çš„ SAC æ¨¡å‹ï¼ŒåŒ…å« Actorã€åŒ Criticã€Target Network å’Œ Temperature
5. SACModelManager: æ¨¡å‹ç®¡ç†å™¨ï¼Œæä¾›ç»Ÿä¸€çš„é…ç½®å’ŒçŠ¶æ€ç®¡ç†æ¥å£

ä½œè€…: NavRL Team
æ—¥æœŸ: 2026å¹´1æœˆ6æ—¥
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from tensordict.tensordict import TensorDict
from tensordict.nn import TensorDictModuleBase, TensorDictSequential, TensorDictModule
from einops.layers.torch import Rearrange
from torchrl.modules import ProbabilisticActor
from torchrl.envs.transforms import CatTensors
from torchrl.modules.distributions import TanhNormal
from copy import deepcopy
import os
import tempfile
import json
from pathlib import Path
from typing import Dict, Optional, Tuple, List, Union

# wandb integration
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("âš ï¸  wandb not available. Model uploading will be disabled.")

# å¯¼å…¥é¡¹ç›®ä¾èµ–
from utils import vec_to_world, GaussianActor


class SACFeatureExtractor(nn.Module):
    """
    SAC å…±äº«ç‰¹å¾æå–å™¨
    
    åŠŸèƒ½ï¼š
    1. Lidar CNN ç‰¹å¾æå–
    2. åŠ¨æ€éšœç¢ç‰©ç¼–ç å™¨
    3. çŠ¶æ€ç¼–ç å™¨
    4. ç‰¹å¾èåˆï¼ˆæ‹¼æ¥ + LayerNormï¼‰
    """
    
    def __init__(self, device: torch.device):
        super().__init__()
        self.device = device
        
        # Lidar ç‰¹å¾æå–ç½‘ç»œ
        self.lidar_cnn = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=4, kernel_size=[5, 3], padding=[2, 1]), 
            nn.ELU(), 
            nn.Conv2d(in_channels=4, out_channels=16, kernel_size=[5, 3], stride=[2, 1], padding=[2, 1]), 
            nn.ELU(),
            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=[5, 3], stride=[2, 2], padding=[2, 1]), 
            nn.ELU(),
            Rearrange("n c w h -> n (c w h)"),
            nn.Linear(in_features=288, out_features=128), 
            nn.LayerNorm(128),
        ).to(device)
        
        # åŠ¨æ€éšœç¢ç‰©ç‰¹å¾æå–ç½‘ç»œ
        self.dyn_obs_net = nn.Sequential(
            Rearrange("n c w h -> n (c w h)"),
            nn.Linear(50, 128),
            nn.LeakyReLU(),
            nn.LayerNorm(128),
            nn.Linear(128, 64),
            nn.LeakyReLU(),
            nn.LayerNorm(64),
        ).to(device)
        
        # TensorDict æ ¼å¼çš„ç‰¹å¾æå–æµæ°´çº¿
        self.feature_extractor = TensorDictSequential(
            TensorDictModule(self.lidar_cnn, [("observation", "lidar")], ["_cnn_feature"]),
            TensorDictModule(self.dyn_obs_net, [("observation", "dynamic_obstacle")], ["_dynamic_obstacle_feature"]),
            CatTensors(["_cnn_feature", ("observation", "state"), "_dynamic_obstacle_feature"], "_feature", del_keys=False), 
            TensorDictModule(nn.LayerNorm(200), ["_feature"], ["_feature"]),
        ).to(device)
        
        print(f"âœ… SACFeatureExtractor initialized on {device}")
    
    def forward(self, observation: Dict) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            observation: è§‚æµ‹å­—å…¸ï¼ŒåŒ…å« lidar, dynamic_obstacle, state
            
        Returns:
            feature: èåˆç‰¹å¾ [Batch, 200]
        """
        batch_size = observation["lidar"].shape[0]
        tensordict = TensorDict({"observation": observation}, batch_size=batch_size)
        tensordict = self.feature_extractor(tensordict)
        return tensordict["_feature"]


class ActorNetwork(nn.Module):
    """
    SAC Actor ç½‘ç»œ
    
    è¾“å‡º TanhNormal åˆ†å¸ƒçš„å‚æ•°ï¼ˆloc, scaleï¼‰
    """
    
    def __init__(self, obs_dim: int, action_dim: int, device: torch.device):
        super().__init__()
        self.device = device
        self.action_dim = action_dim
        
        # ç‰¹å¾æå–å™¨
        self.feature_extractor = SACFeatureExtractor(device)
        
        # Actor head
        self.actor = ProbabilisticActor(
            TensorDictSequential(
                TensorDictModule(nn.Sequential(
                    nn.Linear(200, 256),
                    nn.LeakyReLU(),
                    nn.LayerNorm(256),
                    nn.Linear(256, 256),
                    nn.LeakyReLU(),
                    nn.LayerNorm(256),
                ), in_keys=["_feature"], out_keys=["_feature_"]),
                TensorDictModule(GaussianActor(action_dim), in_keys=["_feature_"], out_keys=["loc", "scale"])
            ),
            in_keys=["loc", "scale"],
            out_keys=["action_normalized"], 
            distribution_class=TanhNormal,
            return_log_prob=True
        ).to(device)
        
        print(f"âœ… ActorNetwork initialized with action_dim={action_dim}")

    def forward(self, state: Dict) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            state: è§‚æµ‹å­—å…¸
            
        Returns:
            action_normalized: å½’ä¸€åŒ–åŠ¨ä½œ [Batch, action_dim]
            loc: å‡å€¼ [Batch, action_dim]
            scale: æ ‡å‡†å·® [Batch, action_dim]
        """
        tensordict = TensorDict({"observation": state}, batch_size=state["lidar"].shape[0])
        tensordict = self.feature_extractor.feature_extractor(tensordict)
        tensordict = self.actor(tensordict)
        return tensordict["action_normalized"], tensordict["loc"], tensordict["scale"]


class CriticNetwork(nn.Module):
    """
    SAC Critic ç½‘ç»œï¼ˆQ å‡½æ•°ï¼‰
    
    è¾“å…¥çŠ¶æ€å’ŒåŠ¨ä½œï¼Œè¾“å‡º Q å€¼
    """
    
    def __init__(self, obs_dim: int, action_dim: int, device: torch.device):
        super().__init__()
        self.device = device
        self.action_dim = action_dim
        
        # ç‰¹å¾æå–å™¨
        self.feature_extractor = SACFeatureExtractor(device)
        
        # Q ç½‘ç»œ
        self.qvalue = TensorDictSequential(
            CatTensors(["_feature", "action_normalized"], "_feature_action", del_keys=False),
            TensorDictModule(
                nn.Sequential(
                    nn.Linear(200 + action_dim, 256),
                    nn.LeakyReLU(),
                    nn.LayerNorm(256),
                    nn.Linear(256, 256),
                    nn.LeakyReLU(),
                    nn.LayerNorm(256),
                    nn.Linear(256, 1),
                ),
                in_keys=["_feature_action"],
                out_keys=["state_action_value"],
            ),
        ).to(device)
        
        print(f"âœ… CriticNetwork initialized with action_dim={action_dim}")
    
    def forward(self, s: Dict, a: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            s: è§‚æµ‹å­—å…¸
            a: åŠ¨ä½œ [Batch, action_dim]
            
        Returns:
            q: Q å€¼ [Batch, 1]
        """
        tensordict = TensorDict(
            {"observation": s, "action_normalized": a.squeeze(1)}, 
            batch_size=s["lidar"].shape[0]
        )
        tensordict = self.feature_extractor.feature_extractor(tensordict)
        q = self.qvalue(tensordict)["state_action_value"]
        return q


class SACModel(TensorDictModuleBase):
    """
    å®Œæ•´çš„ SAC æ¨¡å‹
    
    åŒ…å«ï¼š
    1. Actor ç½‘ç»œ
    2. åŒ Critic ç½‘ç»œï¼ˆQ1, Q2ï¼‰
    3. Target Critic ç½‘ç»œï¼ˆQ1_target, Q2_targetï¼‰
    4. Temperature å‚æ•°ï¼ˆalphaï¼‰
    5. ä¼˜åŒ–å™¨
    """
    
    def __init__(self, cfg, observation_spec, action_spec, device: torch.device):
        super().__init__()
        self.cfg = cfg
        self.obs_dim = observation_spec
        self.act_dim = action_spec
        self.device = device
        
        # æå– action_dim
        if hasattr(self.act_dim, "shape"):
            shape = tuple(self.act_dim.shape)
            self.act_dim = int(shape[-1]) if len(shape) > 0 else int(shape[0])
        else:
            self.act_dim = int(self.act_dim)
        
        # åˆå§‹åŒ–ç½‘ç»œ
        print("ğŸš€ Initializing SAC networks...")
        self.actor = ActorNetwork(self.obs_dim, self.act_dim, device).to(self.device)
        self.critic1 = CriticNetwork(self.obs_dim, self.act_dim, device).to(self.device)
        self.critic2 = CriticNetwork(self.obs_dim, self.act_dim, device).to(self.device)
        self.critic1_target = deepcopy(self.critic1)
        self.critic2_target = deepcopy(self.critic2)
        
        # Temperature å‚æ•°
        self.log_alpha = nn.Parameter(torch.log(torch.tensor(5.0, device=device)))
        self.alpha = self.log_alpha.exp().detach()
        self.target_entropy = -float(self.act_dim)
        
        # è¶…å‚æ•°
        self.gamma = getattr(cfg, 'gamma', 0.99)
        self.action_limit = getattr(cfg.actor, 'action_limit', 2.0)
        
        # ä¼˜åŒ–å™¨
        self.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=cfg.actor.learning_rate)
        self.critic1_optim = torch.optim.Adam(self.critic1.parameters(), lr=cfg.critic.learning_rate)
        self.critic2_optim = torch.optim.Adam(self.critic2.parameters(), lr=cfg.critic.learning_rate)
        self.alpha_optim = torch.optim.Adam([self.log_alpha], lr=cfg.alpha_learning_rate)
        
        # å‚æ•°åˆå§‹åŒ–
        self._init_weights()
        
        print(f"âœ… SACModel initialized with action_dim={self.act_dim}")
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–ï¼ˆæ­£äº¤åˆå§‹åŒ–ï¼‰"""
        def init_(module):
            from torch.nn.parameter import UninitializedParameter
            if isinstance(module, (nn.Linear, nn.Conv2d)):
                w = getattr(module, "weight", None)
                b = getattr(module, "bias", None)
                if w is None or isinstance(w, UninitializedParameter):
                    return
                nn.init.orthogonal_(module.weight, 0.01)
                if b is not None and not isinstance(b, UninitializedParameter):
                    nn.init.constant_(module.bias, 0.0)
        
        self.actor.apply(init_)
        self.critic1.apply(init_)
        self.critic2.apply(init_)
        self.critic1_target.apply(init_)
        self.critic2_target.apply(init_)
        print("âœ… Weights initialized with orthogonal initialization")
    
    def get_action(self, state: Dict, deterministic: bool = True) -> torch.Tensor:
        """
        è·å–åŠ¨ä½œï¼ˆæ¨ç†æ¥å£ï¼‰
        
        Args:
            state: è§‚æµ‹å­—å…¸
            deterministic: æ˜¯å¦ç¡®å®šæ€§è¾“å‡ºï¼ˆä½¿ç”¨å‡å€¼ï¼‰
            
        Returns:
            action: åŠ¨ä½œå¼ é‡
        """
        if deterministic:
            with torch.no_grad():
                action, mu, log_std = self.actor(state)
        else:
            action, mu, log_std = self.actor(state)
        return action
    
    def __call__(self, td: TensorDict) -> TensorDict:
        """
        æ¨¡å‹è°ƒç”¨ï¼ˆç¯å¢ƒäº¤äº’æ¥å£ï¼‰
        
        Args:
            td: TensorDictï¼ŒåŒ…å«è§‚æµ‹
            
        Returns:
            td: æ·»åŠ äº†åŠ¨ä½œçš„ TensorDict
        """
        td = td.to(self.device)
        action_n, mu, log_std = self.actor(td["agents", "observation"])
        actions_world = self.actions_to_world(action_n, td).squeeze(-1)
        td["agents", "action"] = actions_world
        td["agents", "action_normalized"] = action_n
        return td
    
    def train_step(self, replay_buffer, batch_size: int, tau: float = 0.005) -> Dict[str, float]:
        """
        SAC è®­ç»ƒæ­¥éª¤
        
        Args:
            replay_buffer: ç»éªŒå›æ”¾ç¼“å†²åŒº
            batch_size: æ‰¹å¤§å°
            tau: è½¯æ›´æ–°ç³»æ•°
            
        Returns:
            loss_info: æŸå¤±ä¿¡æ¯å­—å…¸
        """
        train_tds = []
        
        for _ in range(self.cfg.num_minibatches):
            batch = replay_buffer.sample(batch_size).to(self.device)
            states = batch['agents', 'observation'].squeeze(-1).to(self.device)
            actions = batch['agents', 'action_normalized'].to(self.device)
            rewards = batch['next', 'agents', 'reward'].squeeze(1).to(self.device)
            next_states = batch['next', 'agents', 'observation'].squeeze(-1).to(self.device)
            dones = batch['next', 'terminated'].squeeze(-1).to(torch.bool).float().to(self.device)
            
            # ============ æ›´æ–° Critics ============
            with torch.no_grad():
                _, next_mu, next_log_std = self.actor(next_states)
                next_std = next_log_std.exp().clamp(min=1e-6)
                next_normal = torch.distributions.Normal(next_mu, next_std)
                next_x_t = next_normal.rsample()
                next_actions = torch.tanh(next_x_t)
                
                next_log_probs = next_normal.log_prob(next_x_t) - torch.log(1 - next_actions.pow(2) + 1e-6)
                next_log_probs = next_log_probs.sum(-1)
                
                next_q1 = self.critic1_target(next_states, next_actions).squeeze(-1)
                next_q2 = self.critic2_target(next_states, next_actions).squeeze(-1)
                next_q = torch.min(next_q1, next_q2)
                
                target_q = rewards + self.gamma * (1 - dones) * (next_q - self.alpha * next_log_probs)
            
            q1 = self.critic1(states, actions).squeeze(-1)
            q2 = self.critic2(states, actions).squeeze(-1)
            
            critic1_loss = F.mse_loss(q1, target_q)
            critic2_loss = F.mse_loss(q2, target_q)
            
            self.critic1_optim.zero_grad()
            critic1_loss.backward()
            self.critic1_optim.step()
            
            self.critic2_optim.zero_grad()
            critic2_loss.backward()
            self.critic2_optim.step()
            
            # ============ æ›´æ–° Actor ============
            _, mu, log_std = self.actor(states)
            std = log_std.exp().clamp(min=1e-6)
            normal = torch.distributions.Normal(mu, std)
            x_t = normal.rsample()
            actions_new = torch.tanh(x_t)
            
            log_probs = normal.log_prob(x_t) - torch.log(1 - actions_new.pow(2) + 1e-6)
            log_probs = log_probs.sum(-1)
            
            q1_new = self.critic1(states, actions_new).squeeze(-1)
            q2_new = self.critic2(states, actions_new).squeeze(-1)
            q_min = torch.min(q1_new, q2_new)
            
            actor_loss = (self.alpha * log_probs - q_min).mean()
            
            self.actor_optim.zero_grad()
            actor_loss.backward()
            self.actor_optim.step()
            
            # ============ æ›´æ–° Temperature ============
            alpha_loss = -(self.log_alpha * (log_probs + self.target_entropy).detach()).mean()
            
            self.alpha_optim.zero_grad()
            alpha_loss.backward()
            self.alpha_optim.step()
            self.alpha = self.log_alpha.exp()
            
            # ============ è½¯æ›´æ–° Target Networks ============
            self._soft_update(self.critic1_target, self.critic1, tau)
            self._soft_update(self.critic2_target, self.critic2, tau)
            
            # è®°å½•è®­ç»ƒä¿¡æ¯
            train_td = TensorDict({
                "actor_loss": actor_loss.item(),
                "q1_loss": critic1_loss.item(),
                "q2_loss": critic2_loss.item(),
                "alpha_loss": alpha_loss.item(),
                "alpha": self.alpha.item(),
                "actor_lp": log_probs.mean(),
                "q1": q1.mean(),
                "q_min": q_min.mean(),
                "q1_new": q1_new.mean(),
                "td_error": (q1_new - target_q).mean(),
                "td_error_target": (q1 - target_q).mean(),
            }, [])
            train_tds.append(train_td)
        
        loss_infos = torch.stack(train_tds).to_tensordict()
        loss_infos = loss_infos.apply(torch.mean, batch_size=[])
        return {k: v.mean().item() for k, v in loss_infos.items()}
    
    def actions_to_world(self, actions: torch.Tensor, tensordict: TensorDict) -> torch.Tensor:
        """å°†åŠ¨ä½œä»å±€éƒ¨åæ ‡ç³»è½¬æ¢åˆ°ä¸–ç•Œåæ ‡ç³»"""
        actions = actions * self.cfg.actor.action_limit
        actions_world = vec_to_world(actions, tensordict["agents", "observation", "direction"])
        return actions_world
    
    def _soft_update(self, target: nn.Module, source: nn.Module, tau: float):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        for target_param, source_param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(tau * source_param.data + (1 - tau) * target_param.data)


class SACModelManager:
    """
    SAC æ¨¡å‹ç®¡ç†å™¨
    
    æä¾›ç»Ÿä¸€çš„é…ç½®å’ŒçŠ¶æ€ç®¡ç†æ¥å£ï¼ŒåŒ…æ‹¬ï¼š
    1. æ¨¡å‹åˆ›å»ºå’Œåˆå§‹åŒ–
    2. æ£€æŸ¥ç‚¹ä¿å­˜å’ŒåŠ è½½
    3. wandb é›†æˆ
    4. è®­ç»ƒæ¨¡å¼åˆ‡æ¢
    """
    
    def __init__(self, cfg, observation_spec, action_spec, device: torch.device):
        """
        åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        
        Args:
            cfg: é…ç½®å¯¹è±¡
            observation_spec: è§‚æµ‹ç©ºé—´è§„æ ¼
            action_spec: åŠ¨ä½œç©ºé—´è§„æ ¼
            device: è®¾å¤‡ï¼ˆcpu/cudaï¼‰
        """
        self.cfg = cfg
        self.device = device
        
        # åˆ›å»ºæ¨¡å‹
        self.model = SACModel(cfg, observation_spec, action_spec, device)
        
        print(f"âœ… SACModelManager initialized on {device}")
    
    def save_checkpoint(self, path: Union[str, Path], step: int, **extra_info):
        """
        ä¿å­˜æ£€æŸ¥ç‚¹
        
        Args:
            path: ä¿å­˜è·¯å¾„
            step: è®­ç»ƒæ­¥æ•°
            **extra_info: é¢å¤–ä¿¡æ¯ï¼ˆå¦‚ replay_buffer çŠ¶æ€ï¼‰
        """
        path = Path(path)
        path.parent.mkdir(parents=True, exist_ok=True)
        
        checkpoint = {
            'step': step,
            'model_state_dict': self.model.state_dict(),
            'actor_optim_state_dict': self.model.actor_optim.state_dict(),
            'critic1_optim_state_dict': self.model.critic1_optim.state_dict(),
            'critic2_optim_state_dict': self.model.critic2_optim.state_dict(),
            'alpha_optim_state_dict': self.model.alpha_optim.state_dict(),
            'log_alpha': self.model.log_alpha.data,
            'cfg': dict(self.cfg),
            **extra_info
        }
        
        torch.save(checkpoint, path)
        print(f"ğŸ’¾ Checkpoint saved to {path}")
        
        # ä¸Šä¼ åˆ° wandb
        if WANDB_AVAILABLE and wandb.run is not None:
            artifact = wandb.Artifact(
                name=f"sac-model-step-{step}",
                type="model",
                metadata={"step": step}
            )
            artifact.add_file(str(path))
            wandb.log_artifact(artifact)
            print(f"â˜ï¸  Checkpoint uploaded to wandb")
    
    def load_checkpoint(self, path: Union[str, Path], load_optimizers: bool = True) -> Dict:
        """
        åŠ è½½æ£€æŸ¥ç‚¹
        
        Args:
            path: æ£€æŸ¥ç‚¹è·¯å¾„
            load_optimizers: æ˜¯å¦åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
            
        Returns:
            checkpoint: æ£€æŸ¥ç‚¹å­—å…¸
        """
        path = Path(path)
        if not path.exists():
            raise FileNotFoundError(f"Checkpoint not found: {path}")
        
        checkpoint = torch.load(path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        if load_optimizers:
            self.model.actor_optim.load_state_dict(checkpoint['actor_optim_state_dict'])
            self.model.critic1_optim.load_state_dict(checkpoint['critic1_optim_state_dict'])
            self.model.critic2_optim.load_state_dict(checkpoint['critic2_optim_state_dict'])
            self.model.alpha_optim.load_state_dict(checkpoint['alpha_optim_state_dict'])
        
        self.model.log_alpha.data = checkpoint['log_alpha']
        self.model.alpha = self.model.log_alpha.exp().detach()
        
        print(f"âœ… Checkpoint loaded from {path} (step={checkpoint['step']})")
        return checkpoint
    
    def set_training_mode(self, mode: bool):
        """è®¾ç½®è®­ç»ƒ/è¯„ä¼°æ¨¡å¼"""
        self.model.train(mode)
    
    def get_action(self, state: Dict, deterministic: bool = True) -> torch.Tensor:
        """è·å–åŠ¨ä½œï¼ˆæ¨ç†æ¥å£ï¼‰"""
        return self.model.get_action(state, deterministic)
    
    def __call__(self, td: TensorDict) -> TensorDict:
        """æ¨¡å‹è°ƒç”¨ï¼ˆç¯å¢ƒäº¤äº’æ¥å£ï¼‰"""
        return self.model(td)
    
    def train_step(self, replay_buffer, batch_size: int, tau: float = 0.005) -> Dict[str, float]:
        """è®­ç»ƒæ­¥éª¤"""
        return self.model.train_step(replay_buffer, batch_size, tau)
    
    def get_model_info(self) -> Dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'action_dim': self.model.act_dim,
            'gamma': self.model.gamma,
            'action_limit': self.model.action_limit,
            'alpha': self.model.alpha.item(),
            'target_entropy': self.model.target_entropy,
            'num_actor_params': sum(p.numel() for p in self.model.actor.parameters()),
            'num_critic_params': sum(p.numel() for p in self.model.critic1.parameters()),
        }


# ä¾¿æ·å‡½æ•°
def create_sac_model(cfg, observation_spec, action_spec, device: torch.device) -> SACModelManager:
    """
    åˆ›å»º SAC æ¨¡å‹ç®¡ç†å™¨ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        cfg: é…ç½®å¯¹è±¡
        observation_spec: è§‚æµ‹ç©ºé—´è§„æ ¼
        action_spec: åŠ¨ä½œç©ºé—´è§„æ ¼
        device: è®¾å¤‡
        
    Returns:
        model_manager: SAC æ¨¡å‹ç®¡ç†å™¨
    """
    return SACModelManager(cfg, observation_spec, action_spec, device)
