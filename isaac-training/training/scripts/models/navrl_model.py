'''
Author: zdytim zdytim@foxmail.com
Date: 2026-01-05 22:20:12
LastEditors: zdytim zdytim@foxmail.com
LastEditTime: 2026-01-07 00:16:15
FilePath: /NavRL/isaac-training/training/scripts/models/navrl_model.py
Description: è¿™æ˜¯é»˜è®¤è®¾ç½®,è¯·è®¾ç½®`customMade`, æ‰“å¼€koroFileHeaderæŸ¥çœ‹é…ç½® è¿›è¡Œè®¾ç½®: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE
'''
"""
NavRL Model Manager
===================
æŠ½è±¡çš„æ¨¡å‹ç®¡ç†æ¨¡å—ï¼Œå°†PPO-ViTæ¨¡å‹çš„æ ¸å¿ƒé€»è¾‘ä»è®­ç»ƒè„šæœ¬ä¸­åˆ†ç¦»å‡ºæ¥ã€‚
æä¾›ç»Ÿä¸€çš„æ¨¡å‹åˆ›å»ºã€åŠ è½½ã€ä¿å­˜å’Œé…ç½®ç®¡ç†æ¥å£ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. SharedFeatureExtractor: ViT-basedç‰¹å¾æå–å™¨ï¼Œæ”¯æŒå‚æ•°å†»ç»“/è§£å†»
2. NavRLModel: å®Œæ•´çš„PPO-ViTæ¨¡å‹ï¼ŒåŒ…å«Actor/Critic
3. ModelManager: æ¨¡å‹ç®¡ç†å™¨ï¼Œæä¾›ç»Ÿä¸€çš„é…ç½®å’ŒçŠ¶æ€ç®¡ç†æ¥å£
4. æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒã€åˆ†ç»„ä¼˜åŒ–å™¨ã€å‚æ•°ç®¡ç†ç­‰é«˜çº§åŠŸèƒ½

ä½œè€…: NavRL Team
æ—¥æœŸ: 2026å¹´1æœˆ5æ—¥
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from tensordict.tensordict import TensorDict
from tensordict.nn import TensorDictModuleBase, TensorDictSequential, TensorDictModule
from einops.layers.torch import Rearrange
from torchrl.modules import ProbabilisticActor
from torch.cuda.amp import autocast, GradScaler
import os
import tempfile
import json
from pathlib import Path
import torch.nn.utils.spectral_norm as spectral_norm
from typing import Dict, Optional, Tuple, List, Union

# wandb integration for model management
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("âš ï¸  wandb not available. Model uploading will be disabled.")

# å¯¼å…¥é¡¹ç›®ä¾èµ–
from utils import ValueNorm, make_mlp, IndependentNormal, Actor, GAE, make_batch, IndependentBeta, BetaActor, vec_to_world
from VIT import VIT


class SharedFeatureExtractor(nn.Module):
    """
    å…±äº«ç‰¹å¾æå–å™¨
    
    åŠŸèƒ½ï¼š
    1. ViT backboneç”¨äºè§†è§‰ç‰¹å¾æå–
    2. åŠ¨æ€éšœç¢ç‰©ç¼–ç å™¨
    3. çŠ¶æ€ç¼–ç å™¨
    4. ç‰¹å¾èåˆç½‘ç»œ
    5. æ”¯æŒé€‰æ‹©æ€§å‚æ•°å†»ç»“/è§£å†»
    """
    
    def __init__(self, device: torch.device, pretrained_checkpoint_path: Optional[str] = None, input_size: tuple = (224, 224)):
        super().__init__()
        self.device = device
        self.input_size = input_size
        
        # ViT Backbone with dynamic sizing
        print(f"ğŸ§  Initializing ViT backbone for {input_size}...")
        self.vit = VIT(input_size=input_size).to(device)
        
        # å‚æ•°ç®¡ç†
        self._load_vit_weights(pretrained_checkpoint_path)
        self._setup_parameter_training()
        
        # å…¶ä»–ç»„ä»¶åˆå§‹åŒ–
        self._init_other_modules()
        
        print(f"âœ… SharedFeatureExtractor initialized on {device} for {input_size}")

    def forward(self, camera: torch.Tensor, dynamic_obstacle: torch.Tensor, state: torch.Tensor) -> torch.Tensor:
        # , hide: torch.Tensor
        """
        å‰å‘ä¼ æ’­
        
        Args:
            camera: ç›¸æœºè¾“å…¥ [Batch, 1, H, W]
            dynamic_obstacle: åŠ¨æ€éšœç¢ç‰© [Batch, C, W, H]  
            state: çŠ¶æ€ä¿¡æ¯ [Batch, 8]
            
        Returns:
            latent: èåˆç‰¹å¾ [Batch, 128]
        """
        # 1. ç›¸æœºæ•°æ®é¢„å¤„ç†
        camera = torch.nan_to_num(camera, nan=10.0, posinf=10.0, neginf=0.0)
        camera = camera.clamp(0.0, 10.0)
        
        assert camera.dim() == 4, f"Camera input should be 4D [B,C,H,W], got {camera.shape}"
        assert camera.shape[1] == 1, f"Camera input should be grayscale (1 channel), got {camera.shape[1]} channels"
        
        # å½’ä¸€åŒ–å¤„ç†
        x = (camera / 10 if camera.max() > 1.1 else camera)
        
        # 2. ViTç‰¹å¾æå–ï¼ˆå†»ç»“çŠ¶æ€ï¼‰
        # with torch.no_grad():
        v_feat = self.vit(x)  # [Batch, 512]
        
        # 3. åŠ¨æ€éšœç¢ç‰©ç‰¹å¾æå–
        d_feat = self.dyn_ext(dynamic_obstacle)  # [Batch, 64]
        
        # 4. çŠ¶æ€ç‰¹å¾æå–
        s_feat = self.state_ext(state)  # [Batch, 64]
        
        # 5. ç‰¹å¾èåˆ
        combined = torch.cat([v_feat, d_feat, state], dim=-1)  # [Batch, 584]
        # if len(hide) == 1:
        #     latent, h = self.lstm(combined, hide)
        # else:
        #     latent, h = self.lstm(combined)
        # latent = self.nn_fc2(latent)
        latent = self.fusion_mlp(combined)  # [Batch, 128]

        return latent

    def _load_vit_weights(self, checkpoint_path: Optional[str]) -> None:
        """åŠ è½½ViTé¢„è®­ç»ƒæƒé‡"""
        if checkpoint_path is None:
            print("âš ï¸  No ViT checkpoint provided, using random initialization")
            return
            
        try:
            print(f"ğŸ”„ Loading ViT weights from: {checkpoint_path}")
            full_model_checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            # è¿‡æ»¤ViTç›¸å…³æƒé‡
            vit_state_dict = {}
            ignored_keys = []
            for k, v in full_model_checkpoint.items():
                if k.startswith('encoder_blocks') or k.startswith('decoder') or \
                   k.startswith('up_sample') or k.startswith('pxShuffle') or k.startswith('down_sample'):
                    vit_state_dict[k] = v
                else:
                    ignored_keys.append(k)
            
            # åŠ è½½æƒé‡
            missing_keys, unexpected_keys = self.vit.load_state_dict(vit_state_dict, strict=False)
            
            print(f"âœ… ViT weights loaded successfully!")
            print(f"   - Loaded keys: {len(vit_state_dict)}")
            print(f"   - Ignored keys: {len(ignored_keys)}")
            if missing_keys:
                print(f"   - Missing keys: {len(missing_keys)} (will use random init)")
            if unexpected_keys:
                print(f"   - Unexpected keys: {len(unexpected_keys)}")
                
        except Exception as e:
            print(f"âŒ Failed to load ViT weights: {e}")
            print("   - Using random initialization")
    
    def _setup_parameter_training(self) -> None:
        """è®¾ç½®å‚æ•°è®­ç»ƒçŠ¶æ€ï¼šå†»ç»“encoderï¼Œè§£å†»decoder"""
        # 1. å…¨éƒ¨å†»ç»“
        self._freeze_all_vit_parameters()
        
        # 2. é€‰æ‹©æ€§è§£å†»
        self._unfreeze_decoder_modules()
    
    def _freeze_all_vit_parameters(self) -> None:
        """å†»ç»“æ‰€æœ‰ViTå‚æ•°"""
        frozen_params = 0
        for param in self.vit.parameters():
            param.requires_grad = False
            frozen_params += param.numel()
        
        print(f"â„ï¸  Frozen all ViT parameters: {frozen_params:,}")
    
    def _unfreeze_decoder_modules(self) -> None:
        """è§£å†»decoderæ¨¡å—è¿›è¡Œfine-tuning"""
        decoder_modules = ['encoder_blocks', 'decoder', 'up_sample', 'pxShuffle']
        unfrozen_params = 0
        
        print("ğŸ”“ Unfreezing ViT decoder modules for fine-tuning:")
        for module_name in decoder_modules:
            if hasattr(self.vit, module_name):
                module = getattr(self.vit, module_name)
                for param in module.parameters():
                    param.requires_grad = True
                    unfrozen_params += param.numel()
                print(f"   - {module_name}: âœ… unfrozen")
            else:
                print(f"   - {module_name}: âš ï¸ not found in model")
        
        print(f"   - Total decoder parameters: {unfrozen_params:,}")
    
    def _init_other_modules(self) -> None:
        """åˆå§‹åŒ–å…¶ä»–ç½‘ç»œæ¨¡å—"""
        # åŠ¨æ€éšœç¢ç‰©æå–å™¨
        self.dyn_ext = nn.Sequential(
            Rearrange("n c w h -> n (c w h)"),
            nn.Linear(50, 128), nn.LeakyReLU(), nn.LayerNorm(128),
            nn.Linear(128, 64), nn.LeakyReLU(), nn.LayerNorm(64),
        ).to(self.device)

        # çŠ¶æ€ç‰¹å¾æå–å™¨
        self.state_ext = nn.Sequential(
            nn.Linear(8, 64), nn.LeakyReLU(), nn.LayerNorm(64),
            nn.Linear(64, 64), nn.LeakyReLU(), nn.LayerNorm(64),
        ).to(self.device)

        # èåˆç½‘ç»œ
        # self.lstm = (nn.LSTM(input_size=584, hidden_size=128,num_layers=3, dropout=0.1)).to(self.device)
        # self.nn_fc2 = spectral_norm(nn.Linear(128, 3)).to(self.device)
        self.fusion_mlp = nn.Sequential(
            nn.Linear(584, 512), nn.LeakyReLU(), nn.LayerNorm(512),
            nn.Linear(512, 256), nn.LeakyReLU(), nn.LayerNorm(256),
            nn.Linear(256, 128), nn.LeakyReLU(), nn.LayerNorm(128)
        ).to(self.device)
    
    # === é«˜çº§å‚æ•°ç®¡ç†æ¥å£ ===
    
    def freeze_vit_encoder(self) -> None:
        """å†»ç»“ViT encoderï¼ˆä¿æŒdecoderè§£å†»çŠ¶æ€ï¼‰"""
        encoder_modules = ['encoder_blocks']
        frozen_params = 0
        
        for module_name in encoder_modules:
            if hasattr(self.vit, module_name):
                module = getattr(self.vit, module_name)
                for param in module.parameters():
                    param.requires_grad = False
                    frozen_params += param.numel()
        
        print(f"â„ï¸  Frozen encoder: {frozen_params:,} parameters")
    
    def unfreeze_all_vit(self) -> None:
        """è§£å†»æ‰€æœ‰ViTå‚æ•°ï¼ˆç”¨äºå®Œå…¨fine-tuningï¼‰"""
        unfrozen_params = 0
        for param in self.vit.parameters():
            param.requires_grad = True
            unfrozen_params += param.numel()
        
        print(f"ğŸ”“ Unfrozen all ViT: {unfrozen_params:,} parameters")
    
    def get_parameter_groups(self) -> Dict[str, List[torch.nn.Parameter]]:
        """
        è·å–å‚æ•°ç»„ç”¨äºä¼˜åŒ–å™¨é…ç½®
        
        Returns:
            å‚æ•°ç»„å­—å…¸ï¼ŒåŒ…å«ViTå’Œå…¶ä»–æ¨¡å—çš„å‚æ•°
        """
        vit_params = [p for p in self.vit.parameters() if p.requires_grad]
        other_params = []
        
        for module in [self.dyn_ext, self.state_ext, self.fusion_mlp]:
            other_params.extend([p for p in module.parameters() if p.requires_grad])
        
        return {
            'vit_decoder': vit_params,
            'other_modules': other_params
        }
    
    def get_parameter_stats(self) -> Dict[str, int]:
        """è·å–å‚æ•°ç»Ÿè®¡ä¿¡æ¯"""
        param_groups = self.get_parameter_groups()
        return {
            'vit_decoder_count': sum(p.numel() for p in param_groups['vit_decoder']),
            'other_modules_count': sum(p.numel() for p in param_groups['other_modules']),
            'total_trainable': sum(p.numel() for p in self.parameters() if p.requires_grad)
        }


class NavRLModel(TensorDictModuleBase):
    """
    NavRLå®Œæ•´æ¨¡å‹
    
    åŒ…å«ï¼š
    1. SharedFeatureExtractor: ç‰¹å¾æå–
    2. Actor Head: ç­–ç•¥ç½‘ç»œ
    3. Critic Head: ä»·å€¼ç½‘ç»œ
    4. ä¼˜åŒ–å™¨å’Œè®­ç»ƒå·¥å…·
    """
    
    def __init__(self, cfg, observation_spec, action_spec, device: torch.device):
        super().__init__()
        self.cfg = cfg
        self.device = device
        
        # å¤„ç†action_spec
        if hasattr(action_spec, "shape"):
            shape = tuple(action_spec.shape)
            self.action_dim = int(shape[-1]) if len(shape) > 0 else int(shape[0])
        else:
            self.action_dim = int(action_spec)
        
        # åˆå§‹åŒ–ç½‘ç»œç»„ä»¶
        self._init_networks(observation_spec)
        
        # åˆå§‹åŒ–è®­ç»ƒå·¥å…·
        self._init_training_tools()
        
        print(f"âœ… NavRLModel initialized with {self.action_dim}D actions")
    
    def _init_networks(self, observation_spec) -> None:
        """åˆå§‹åŒ–ç½‘ç»œç»„ä»¶"""
        # 1. ä»é…ç½®è·å–è¾“å…¥å°ºå¯¸
        input_size = getattr(self.cfg.feature_extractor, 'input_size', (224, 224))
        if isinstance(input_size, (list, tuple)) and len(input_size) == 2:
            input_size = tuple(input_size)
        else:
            input_size = (224, 224)  # é»˜è®¤å€¼
            
        # 2. å…±äº«ç‰¹å¾æå–å™¨
        pretrained_path = getattr(self.cfg.feature_extractor, 'pretrained_checkpoint_path', None)
        self.shared_features = SharedFeatureExtractor(
            self.device, 
            pretrained_path,
            input_size=input_size  # ä¼ é€’é…ç½®çš„è¾“å…¥å°ºå¯¸
        )
        
        # ... å…¶ä½™åˆå§‹åŒ–ä»£ç ä¿æŒä¸å˜ ...
        
        print(f"âœ… NavRLModel initialized with input size: {input_size}")

        # 2. Actor Head
        self.actor_head = ProbabilisticActor(
            TensorDictModule(
                BetaActor(self.action_dim), 
                in_keys=["_latent"], 
                out_keys=["alpha", "beta"]
            ),
            in_keys=["alpha", "beta"],
            out_keys=[("agents", "action_normalized")],
            distribution_class=IndependentBeta,
            return_log_prob=True
        ).to(self.device)

        # 3. Critic Head
        self.critic_head = nn.Linear(128, 1).to(self.device)
        
        # 4. åˆå§‹åŒ–ç½‘ç»œæƒé‡
        self._init_dummy_forward(observation_spec)
        self._init_weights()
    
    def _init_dummy_forward(self, observation_spec) -> None:
        """æ‰§è¡Œdummy forwardä»¥åˆå§‹åŒ–LazyLinear"""
        dummy_tensordict = observation_spec.zero().unsqueeze(0).to(self.device).reshape(-1)
        
        with torch.no_grad():
            latent = self.shared_features(
                dummy_tensordict["agents", "observation", "camera"],
                dummy_tensordict["agents", "observation", "dynamic_obstacle"],
                dummy_tensordict["agents", "observation", "state"]
            )
            dummy_tensordict.set("_latent", latent)
            self.actor_head(dummy_tensordict)
    
    def _init_weights(self) -> None:
        """åˆå§‹åŒ–Actorå’ŒCriticæƒé‡"""
        def init_(m):
            if isinstance(m, nn.Linear):
                weight = getattr(m, "weight", None)
                if isinstance(weight, torch.nn.parameter.UninitializedParameter):
                    return
                nn.init.orthogonal_(weight, 0.01)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.)
        
        print("ğŸ”„ Initializing Actor and Critic weights...")
        self.actor_head.apply(init_)
        self.critic_head.apply(init_)
        print("   - Network weights initialized âœ…")
    
    def _init_training_tools(self) -> None:
        """åˆå§‹åŒ–è®­ç»ƒå·¥å…·"""
        # 1. ä¼˜åŒ–å™¨ï¼ˆåˆ†ç»„å­¦ä¹ ç‡ï¼‰
        self.optimizer = self._create_grouped_optimizer()
        
        # 2. è®­ç»ƒå·¥å…·
        self.gae = GAE(0.99, 0.95)
        self.value_norm = ValueNorm(1).to(self.device)
        self.critic_loss_fn = nn.HuberLoss(delta=10)
        
        # 3. æ··åˆç²¾åº¦è®­ç»ƒ
        self.use_amp = getattr(self.cfg, 'use_amp', True)
        self.scaler = GradScaler() if self.use_amp else None
        
        if self.use_amp:
            print("âœ… Mixed Precision (AMP) enabled - saving 30-50% GPU memory")
    
    def _create_grouped_optimizer(self) -> torch.optim.Optimizer:
        """åˆ›å»ºåˆ†ç»„ä¼˜åŒ–å™¨"""
        param_groups = []
        
        # 1. ViT decoderå‚æ•°ï¼ˆä½å­¦ä¹ ç‡ï¼‰
        feature_groups = self.shared_features.get_parameter_groups()
        vit_params = feature_groups['vit_decoder']
        
        if vit_params:
            decoder_lr = self.cfg.actor.learning_rate * 0.1  # 10å€é™ä½
            param_groups.append({
                'params': vit_params, 
                'lr': decoder_lr,
                'name': 'vit_decoder'
            })
            print(f"ğŸ“š ViT decoder: {sum(p.numel() for p in vit_params):,} params @ lr={decoder_lr}")
        
        # 2. å…¶ä»–æ¨¡å—å‚æ•°ï¼ˆæ­£å¸¸å­¦ä¹ ç‡ï¼‰
        other_params = feature_groups['other_modules']
        other_params.extend([p for p in self.actor_head.parameters() if p.requires_grad])
        other_params.extend([p for p in self.critic_head.parameters() if p.requires_grad])
        
        param_groups.append({
            'params': other_params, 
            'lr': self.cfg.actor.learning_rate,
            'name': 'task_specific'
        })
        
        print(f"ğŸ¯ Task-specific: {sum(p.numel() for p in other_params):,} params @ lr={self.cfg.actor.learning_rate}")
        print(f"ğŸ“Š Total trainable: {sum(p.numel() for p in vit_params + other_params):,} parameters")
        
        return torch.optim.Adam(param_groups)
    
    def __call__(self, tensordict: TensorDict) -> TensorDict:
        """æ¨ç†æ¨¡å¼ï¼šç”¨äºç¯å¢ƒäº¤äº’"""
        # 1. ç‰¹å¾æå–
        latent = self.shared_features(
            tensordict["agents", "observation", "camera"],
            tensordict["agents", "observation", "dynamic_obstacle"],
            tensordict["agents", "observation", "state"]
        )
        tensordict.set("_latent", latent)
        
        # 2. ç­–ç•¥é‡‡æ ·
        self.actor_head(tensordict)
        
        # 3. ä»·å€¼ä¼°è®¡
        value = self.critic_head(latent)
        tensordict.set("state_value", value)

        # 4. åæ ‡è½¬æ¢ (Local -> World)
        actions = (2 * tensordict["agents", "action_normalized"] * self.cfg.actor.action_limit) - self.cfg.actor.action_limit
        actions_world = vec_to_world(actions, tensordict["agents", "observation", "direction"])
        tensordict["agents", "action"] = actions_world
        
        return tensordict
    
    def get_model_info(self) -> Dict[str, Union[int, str]]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        feature_stats = self.shared_features.get_parameter_stats()
        
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'frozen_parameters': total_params - trainable_params,
            'vit_decoder_params': feature_stats['vit_decoder_count'],
            'other_modules_params': feature_stats['other_modules_count'],
            'action_dim': self.action_dim,
            'device': str(self.device),
            'use_amp': self.use_amp
        }


class ModelManager:
    """
    æ¨¡å‹ç®¡ç†å™¨
    
    æä¾›ç»Ÿä¸€çš„æ¨¡å‹åˆ›å»ºã€åŠ è½½ã€ä¿å­˜å’Œé…ç½®ç®¡ç†æ¥å£
    """
    
    def __init__(self, cfg, observation_spec, action_spec, device: torch.device):
        self.cfg = cfg
        self.observation_spec = observation_spec
        self.action_spec = action_spec
        self.device = device
        
        # åˆ›å»ºæ¨¡å‹
        self.model = NavRLModel(cfg, observation_spec, action_spec, device)
        
        print("ğŸ‰ ModelManager initialized successfully")
    
    def load_checkpoint(self, checkpoint_path: str, load_optimizer: bool = True) -> bool:
        """
        åŠ è½½å®Œæ•´æ£€æŸ¥ç‚¹
        
        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
            load_optimizer: æ˜¯å¦åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
            
        Returns:
            åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            print(f"ğŸ”„ Loading checkpoint: {checkpoint_path}")
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            # åŠ è½½æ¨¡å‹çŠ¶æ€
            if 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
            elif 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict'] 
            else:
                state_dict = checkpoint
            
            # ç»Ÿè®¡åŠ è½½ç»“æœ
            loaded_stats = self._load_model_state(state_dict)
            
            # åŠ è½½è®­ç»ƒçŠ¶æ€
            if load_optimizer and 'optimizer_state_dict' in checkpoint:
                try:
                    self.model.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                    print("   ğŸ“ˆ Optimizer state: âœ… loaded")
                except Exception as e:
                    print(f"   ğŸ“ˆ Optimizer state: âŒ failed ({e})")
            
            if 'value_norm_state' in checkpoint:
                try:
                    self.model.value_norm.load_state_dict(checkpoint['value_norm_state'])
                    print("   ğŸ“Š Value normalization: âœ… loaded")
                except Exception as e:
                    print(f"   ğŸ“Š Value normalization: âŒ failed ({e})")
            
            return True
            
        except Exception as e:
            print(f"âŒ Failed to load checkpoint: {e}")
            return False
    
    def _load_model_state(self, state_dict: Dict[str, torch.Tensor]) -> Dict[str, int]:
        """åŠ è½½æ¨¡å‹çŠ¶æ€å¹¶ç»Ÿè®¡ç»“æœ"""
        loaded_stats = {
            'shared_features_vit': 0,
            'shared_features_other': 0,
            'actor_head': 0, 
            'critic_head': 0,
            'skipped': 0
        }
        
        current_state = self.model.state_dict()
        matched_params = {}
        
        for name, param in state_dict.items():
            if name in current_state:
                if current_state[name].shape == param.shape:
                    matched_params[name] = param
                    
                    # åˆ†ç±»ç»Ÿè®¡
                    if name.startswith('shared_features.vit.'):
                        loaded_stats['shared_features_vit'] += 1
                    elif name.startswith('shared_features.'):
                        loaded_stats['shared_features_other'] += 1
                    elif name.startswith('actor_head.'):
                        loaded_stats['actor_head'] += 1
                    elif name.startswith('critic_head.'):
                        loaded_stats['critic_head'] += 1
                else:
                    loaded_stats['skipped'] += 1
            else:
                loaded_stats['skipped'] += 1
        
        # åŠ è½½åŒ¹é…çš„æƒé‡
        self.model.load_state_dict(matched_params, strict=False)
        
        # æ‰“å°ç»Ÿè®¡ç»“æœ
        total_loaded = sum(v for k, v in loaded_stats.items() if k != 'skipped')
        print(f"âœ… Loaded {total_loaded} parameters:")
        
        if loaded_stats['shared_features_vit'] > 0:
            print(f"   ğŸ§  ViT backbone: {loaded_stats['shared_features_vit']} parameters")
        if loaded_stats['shared_features_other'] > 0:
            print(f"   âš™ï¸  Other features: {loaded_stats['shared_features_other']} parameters")
        if loaded_stats['actor_head'] > 0:
            print(f"   ğŸ¯ Actor head: {loaded_stats['actor_head']} parameters")
        if loaded_stats['critic_head'] > 0:
            print(f"   ğŸ¯ Critic head: {loaded_stats['critic_head']} parameters")
        if loaded_stats['skipped'] > 0:
            print(f"   âš ï¸  Skipped: {loaded_stats['skipped']} parameters")
        
        return loaded_stats
    
    def save_checkpoint(self, filepath: str, epoch: int = 0, step: int = 0, 
                       additional_info: Optional[Dict] = None, 
                       upload_to_wandb: bool = False,
                       wandb_alias: Optional[str] = None) -> None:
        """
        ä¿å­˜æ£€æŸ¥ç‚¹å¹¶å¯é€‰ä¸Šä¼ åˆ°wandb
        
        Args:
            filepath: ä¿å­˜è·¯å¾„
            epoch: è®­ç»ƒè½®æ¬¡
            step: è®­ç»ƒæ­¥æ•°
            additional_info: é¢å¤–ä¿¡æ¯
            upload_to_wandb: æ˜¯å¦ä¸Šä¼ åˆ°wandb
            wandb_alias: wandbæ¨¡å‹ç‰ˆæœ¬åˆ«å
        """
        checkpoint = {
            'epoch': epoch,
            'step': step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.model.optimizer.state_dict(),
            'value_norm_state': self.model.value_norm.state_dict(),
            'model_config': self.cfg,
        }
        
        if additional_info:
            checkpoint.update(additional_info)
        
        # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        torch.save(checkpoint, filepath)
        print(f"ğŸ’¾ Checkpoint saved: {filepath}")
        
        # å¯é€‰ï¼šä¸Šä¼ åˆ°wandb
        if upload_to_wandb and WANDB_AVAILABLE:
            self._upload_to_wandb(filepath, step, wandb_alias, additional_info)
    
    def _upload_to_wandb(self, filepath: str, step: int, alias: Optional[str] = None, 
                        metadata: Optional[Dict] = None) -> None:
        """ä¸Šä¼ æ¨¡å‹åˆ°wandb"""
        try:
            if not wandb.run:
                print("âš ï¸  No active wandb run. Cannot upload model.")
                return
            
            # è·å–æ¨¡å‹ä¿¡æ¯
            model_info = self.model.get_model_info()
            
            # åˆ›å»ºartifact
            model_name = f"navrl-model-step-{step}"
            artifact = wandb.Artifact(
                name=model_name,
                type="model",
                metadata={
                    'step': step,
                    'architecture': 'PPO-ViT',
                    'total_parameters': model_info['total_parameters'],
                    'trainable_parameters': model_info['trainable_parameters'],
                    'action_dim': model_info['action_dim'],
                    'device': model_info['device'],
                    'use_amp': model_info['use_amp'],
                    **(metadata or {})
                }
            )
            
            # æ·»åŠ æ¨¡å‹æ–‡ä»¶
            artifact.add_file(filepath)
            
            # åˆ›å»ºæ¨¡å‹å¡ç‰‡
            model_card_path = self._create_model_card(filepath, model_info, step, metadata)
            artifact.add_file(model_card_path, name="model_card.md")
            
            # è®°å½•artifact
            wandb.log_artifact(artifact, aliases=[alias] if alias else None)
            
            print(f"ğŸ“¤ Model uploaded to wandb: {model_name}")
            if alias:
                print(f"   ğŸ·ï¸  Alias: {alias}")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(model_card_path) and "tmp" in model_card_path:
                os.remove(model_card_path)
                
        except Exception as e:
            print(f"âŒ Failed to upload model to wandb: {e}")
    
    def _create_model_card(self, filepath: str, model_info: Dict, step: int, 
                          metadata: Optional[Dict] = None) -> str:
        """åˆ›å»ºæ¨¡å‹å¡ç‰‡"""
        # åˆ›å»ºä¸´æ—¶æ¨¡å‹å¡ç‰‡æ–‡ä»¶
        with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:
            f.write(f"""# NavRL Model Card - Step {step}

## Model Overview
- **Architecture**: PPO-ViT with Shared Feature Extractor  
- **Training Step**: {step:,}
- **File Size**: {os.path.getsize(filepath) / (1024**2):.2f} MB

## Architecture Details
- **Total Parameters**: {model_info['total_parameters']:,}
- **Trainable Parameters**: {model_info['trainable_parameters']:,}  
- **Frozen Parameters**: {model_info['frozen_parameters']:,}
- **Action Dimension**: {model_info['action_dim']}
- **Mixed Precision**: {'Enabled' if model_info['use_amp'] else 'Disabled'}

## Component Breakdown
- **ViT Decoder**: {model_info['vit_decoder_params']:,} parameters
- **Other Modules**: {model_info['other_modules_params']:,} parameters

## Training Configuration
- **Device**: {model_info['device']}
- **Optimizer**: Adam with grouped learning rates
- **Loss Function**: PPO with value clipping

""")

            # æ·»åŠ é¢å¤–çš„å…ƒæ•°æ®ä¿¡æ¯
            if metadata:
                f.write("## Training Metrics\n")
                for key, value in metadata.items():
                    if isinstance(value, (int, float)):
                        f.write(f"- **{key.replace('_', ' ').title()}**: {value:.4f}\n")
                    else:
                        f.write(f"- **{key.replace('_', ' ').title()}**: {value}\n")
                f.write("\n")
            
            f.write("""## Usage
```python
# åŠ è½½æ¨¡å‹
from models import load_pretrained_model
model_manager = load_pretrained_model(checkpoint_path, cfg, obs_spec, act_spec, device)
model = model_manager.get_model()

# æ¨ç†
output = model(input_tensordict)
```

## Model Components
1. **SharedFeatureExtractor**: ViT-based visual feature extraction
2. **Actor Head**: Policy network with Beta distribution
3. **Critic Head**: Value function estimation
4. **Optimization**: Grouped learning rates for ViT fine-tuning

Generated by NavRL ModelManager
""")
            return f.name
    
    def upload_model_to_registry(self, model_name: str, description: str = "",
                                tags: Optional[List[str]] = None,
                                step: int = 0) -> None:
        """
        ä¸Šä¼ æ¨¡å‹åˆ°wandbæ¨¡å‹æ³¨å†Œè¡¨
        
        Args:
            model_name: æ¨¡å‹åç§°
            description: æ¨¡å‹æè¿°  
            tags: æ ‡ç­¾åˆ—è¡¨
            step: è®­ç»ƒæ­¥æ•°
        """
        if not WANDB_AVAILABLE or not wandb.run:
            print("âš ï¸  wandb not available or no active run.")
            return
        
        try:
            # åˆ›å»ºä¸´æ—¶ä¿å­˜è·¯å¾„
            with tempfile.NamedTemporaryFile(suffix='.pt', delete=False) as f:
                temp_path = f.name
            
            # ä¿å­˜æ¨¡å‹
            self.save_checkpoint(temp_path, step=step)
            
            # è·å–æ¨¡å‹ä¿¡æ¯
            model_info = self.model.get_model_info()
            
            # åˆ›å»ºæ¨¡å‹artifactç”¨äºæ³¨å†Œè¡¨
            artifact = wandb.Artifact(
                name=model_name,
                type="model",
                description=description,
                metadata={
                    'framework': 'NavRL',
                    'architecture': 'PPO-ViT',
                    'step': step,
                    **model_info
                }
            )
            
            # æ·»åŠ æ–‡ä»¶
            artifact.add_file(temp_path, name="model.pt")
            
            # åˆ›å»ºå’Œæ·»åŠ æ¨¡å‹å¡ç‰‡
            model_card_path = self._create_model_card(temp_path, model_info, step)
            artifact.add_file(model_card_path, name="README.md")
            
            # è®°å½•åˆ°æ³¨å†Œè¡¨
            wandb.run.log_artifact(artifact, aliases=tags or ["latest"])
            
            print(f"ğŸ¯ Model '{model_name}' uploaded to wandb registry")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.remove(temp_path)
            if os.path.exists(model_card_path):
                os.remove(model_card_path)
                
        except Exception as e:
            print(f"âŒ Failed to upload to registry: {e}")
    
    def load_from_wandb(self, artifact_path: str, load_optimizer: bool = True) -> bool:
        """
        ä»wandb artifactåŠ è½½æ¨¡å‹
        
        Args:
            artifact_path: wandb artifactè·¯å¾„ (e.g., "username/project/model-name:version")
            load_optimizer: æ˜¯å¦åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
            
        Returns:
            åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        if not WANDB_AVAILABLE:
            print("âŒ wandb not available")
            return False
        
        try:
            print(f"ğŸ”„ Loading model from wandb: {artifact_path}")
            
            # ä¸‹è½½artifact
            artifact = wandb.use_artifact(artifact_path)
            artifact_dir = artifact.download()
            
            # å¯»æ‰¾æ¨¡å‹æ–‡ä»¶
            model_files = list(Path(artifact_dir).glob("*.pt"))
            if not model_files:
                print("âŒ No .pt model file found in artifact")
                return False
            
            model_path = str(model_files[0])
            print(f"   ğŸ“‚ Model file: {model_path}")
            
            # åŠ è½½æ¨¡å‹
            success = self.load_checkpoint(model_path, load_optimizer)
            
            if success:
                print(f"âœ… Model loaded from wandb successfully")
                # æ‰“å°artifactå…ƒæ•°æ®
                if hasattr(artifact, 'metadata') and artifact.metadata:
                    print("   ğŸ“‹ Artifact metadata:")
                    for key, value in artifact.metadata.items():
                        print(f"      {key}: {value}")
            
            return success
            
        except Exception as e:
            print(f"âŒ Failed to load from wandb: {e}")
            return False
    
    def print_model_summary(self) -> None:
        """æ‰“å°æ¨¡å‹æ‘˜è¦"""
        info = self.model.get_model_info()
        
        print("\n" + "="*60)
        print("ğŸ“Š NavRL Model Summary")
        print("="*60)
        print(f"ğŸ—ï¸  Architecture: PPO-ViT with Shared Feature Extractor")
        print(f"ğŸ¯ Action Dimension: {info['action_dim']}")
        print(f"ğŸ’¾ Device: {info['device']}")
        print(f"âš¡ Mixed Precision: {'Enabled' if info['use_amp'] else 'Disabled'}")
        print(f"ğŸ“ˆ Total Parameters: {info['total_parameters']:,}")
        print(f"ğŸ”„ Trainable Parameters: {info['trainable_parameters']:,}")
        print(f"â„ï¸  Frozen Parameters: {info['frozen_parameters']:,}")
        print(f"   - ViT Decoder: {info['vit_decoder_params']:,}")
        print(f"   - Other Modules: {info['other_modules_params']:,}")
        print("="*60 + "\n")
    
    def get_model(self) -> NavRLModel:
        """è·å–æ¨¡å‹å®ä¾‹"""
        return self.model
    
    def set_training_mode(self, mode: bool = True) -> None:
        """è®¾ç½®è®­ç»ƒ/è¯„ä¼°æ¨¡å¼"""
        self.model.train(mode)
        if mode:
            print("ğŸƒ Model set to TRAINING mode")
        else:
            print("ğŸ” Model set to EVALUATION mode")
    
    def freeze_vit_encoder(self) -> None:
        """å†»ç»“ViT encoder"""
        self.model.shared_features.freeze_vit_encoder()
    
    def unfreeze_all_vit(self) -> None:
        """è§£å†»æ‰€æœ‰ViTå‚æ•°"""
        self.model.shared_features.unfreeze_all_vit()
        # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨ä»¥åŒ…å«æ–°çš„å¯è®­ç»ƒå‚æ•°
        self.model.optimizer = self.model._create_grouped_optimizer()
        print("ğŸ”„ Optimizer updated with unfrozen parameters")


# === å·¥å‚å‡½æ•° ===

def create_navrl_model(cfg, observation_spec, action_spec, device: torch.device) -> ModelManager:
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºNavRLæ¨¡å‹ç®¡ç†å™¨
    
    Args:
        cfg: é…ç½®å¯¹è±¡
        observation_spec: è§‚å¯Ÿç©ºé—´è§„æ ¼
        action_spec: åŠ¨ä½œç©ºé—´è§„æ ¼  
        device: è®¡ç®—è®¾å¤‡
        
    Returns:
        ModelManagerå®ä¾‹
    """
    return ModelManager(cfg, observation_spec, action_spec, device)


def load_pretrained_model(checkpoint_path: str, cfg, observation_spec, action_spec, 
                         device: torch.device, load_optimizer: bool = True) -> ModelManager:
    """
    å·¥å‚å‡½æ•°ï¼šåŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    
    Args:
        checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
        cfg: é…ç½®å¯¹è±¡
        observation_spec: è§‚å¯Ÿç©ºé—´è§„æ ¼
        action_spec: åŠ¨ä½œç©ºé—´è§„æ ¼
        device: è®¡ç®—è®¾å¤‡
        load_optimizer: æ˜¯å¦åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
        
    Returns:
        ModelManagerå®ä¾‹
    """
    manager = create_navrl_model(cfg, observation_spec, action_spec, device)
    success = manager.load_checkpoint(checkpoint_path, load_optimizer)
    
    if not success:
        print("âš ï¸  ç»§ç»­ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
    
    return manager


if __name__ == "__main__":
    # ç¤ºä¾‹ä½¿ç”¨
    print("NavRL Model Manager - ç‹¬ç«‹æµ‹è¯•")
    
    # è¿™é‡Œå¯ä»¥æ·»åŠ ç‹¬ç«‹çš„æµ‹è¯•ä»£ç 
    pass