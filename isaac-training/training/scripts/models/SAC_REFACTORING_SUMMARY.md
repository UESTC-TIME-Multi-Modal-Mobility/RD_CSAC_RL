<!--
 * @Author: zdytim zdytim@foxmail.com
 * @Date: 2026-01-06 11:23:07
 * @LastEditors: zdytim zdytim@foxmail.com
 * @LastEditTime: 2026-01-06 11:23:08
 * @FilePath: /NavRL/isaac-training/training/scripts/models/SAC_REFACTORING_SUMMARY.md
 * @Description: è¿™æ˜¯é»˜è®¤è®¾ç½®,è¯·è®¾ç½®`customMade`, æ‰“å¼€koroFileHeaderæŸ¥çœ‹é…ç½® è¿›è¡Œè®¾ç½®: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE
-->
"""
SAC æ¨¡å‹æŠ½è±¡åŒ–æ€»ç»“
==================

æœ¬æ–‡æ¡£è¯´æ˜äº†å°† SAC æ¨¡å‹ä»è®­ç»ƒè„šæœ¬ä¸­æŠ½è±¡å‡ºæ¥çš„å·¥ä½œã€‚

ä½œè€…: NavRL Team
æ—¥æœŸ: 2026å¹´1æœˆ6æ—¥
"""

## 1. æ”¹åŠ¨æ¦‚è¿°

å°† SAC (Soft Actor-Critic) çš„æ¨¡å‹å®šä¹‰ä»è®­ç»ƒè„šæœ¬ä¸­åˆ†ç¦»å‡ºæ¥ï¼Œæ”¾åˆ°ç‹¬ç«‹çš„æ¨¡å‹ç®¡ç†æ¨¡å—ä¸­ï¼Œ
éµå¾ªä¸ PPO-ViT æ¨¡å‹ç›¸åŒçš„è®¾è®¡æ¨¡å¼ã€‚

## 2. æ–‡ä»¶ç»“æ„

### æ–°å¢æ–‡ä»¶

```
isaac-training/training/scripts/models/
â”œâ”€â”€ sac_model.py                    # ğŸ†• SAC æ¨¡å‹ç®¡ç†æ¨¡å—
â”œâ”€â”€ example_sac_usage.py            # ğŸ†• ä½¿ç”¨ç¤ºä¾‹è„šæœ¬
â””â”€â”€ README.md                       # ğŸ“ æ›´æ–°æ–‡æ¡£
```

### ä¿®æ”¹æ–‡ä»¶

```
isaac-training/training/scripts/
â””â”€â”€ SAC_v1.py                       # ğŸ“ æ·»åŠ  SAC_V2 åŒ…è£…ç±»
```

## 3. ä¸»è¦ç»„ä»¶

### 3.1 SACFeatureExtractor

å…±äº«ç‰¹å¾æå–å™¨ï¼ŒåŒ…å«ï¼š
- Lidar CNN ç‰¹å¾æå–ï¼ˆConv2D + ELU + LayerNormï¼‰
- åŠ¨æ€éšœç¢ç‰©ç¼–ç å™¨ï¼ˆMLPï¼‰
- ç‰¹å¾èåˆï¼ˆæ‹¼æ¥ + LayerNormï¼‰
- TensorDict æ¥å£æ”¯æŒ

**è¾“å…¥ï¼š**
- `lidar`: [Batch, 1, 60, 60]
- `dynamic_obstacle`: [Batch, 1, 10, 5]
- `state`: [Batch, 8]

**è¾“å‡ºï¼š**
- `feature`: [Batch, 200] (128 + 64 + 8)

### 3.2 ActorNetwork

Actor ç½‘ç»œï¼Œè¾“å‡º TanhNormal åˆ†å¸ƒå‚æ•°ï¼š
- ç‰¹å¾æå–å™¨ â†’ 200ç»´ç‰¹å¾
- MLP (200 â†’ 256 â†’ 256) + LayerNorm
- GaussianActor â†’ (loc, scale)
- TanhNormal åˆ†å¸ƒåŒ…è£…

**è¾“å‡ºï¼š**
- `action_normalized`: å½’ä¸€åŒ–åŠ¨ä½œ [-1, 1]
- `loc`: åˆ†å¸ƒå‡å€¼
- `scale`: åˆ†å¸ƒæ ‡å‡†å·®

### 3.3 CriticNetwork

Critic ç½‘ç»œï¼ˆQå‡½æ•°ï¼‰ï¼š
- ç‰¹å¾æå–å™¨ â†’ 200ç»´ç‰¹å¾
- æ‹¼æ¥åŠ¨ä½œ â†’ 200 + action_dim
- MLP (200+act_dim â†’ 256 â†’ 256 â†’ 1)
- è¾“å‡º Q å€¼

**è¾“å…¥ï¼š**
- `state`: è§‚æµ‹å­—å…¸
- `action`: [Batch, action_dim]

**è¾“å‡ºï¼š**
- `q_value`: [Batch, 1]

### 3.4 SACModel

å®Œæ•´çš„ SAC æ¨¡å‹ï¼ŒåŒ…å«ï¼š
- Actor ç½‘ç»œ
- åŒ Critic ç½‘ç»œï¼ˆQ1, Q2ï¼‰
- Target Critic ç½‘ç»œï¼ˆQ1_target, Q2_targetï¼‰
- Temperature å‚æ•°ï¼ˆlog_alpha, alphaï¼‰
- ä¼˜åŒ–å™¨ï¼ˆactor_optim, critic1_optim, critic2_optim, alpha_optimï¼‰
- è®­ç»ƒé€»è¾‘ï¼ˆtrain_stepï¼‰
- è½¯æ›´æ–°é€»è¾‘ï¼ˆ_soft_updateï¼‰

**æ ¸å¿ƒæ–¹æ³•ï¼š**
- `get_action(state, deterministic)`: æ¨ç†æ¥å£
- `__call__(td)`: ç¯å¢ƒäº¤äº’æ¥å£
- `train_step(replay_buffer, batch_size, tau)`: è®­ç»ƒæ­¥éª¤
- `actions_to_world(actions, tensordict)`: åæ ‡è½¬æ¢

### 3.5 SACModelManager

æ¨¡å‹ç®¡ç†å™¨ï¼Œæä¾›ç»Ÿä¸€æ¥å£ï¼š
- æ¨¡å‹åˆ›å»ºå’Œåˆå§‹åŒ–
- æ£€æŸ¥ç‚¹ä¿å­˜ (`save_checkpoint`)
- æ£€æŸ¥ç‚¹åŠ è½½ (`load_checkpoint`)
- wandb é›†æˆï¼ˆè‡ªåŠ¨ä¸Šä¼ ï¼‰
- è®­ç»ƒæ¨¡å¼åˆ‡æ¢ (`set_training_mode`)
- æ¨¡å‹ä¿¡æ¯æŸ¥è¯¢ (`get_model_info`)

## 4. ä½¿ç”¨æ–¹å¼

### 4.1 æ–¹å¼ä¸€ï¼šç›´æ¥ä½¿ç”¨ SACModelManagerï¼ˆæ¨èï¼‰

```python
from models.sac_model import SACModelManager

# åˆ›å»ºæ¨¡å‹ç®¡ç†å™¨
manager = SACModelManager(
    cfg=cfg,
    observation_spec=env.observation_spec,
    action_spec=env.action_spec,
    device=device
)

# æ¨ç†
actions = manager.get_action(observations, deterministic=True)

# è®­ç»ƒ
loss_info = manager.train_step(replay_buffer, batch_size=256, tau=0.005)

# ä¿å­˜/åŠ è½½
manager.save_checkpoint("checkpoint.pt", step=10000)
manager.load_checkpoint("checkpoint.pt")
```

### 4.2 æ–¹å¼äºŒï¼šä½¿ç”¨ SAC_V2 åŒ…è£…ç±»ï¼ˆå‘åå…¼å®¹ï¼‰

```python
from SAC_v1 import SAC_V2

# åˆ›å»º SAC agentï¼ˆæ¥å£ä¸åŸå§‹ SAC ç›¸åŒï¼‰
agent = SAC_V2(cfg, obs_spec, act_spec, device)

# ä½¿ç”¨æ–¹å¼ä¸åŸå§‹ SAC å®Œå…¨ç›¸åŒ
agent.get_action(state)
agent.train(replay_buffer, batch_size)
agent.save_checkpoint(path, step)
```

### 4.3 æ–¹å¼ä¸‰ï¼šä¾¿æ·å‡½æ•°

```python
from models.sac_model import create_sac_model

# ä¸€è¡Œåˆ›å»º
manager = create_sac_model(cfg, obs_spec, act_spec, device)
```

## 5. è®¾è®¡ä¼˜åŠ¿

### 5.1 å…³æ³¨ç‚¹åˆ†ç¦»
- **æ¨¡å‹å®šä¹‰** â†’ `models/sac_model.py`
- **è®­ç»ƒé€»è¾‘** â†’ `train_sac.py` (è®­ç»ƒè„šæœ¬)
- **é…ç½®ç®¡ç†** â†’ `cfg/*.yaml`

### 5.2 ä»£ç å¤ç”¨
- ç‰¹å¾æå–å™¨åœ¨ Actor å’Œ Critic é—´å…±äº«æ¶æ„
- ç»Ÿä¸€çš„æ£€æŸ¥ç‚¹ç®¡ç†æ¥å£
- wandb é›†æˆå¼€ç®±å³ç”¨

### 5.3 æ˜“äºæ‰©å±•
- æ–°å¢æ¨¡å‹åªéœ€ç»§æ‰¿ `TensorDictModuleBase`
- å®ç° `get_action`, `train_step`, `__call__` æ–¹æ³•
- ç®¡ç†å™¨è‡ªåŠ¨å¤„ç†æ£€æŸ¥ç‚¹å’Œé…ç½®

### 5.4 å‘åå…¼å®¹
- ä¿ç•™åŸå§‹ SAC ç±»ï¼ˆæ ‡è®°ä¸ºåºŸå¼ƒï¼‰
- æä¾› SAC_V2 åŒ…è£…ç±»
- æ¥å£å®Œå…¨ä¸€è‡´ï¼Œæ— éœ€ä¿®æ”¹ç°æœ‰ä»£ç 

## 6. æ£€æŸ¥ç‚¹æ ¼å¼

ä¿å­˜çš„æ£€æŸ¥ç‚¹åŒ…å«ï¼š
```python
{
    'step': int,                              # è®­ç»ƒæ­¥æ•°
    'model_state_dict': OrderedDict,          # æ¨¡å‹å‚æ•°
    'actor_optim_state_dict': dict,           # Actor ä¼˜åŒ–å™¨
    'critic1_optim_state_dict': dict,         # Critic1 ä¼˜åŒ–å™¨
    'critic2_optim_state_dict': dict,         # Critic2 ä¼˜åŒ–å™¨
    'alpha_optim_state_dict': dict,           # Temperature ä¼˜åŒ–å™¨
    'log_alpha': Tensor,                      # log(alpha) å‚æ•°
    'cfg': dict,                              # é…ç½®ä¿¡æ¯
    **extra_info                              # é¢å¤–ä¿¡æ¯ï¼ˆå¦‚ replay_bufferï¼‰
}
```

## 7. wandb é›†æˆ

å¦‚æœ wandb å¯ç”¨ä¸”å·²åˆå§‹åŒ–ï¼Œ`save_checkpoint` ä¼šè‡ªåŠ¨ï¼š
1. ä¿å­˜æ£€æŸ¥ç‚¹åˆ°æœ¬åœ°
2. åˆ›å»º wandb Artifactï¼ˆç±»å‹ï¼šmodelï¼‰
3. ä¸Šä¼ åˆ° wandb äº‘ç«¯
4. å…³è”åˆ°å½“å‰ run

ç¦ç”¨ wandbï¼š
```python
import wandb
wandb.init(mode="disabled")
```

## 8. è¿ç§»æŒ‡å—

### æ­¥éª¤ 1: æ›´æ–°å¯¼å…¥

**æ—§ä»£ç ï¼š**
```python
from SAC_v1 import SAC
```

**æ–°ä»£ç ï¼š**
```python
from SAC_v1 import SAC_V2  # æˆ–
from models.sac_model import SACModelManager
```

### æ­¥éª¤ 2: åˆ›å»ºæ¨¡å‹

**æ—§ä»£ç ï¼š**
```python
sac_agent = SAC(cfg, obs_spec, act_spec, device)
```

**æ–°ä»£ç ï¼š**
```python
# æ–¹å¼1ï¼ˆæ¨èï¼‰
manager = SACModelManager(cfg, obs_spec, act_spec, device)

# æ–¹å¼2ï¼ˆå…¼å®¹ï¼‰
sac_agent = SAC_V2(cfg, obs_spec, act_spec, device)
```

### æ­¥éª¤ 3: å…¶ä»–ä»£ç æ— éœ€ä¿®æ”¹

æ‰€æœ‰æ–¹æ³•æ¥å£ä¿æŒä¸€è‡´ï¼š
- `get_action(state, deterministic)`
- `train(replay_buffer, batch_size, tau)`
- `save_checkpoint(path, step, **extra)`
- `load_checkpoint(path, load_optimizers)`

## 9. æµ‹è¯•å’ŒéªŒè¯

### è¿è¡Œç¤ºä¾‹è„šæœ¬

```bash
cd isaac-training/training/scripts/models
python example_sac_usage.py
```

### éªŒè¯é¡¹ç›®
- âœ… æ¨¡å‹åˆ›å»ºå’Œåˆå§‹åŒ–
- âœ… æ¨ç†ï¼ˆç¡®å®šæ€§å’Œéšæœºï¼‰
- âœ… æ£€æŸ¥ç‚¹ä¿å­˜å’ŒåŠ è½½
- âœ… å‘åå…¼å®¹æ€§
- âš ï¸  è®­ç»ƒæ­¥éª¤ï¼ˆéœ€è¦çœŸå® ReplayBufferï¼‰

## 10. æœªæ¥æ”¹è¿›

- [ ] æ·»åŠ æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒï¼ˆAMPï¼‰
- [ ] æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ
- [ ] æ·»åŠ æ¨¡å‹å‰ªæå’Œé‡åŒ–æ¥å£
- [ ] å®ç°è‡ªåŠ¨è¶…å‚æ•°è°ƒä¼˜
- [ ] æ”¯æŒæ›´å¤š SAC å˜ä½“ï¼ˆå¦‚ SAC-Discreteï¼‰

## 11. å‚è€ƒèµ„æ–™

- TorchRL æ–‡æ¡£: https://pytorch.org/rl/
- TensorDict æ–‡æ¡£: https://github.com/pytorch/tensordict
- SAC è®ºæ–‡: "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning"
- NavRL PPO-ViT æ¨¡å‹: `models/navrl_model.py`

---

**æ€»ç»“ï¼š** æœ¬æ¬¡é‡æ„å°† SAC æ¨¡å‹å®Œå…¨æŠ½è±¡åŒ–ï¼Œæä¾›äº†ç»Ÿä¸€ã€å¯æ‰©å±•ã€æ˜“ç»´æŠ¤çš„æ¨¡å‹ç®¡ç†æ¥å£ï¼Œ
åŒæ—¶ä¿æŒå‘åå…¼å®¹æ€§ã€‚è®­ç»ƒè„šæœ¬ç°åœ¨åªéœ€å…³æ³¨é‡‡æ ·ã€è®­ç»ƒå¾ªç¯å’Œæ—¥å¿—è®°å½•ç­‰é«˜å±‚é€»è¾‘ã€‚
