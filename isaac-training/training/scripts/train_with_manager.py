'''
Author: zdytim zdytim@foxmail.com
Date: 2026-01-05 22:21:12
LastEditors: zdytim zdytim@foxmail.com
LastEditTime: 2026-01-05 22:21:13
FilePath: /NavRL/isaac-training/training/scripts/train_with_manager.py
Description: è¿™æ˜¯é»˜è®¤è®¾ç½®,è¯·è®¾ç½®`customMade`, æ‰“å¼€koroFileHeaderæŸ¥çœ‹é…ç½® è¿›è¡Œè®¾ç½®: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE
'''
"""
NavRLè®­ç»ƒè„šæœ¬ç¤ºä¾‹ - ä½¿ç”¨æ–°çš„æ¨¡å‹ç®¡ç†å™¨
=============================================

å±•ç¤ºå¦‚ä½•ä½¿ç”¨æŠ½è±¡çš„NavRLæ¨¡å‹ç®¡ç†å™¨æ¥ç®€åŒ–è®­ç»ƒä»£ç ã€‚

ä¸»è¦æ”¹è¿›ï¼š
1. æ¨¡å‹åˆ›å»ºå’Œç®¡ç†é€»è¾‘åˆ†ç¦»
2. ç»Ÿä¸€çš„æ£€æŸ¥ç‚¹åŠ è½½/ä¿å­˜æ¥å£
3. æ¸…æ™°çš„å‚æ•°ç®¡ç†å’Œé…ç½®
4. æ›´å¥½çš„ä»£ç ç»„ç»‡å’Œå¤ç”¨æ€§

ä½¿ç”¨æ–¹æ³•ï¼š
1. å¯¼å…¥æ¨¡å‹ç®¡ç†å™¨
2. åˆ›å»ºæˆ–åŠ è½½æ¨¡å‹  
3. ä½¿ç”¨ç»Ÿä¸€çš„è®­ç»ƒæ¥å£
"""

import torch
import hydra
from omegaconf import DictConfig
from tensordict import TensorDict
from torchrl.envs import ParallelEnv

# å¯¼å…¥æ–°çš„æ¨¡å‹ç®¡ç†å™¨
from models import create_navrl_model, load_pretrained_model

# å¯¼å…¥å…¶ä»–å¿…è¦æ¨¡å—
from env import NavigationEnv
from utils import make_batch, vec_to_world


class NavRLTrainer:
    """
    NavRLè®­ç»ƒå™¨ - ä½¿ç”¨æ–°çš„æ¨¡å‹ç®¡ç†æ¶æ„
    
    ä¼˜åŠ¿ï¼š
    1. æ¸…æ™°çš„æ¨¡å‹ç”Ÿå‘½å‘¨æœŸç®¡ç†
    2. ç»Ÿä¸€çš„æ£€æŸ¥ç‚¹å¤„ç†
    3. çµæ´»çš„å‚æ•°é…ç½®
    4. æ›´å¥½çš„å¯è¯»æ€§å’Œç»´æŠ¤æ€§
    """
    
    def __init__(self, cfg: DictConfig):
        self.cfg = cfg
        self.device = torch.device(cfg.device)
        
        # åˆ›å»ºç¯å¢ƒ
        self.env = self._create_environment()
        
        # è·å–è§‚å¯Ÿå’ŒåŠ¨ä½œè§„æ ¼
        self.observation_spec = self.env.observation_spec
        self.action_spec = self.env.action_spec
        
        # åˆ›å»ºæ¨¡å‹ç®¡ç†å™¨
        self.model_manager = self._create_model()
        
        # æ‰“å°æ¨¡å‹æ‘˜è¦
        self.model_manager.print_model_summary()
        
        print("ğŸ‰ NavRL Trainer initialized successfully!")
    
    def _create_environment(self) -> ParallelEnv:
        """åˆ›å»ºè®­ç»ƒç¯å¢ƒ"""
        print("ğŸ—ï¸  Creating training environment...")
        
        # åˆ›å»ºå•ä¸ªç¯å¢ƒå®ä¾‹
        env_fn = lambda: NavigationEnv(self.cfg)
        env = ParallelEnv(
            num_workers=self.cfg.env.num_envs,
            create_env_fn=env_fn,
            device=self.device
        )
        
        print(f"   - Environment created with {self.cfg.env.num_envs} parallel workers")
        return env
    
    def _create_model(self):
        """åˆ›å»ºæˆ–åŠ è½½æ¨¡å‹"""
        # æ£€æŸ¥æ˜¯å¦æœ‰é¢„è®­ç»ƒæ£€æŸ¥ç‚¹éœ€è¦åŠ è½½
        checkpoint_path = getattr(self.cfg, 'resume_checkpoint', None)
        
        if checkpoint_path and checkpoint_path != "":
            print(f"ğŸ”„ Loading from checkpoint: {checkpoint_path}")
            model_manager = load_pretrained_model(
                checkpoint_path=checkpoint_path,
                cfg=self.cfg,
                observation_spec=self.observation_spec,
                action_spec=self.action_spec,
                device=self.device,
                load_optimizer=True
            )
        else:
            print("ğŸ†• Creating new model...")
            model_manager = create_navrl_model(
                cfg=self.cfg,
                observation_spec=self.observation_spec,
                action_spec=self.action_spec,
                device=self.device
            )
        
        return model_manager
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print("ğŸš€ Starting training...")
        
        model = self.model_manager.get_model()
        env = self.env
        
        # è®­ç»ƒå¾ªç¯
        for step in range(self.cfg.total_steps):
            # 1. ç¯å¢ƒäº¤äº’é˜¶æ®µ
            with torch.no_grad():
                # é‡ç½®ç¯å¢ƒï¼ˆå¦‚æœéœ€è¦ï¼‰
                if step == 0 or step % self.cfg.rollout_length == 0:
                    td = env.reset()
                    print(f"   ğŸ”„ Environment reset at step {step}")
                
                # æ”¶é›†è½¨è¿¹æ•°æ®
                rollout_data = self._collect_rollout(model, td, self.cfg.rollout_length)
            
            # 2. æ¨¡å‹è®­ç»ƒé˜¶æ®µ
            if len(rollout_data) >= self.cfg.batch_size:
                # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
                self.model_manager.set_training_mode(True)
                
                # æ‰§è¡Œè®­ç»ƒæ›´æ–°
                train_info = self._train_step(model, rollout_data)
                
                # è®°å½•è®­ç»ƒä¿¡æ¯
                if step % self.cfg.log_interval == 0:
                    self._log_training_info(step, train_info)
            
            # 3. æ¨¡å‹ä¿å­˜
            if step > 0 and step % self.cfg.save_interval == 0:
                self._save_checkpoint(step)
        
        print("âœ… Training completed!")
    
    def _collect_rollout(self, model, td: TensorDict, rollout_length: int) -> list:
        """æ”¶é›†rolloutæ•°æ®"""
        rollout_data = []
        
        for _ in range(rollout_length):
            # æ¨¡å‹æ¨ç†
            td = model(td)
            
            # ç¯å¢ƒæ­¥è¿›
            td = self.env.step(td)
            
            # ä¿å­˜æ•°æ®
            rollout_data.append(td.clone())
        
        return rollout_data
    
    def _train_step(self, model, rollout_data: list) -> dict:
        """æ‰§è¡Œä¸€æ¬¡è®­ç»ƒæ­¥éª¤"""
        # å°†rolloutæ•°æ®è½¬æ¢ä¸ºè®­ç»ƒæ‰¹æ¬¡
        batch_td = make_batch(rollout_data, self.device)
        
        # æ¨¡å‹è®­ç»ƒ
        train_info = model.train(batch_td)
        
        return train_info
    
    def _log_training_info(self, step: int, info: dict) -> None:
        """è®°å½•è®­ç»ƒä¿¡æ¯"""
        print(f"Step {step}:")
        for key, value in info.items():
            print(f"   {key}: {value:.4f}")
    
    def _save_checkpoint(self, step: int) -> None:
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        save_path = f"{self.cfg.checkpoint_dir}/checkpoint_step_{step}.pt"
        
        self.model_manager.save_checkpoint(
            filepath=save_path,
            step=step,
            additional_info={'training_step': step}
        )
    
    def evaluate(self, num_episodes: int = 10) -> dict:
        """æ¨¡å‹è¯„ä¼°"""
        print(f"ğŸ” Evaluating model for {num_episodes} episodes...")
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.model_manager.set_training_mode(False)
        
        model = self.model_manager.get_model()
        total_rewards = []
        
        for episode in range(num_episodes):
            td = self.env.reset()
            episode_reward = 0
            done = False
            
            while not done:
                with torch.no_grad():
                    td = model(td)
                    td = self.env.step(td)
                    
                    episode_reward += td["next", "agents", "reward"].sum().item()
                    done = td["next", "terminated"].any().item()
            
            total_rewards.append(episode_reward)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        eval_metrics = {
            'mean_reward': sum(total_rewards) / len(total_rewards),
            'std_reward': torch.tensor(total_rewards).std().item(),
            'min_reward': min(total_rewards),
            'max_reward': max(total_rewards)
        }
        
        print(f"   ğŸ“Š Evaluation results: {eval_metrics}")
        return eval_metrics


# === é«˜çº§ä½¿ç”¨ç¤ºä¾‹ ===

def advanced_training_example(cfg):
    """
    é«˜çº§è®­ç»ƒç¤ºä¾‹ï¼šå±•ç¤ºæ¨¡å‹ç®¡ç†å™¨çš„é«˜çº§åŠŸèƒ½
    """
    print("ğŸ¯ Advanced Training Example")
    
    # 1. åˆ›å»ºè®­ç»ƒå™¨
    trainer = NavRLTrainer(cfg)
    
    # 2. è‡ªå®šä¹‰ViTå‚æ•°ç®¡ç†
    if cfg.training.freeze_vit_encoder:
        print("â„ï¸  Freezing ViT encoder for stable fine-tuning...")
        trainer.model_manager.freeze_vit_encoder()
    
    # 3. æ¨¡å‹ä¿¡æ¯æ£€æŸ¥
    model_info = trainer.model_manager.get_model().get_model_info()
    print(f"ğŸ“Š Trainable parameters: {model_info['trainable_parameters']:,}")
    
    # 4. è®­ç»ƒ
    trainer.train()
    
    # 5. è¯„ä¼°
    eval_results = trainer.evaluate()
    
    return trainer, eval_results


def fine_tuning_example(cfg, pretrained_checkpoint):
    """
    Fine-tuningç¤ºä¾‹ï¼šä»é¢„è®­ç»ƒæ¨¡å‹å¼€å§‹è®­ç»ƒ
    """
    print("ğŸ”§ Fine-tuning Example")
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    cfg.resume_checkpoint = pretrained_checkpoint
    trainer = NavRLTrainer(cfg)
    
    # è§£å†»ViTè¿›è¡Œå®Œæ•´fine-tuning
    if cfg.training.full_vit_finetune:
        print("ğŸ”“ Unfreezing all ViT parameters for full fine-tuning...")
        trainer.model_manager.unfreeze_all_vit()
    
    # è®­ç»ƒå’Œè¯„ä¼°
    trainer.train()
    results = trainer.evaluate()
    
    return trainer, results


# === é…ç½®ç¤ºä¾‹ ===

@hydra.main(version_base=None, config_path="../cfg", config_name="train_ppo_vit")
def main(cfg: DictConfig):
    """
    ä¸»å‡½æ•°ï¼šä½¿ç”¨æ–°çš„æ¨¡å‹ç®¡ç†å™¨è¿›è¡Œè®­ç»ƒ
    
    ä½¿ç”¨æ–¹æ³•ï¼š
    python train_with_manager.py env.num_envs=64 training.freeze_vit_encoder=True
    """
    print("ğŸ‰ NavRL Training with Model Manager")
    print("="*50)
    
    # åŸºç¡€è®­ç»ƒ
    if cfg.training.mode == "basic":
        trainer = NavRLTrainer(cfg)
        trainer.train()
        trainer.evaluate()
    
    # é«˜çº§è®­ç»ƒ
    elif cfg.training.mode == "advanced":
        trainer, results = advanced_training_example(cfg)
    
    # Fine-tuning
    elif cfg.training.mode == "finetune":
        trainer, results = fine_tuning_example(cfg, cfg.resume_checkpoint)
    
    else:
        print(f"âŒ Unknown training mode: {cfg.training.mode}")
    
    print("ğŸ‰ Training session completed!")


if __name__ == "__main__":
    main()


# === å¿«é€Ÿä½¿ç”¨æŒ‡å— ===
"""
å¿«é€Ÿä½¿ç”¨æŒ‡å—ï¼š

1. åŸºç¡€è®­ç»ƒï¼š
   ```python
   from models import create_navrl_model
   
   model_manager = create_navrl_model(cfg, obs_spec, act_spec, device)
   model = model_manager.get_model()
   # ä½¿ç”¨modelè¿›è¡Œè®­ç»ƒ...
   ```

2. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼š
   ```python
   from models import load_pretrained_model
   
   model_manager = load_pretrained_model(checkpoint_path, cfg, obs_spec, act_spec, device)
   model = model_manager.get_model()
   ```

3. å‚æ•°ç®¡ç†ï¼š
   ```python
   # å†»ç»“ViT encoder
   model_manager.freeze_vit_encoder()
   
   # è§£å†»æ‰€æœ‰ViTå‚æ•°
   model_manager.unfreeze_all_vit()
   
   # æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯
   model_manager.print_model_summary()
   ```

4. æ£€æŸ¥ç‚¹ç®¡ç†ï¼š
   ```python
   # ä¿å­˜æ£€æŸ¥ç‚¹
   model_manager.save_checkpoint("checkpoint.pt", step=1000)
   
   # åŠ è½½æ£€æŸ¥ç‚¹
   success = model_manager.load_checkpoint("checkpoint.pt")
   ```

ä¼˜åŠ¿ï¼š
- âœ… ä»£ç æ›´æ¸…æ™°ï¼Œæ¨¡å—åŒ–ç¨‹åº¦é«˜
- âœ… ç»Ÿä¸€çš„æ¨¡å‹ç®¡ç†æ¥å£
- âœ… çµæ´»çš„å‚æ•°é…ç½®å’Œç”Ÿå‘½å‘¨æœŸç®¡ç†
- âœ… æ›´å¥½çš„å¯è¯»æ€§å’Œç»´æŠ¤æ€§
- âœ… æ”¯æŒå¤æ‚çš„è®­ç»ƒåœºæ™¯ï¼ˆé¢„è®­ç»ƒã€fine-tuningç­‰ï¼‰
"""