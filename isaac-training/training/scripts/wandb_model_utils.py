'''
Author: zdytim zdytim@foxmail.com
Date: 2026-01-05 22:32:41
LastEditors: zdytim zdytim@foxmail.com
LastEditTime: 2026-01-05 22:32:44
FilePath: /NavRL/isaac-training/training/scripts/wandb_model_utils.py
Description: è¿™æ˜¯é»˜è®¤è®¾ç½®,è¯·è®¾ç½®`customMade`, æ‰“å¼€koroFileHeaderæŸ¥çœ‹é…ç½® è¿›è¡Œè®¾ç½®: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE
'''
"""
wandb æ¨¡å‹ä¸Šä¼ å·¥å…·å‡½æ•°
========================

ä¸ºç°æœ‰çš„ NavRL è®­ç»ƒè„šæœ¬æä¾› wandb æ¨¡å‹ç®¡ç†åŠŸèƒ½ï¼Œ
æ— éœ€å¤§å¹…ä¿®æ”¹ç°æœ‰ä»£ç å³å¯é›†æˆæ¨¡å‹ç‰ˆæœ¬ç®¡ç†ã€‚
"""

import wandb
import torch
import os
import tempfile
from pathlib import Path
from typing import Dict, Optional, Any


def upload_model_to_wandb(model_state_dict: Dict[str, torch.Tensor], 
                         step: int,
                         eval_metrics: Optional[Dict] = None,
                         model_alias: str = "latest",
                         model_type: str = "checkpoint") -> None:
    """
    ä¸Šä¼ æ¨¡å‹åˆ° wandb artifacts
    
    Args:
        model_state_dict: æ¨¡å‹çŠ¶æ€å­—å…¸
        step: è®­ç»ƒæ­¥æ•°
        eval_metrics: è¯„ä¼°æŒ‡æ ‡
        model_alias: æ¨¡å‹åˆ«å (e.g., "best", "latest")
        model_type: æ¨¡å‹ç±»å‹ (e.g., "checkpoint", "best", "final")
    """
    if not wandb.run:
        print("âš ï¸  No active wandb run. Cannot upload model.")
        return
    
    try:
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜æ¨¡å‹
        with tempfile.NamedTemporaryFile(suffix='.pt', delete=False) as f:
            temp_path = f.name
            torch.save(model_state_dict, temp_path)
        
        # è·å–æ–‡ä»¶å¤§å°
        file_size_mb = os.path.getsize(temp_path) / (1024**2)
        
        # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in model_state_dict.values())
        
        # åˆ›å»º artifact
        artifact_name = f"navrl-model-step-{step}"
        artifact = wandb.Artifact(
            name=artifact_name,
            type="model",
            metadata={
                'step': step,
                'architecture': 'PPO-ViT',
                'total_parameters': total_params,
                'file_size_mb': round(file_size_mb, 2),
                'model_type': model_type,
                'framework': 'NavRL',
                **(eval_metrics or {})
            }
        )
        
        # æ·»åŠ æ¨¡å‹æ–‡ä»¶
        artifact.add_file(temp_path, name="model.pt")
        
        # åˆ›å»ºæ¨¡å‹å¡ç‰‡
        model_card_content = create_model_card(step, total_params, file_size_mb, eval_metrics, model_type)
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:
            f.write(model_card_content)
            model_card_path = f.name
        
        artifact.add_file(model_card_path, name="model_card.md")
        
        # ä¸Šä¼  artifact
        wandb.log_artifact(artifact, aliases=[model_alias])
        
        print(f"ğŸ“¤ Model uploaded to wandb:")
        print(f"   ğŸ†” Name: {artifact_name}")
        print(f"   ğŸ·ï¸  Alias: {model_alias}")
        print(f"   ğŸ“Š Parameters: {total_params:,}")
        print(f"   ğŸ’¾ Size: {file_size_mb:.2f} MB")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(temp_path)
        os.unlink(model_card_path)
        
    except Exception as e:
        print(f"âŒ Failed to upload model to wandb: {e}")


def create_model_card(step: int, total_params: int, file_size_mb: float,
                     eval_metrics: Optional[Dict] = None,
                     model_type: str = "checkpoint") -> str:
    """åˆ›å»ºæ¨¡å‹å¡ç‰‡å†…å®¹"""
    
    content = f"""# NavRL Model - Step {step}

## ğŸ¯ Model Overview
- **Type**: {model_type.title()}
- **Architecture**: PPO-ViT with Shared Feature Extractor
- **Training Step**: {step:,}
- **Parameters**: {total_params:,}
- **File Size**: {file_size_mb:.2f} MB

## ğŸ“ˆ Performance Metrics
"""
    
    if eval_metrics:
        for key, value in eval_metrics.items():
            if isinstance(value, (int, float)):
                content += f"- **{key.replace('_', ' ').title()}**: {value:.4f}\n"
            else:
                content += f"- **{key.replace('_', ' ').title()}**: {value}\n"
    else:
        content += "- No evaluation metrics available\n"
    
    content += f"""
## ğŸ”§ Usage
```python
# Download and load model
import wandb
artifact = wandb.use_artifact('your-project/navrl-model-step-{step}:latest')
artifact_dir = artifact.download()
model_path = artifact_dir + "/model.pt"

# Load model
import torch
state_dict = torch.load(model_path)
model.load_state_dict(state_dict)
```

## ğŸ—ï¸  Architecture Details
- **Framework**: NavRL (PPO with ViT)
- **Feature Extractor**: Vision Transformer with dynamic obstacle processing
- **Actor Head**: Beta distribution policy network
- **Critic Head**: Value function estimation
- **Training**: Mixed precision with grouped learning rates

Generated by NavRL training pipeline at step {step}
"""
    
    return content


def log_model_metrics(eval_metrics: Dict[str, Any], step: int) -> None:
    """è®°å½•æ¨¡å‹ç›¸å…³æŒ‡æ ‡åˆ° wandb"""
    if not wandb.run:
        return
    
    # è®°å½•è¯„ä¼°æŒ‡æ ‡
    for key, value in eval_metrics.items():
        if isinstance(value, (int, float)):
            wandb.log({f"model/{key}": value}, step=step)


def create_model_comparison_table(models_info: List[Dict]) -> None:
    """åˆ›å»ºæ¨¡å‹æ¯”è¾ƒè¡¨æ ¼"""
    if not wandb.run:
        return
    
    try:
        import pandas as pd
        
        # åˆ›å»º DataFrame
        df = pd.DataFrame(models_info)
        
        # åˆ›å»º wandb è¡¨æ ¼
        table = wandb.Table(dataframe=df)
        wandb.log({"model_comparison": table})
        
    except ImportError:
        print("âš ï¸  pandas not available for model comparison table")


def save_and_upload_best_model(model_state_dict: Dict[str, torch.Tensor],
                               step: int,
                               eval_metrics: Dict[str, Any],
                               threshold_metric: str = "mean_reward",
                               best_value_tracker: Dict = None) -> bool:
    """
    ä¿å­˜å¹¶ä¸Šä¼ æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºæŒ‡å®šæŒ‡æ ‡ï¼‰
    
    Args:
        model_state_dict: æ¨¡å‹çŠ¶æ€å­—å…¸
        step: è®­ç»ƒæ­¥æ•°
        eval_metrics: è¯„ä¼°æŒ‡æ ‡
        threshold_metric: ç”¨äºåˆ¤æ–­æœ€ä½³æ¨¡å‹çš„æŒ‡æ ‡
        best_value_tracker: è·Ÿè¸ªæœ€ä½³å€¼çš„å­—å…¸
        
    Returns:
        æ˜¯å¦ä¸Šä¼ äº†æ–°çš„æœ€ä½³æ¨¡å‹
    """
    if threshold_metric not in eval_metrics:
        print(f"âš ï¸  Metric '{threshold_metric}' not found in eval_metrics")
        return False
    
    current_value = eval_metrics[threshold_metric]
    
    # åˆå§‹åŒ–æˆ–æ›´æ–°æœ€ä½³å€¼è·Ÿè¸ª
    if best_value_tracker is None:
        best_value_tracker = {'best_value': float('-inf'), 'best_step': 0}
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„æœ€ä½³æ¨¡å‹
    if current_value > best_value_tracker['best_value']:
        best_value_tracker['best_value'] = current_value
        best_value_tracker['best_step'] = step
        
        print(f"ğŸ† New best model! {threshold_metric}: {current_value:.4f}")
        
        # ä¸Šä¼ æœ€ä½³æ¨¡å‹
        upload_model_to_wandb(
            model_state_dict=model_state_dict,
            step=step,
            eval_metrics=eval_metrics,
            model_alias="best",
            model_type="best"
        )
        
        # è®°å½•æœ€ä½³æ¨¡å‹æŒ‡æ ‡
        wandb.log({
            "best_model/step": step,
            f"best_model/{threshold_metric}": current_value
        }, step=step)
        
        return True
    
    return False


def download_model_from_wandb(artifact_path: str, 
                             download_dir: str = "./downloaded_models") -> Optional[str]:
    """
    ä» wandb ä¸‹è½½æ¨¡å‹
    
    Args:
        artifact_path: wandb artifact è·¯å¾„ (e.g., "username/project/model-name:version")
        download_dir: ä¸‹è½½ç›®å½•
        
    Returns:
        ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥æ—¶è¿”å› None
    """
    try:
        print(f"ğŸ“¥ Downloading model: {artifact_path}")
        
        # ä¸‹è½½ artifact
        artifact = wandb.use_artifact(artifact_path)
        artifact_dir = artifact.download(root=download_dir)
        
        # å¯»æ‰¾æ¨¡å‹æ–‡ä»¶
        model_files = list(Path(artifact_dir).glob("*.pt"))
        if not model_files:
            print("âŒ No .pt model file found in artifact")
            return None
        
        model_path = str(model_files[0])
        print(f"âœ… Model downloaded: {model_path}")
        
        # æ‰“å° artifact ä¿¡æ¯
        if hasattr(artifact, 'metadata') and artifact.metadata:
            print("ğŸ“‹ Model metadata:")
            for key, value in artifact.metadata.items():
                print(f"   {key}: {value}")
        
        return model_path
        
    except Exception as e:
        print(f"âŒ Failed to download model: {e}")
        return None


# === ä½¿ç”¨ç¤ºä¾‹å‡½æ•° ===

def integrate_wandb_to_existing_training():
    """
    å°† wandb æ¨¡å‹ç®¡ç†é›†æˆåˆ°ç°æœ‰è®­ç»ƒè„šæœ¬çš„ç¤ºä¾‹
    """
    print("""
ğŸ”§ é›†æˆ wandb æ¨¡å‹ç®¡ç†åˆ°ç°æœ‰è®­ç»ƒè„šæœ¬ï¼š

1. å¯¼å…¥å·¥å…·å‡½æ•°ï¼š
   from wandb_model_utils import upload_model_to_wandb, save_and_upload_best_model

2. åœ¨æ¨¡å‹ä¿å­˜å¤„æ·»åŠ ä¸Šä¼ ï¼š
   # åŸæœ‰ä¿å­˜ä»£ç 
   torch.save(policy.state_dict(), ckpt_path)
   
   # æ·»åŠ  wandb ä¸Šä¼ 
   upload_model_to_wandb(
       model_state_dict=policy.state_dict(),
       step=training_step,
       model_alias="latest"
   )

3. åœ¨è¯„ä¼°åä¿å­˜æœ€ä½³æ¨¡å‹ï¼š
   eval_metrics = {"mean_reward": 85.2, "success_rate": 0.95}
   save_and_upload_best_model(
       model_state_dict=policy.state_dict(),
       step=training_step,
       eval_metrics=eval_metrics,
       threshold_metric="mean_reward",
       best_value_tracker=best_tracker  # å…¨å±€å˜é‡
   )

4. ä» wandb æ¢å¤è®­ç»ƒï¼š
   model_path = download_model_from_wandb("user/project/model:best")
   if model_path:
       policy.load_state_dict(torch.load(model_path))
    """)


if __name__ == "__main__":
    integrate_wandb_to_existing_training()