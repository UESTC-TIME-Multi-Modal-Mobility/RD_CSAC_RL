'''
Author: zdytim zdytim@foxmail.com
Date: 2025-12-18 00:34:11
LastEditors: zdytim zdytim@foxmail.com
LastEditTime: 2026-01-07 00:00:57
FilePath: /u20/NavRL/isaac-training/training/scripts/VIT.py
Description: ViT Encoder for NavRL - ä»…åŒ…å« encoder_blocks å’Œ decoderï¼Œç”¨äºç‰¹å¾æå–
'''
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.utils.spectral_norm as spectral_norm
from ViTsubmodules import *
# ==============================================================================
# ViT Feature Extractor (ä»…ç”¨äºç‰¹å¾æå–ï¼Œä¸åŒ…å« LSTM/FC2)
# ==============================================================================

class VIT(nn.Module):
    """
    ViT Encoder for Navigation RL - æ”¯æŒåŠ¨æ€è¾“å…¥å°ºå¯¸
    - Input: [Batch, 1, H, W] (å•é€šé“ç°åº¦æ·±åº¦å›¾ï¼Œæ”¯æŒä»»æ„å°ºå¯¸)
    - Output: [Batch, 512] (ç‰¹å¾å‘é‡)
    - éµå¾ªNavRLç¼–ç æ¨¡å¼ï¼Œä½¿ç”¨åŠ¨æ€å°ºå¯¸è®¡ç®—
    """
    def __init__(self, input_size=(224, 224)):
        super().__init__()
        self.input_size = input_size
        
        # ViT Encoder: ä¸¤å±‚ MixTransformer - é€šè¿‡å¤§strideæ§åˆ¶ç‰¹å¾å›¾å°ºå¯¸
        self.encoder_blocks = nn.ModuleList([
            # ç¬¬1å±‚: 224x224 -> 28x28 (stride=8) - å¤§å¹…ä¸‹é‡‡æ ·
            MixTransformerEncoderLayer(
                1, 32, 
                patch_size=7, stride=8, padding=3, 
                n_layers=2, reduction_ratio=8, num_heads=1, expansion_factor=8
            ),
            # ç¬¬2å±‚: 28x28 -> 14x14 (stride=2) - è¿›ä¸€æ­¥å‹ç¼©
            MixTransformerEncoderLayer(
                32, 64, 
                patch_size=3, stride=2, padding=1, 
                n_layers=2, reduction_ratio=4, num_heads=2, expansion_factor=8
            )
        ])

        # åŠ¨æ€è®¡ç®—èåˆå±‚å°ºå¯¸
        self._init_dynamic_layers()
        
        print(f"âœ… VIT initialized for input size: {input_size}")

    def _init_dynamic_layers(self):
        """æ ¹æ®è¾“å…¥å°ºå¯¸åŠ¨æ€åˆå§‹åŒ–èåˆå±‚ - å¤§strideç‰ˆæœ¬"""
        # è®¡ç®—ä¸¤å±‚encoderè¾“å‡ºå°ºå¯¸ (ä½¿ç”¨å¤§stride)
        h1, w1 = self._calc_conv_output_size(self.input_size, 7, 8, 3)  # ç¬¬ä¸€å±‚: 224->28 (stride=8)
        h2, w2 = self._calc_conv_output_size((h1, w1), 3, 2, 1)        # ç¬¬äºŒå±‚: 28->14 (stride=2)
        
        # èåˆå±‚é…ç½® - åŸºäºä¸¤å±‚è¾“å‡ºï¼Œå¤§å¹…å‡å°‘å°ºå¯¸
        self.pxShuffle = nn.PixelShuffle(upscale_factor=2)
        # pxShuffleå: [B, 16, h2*2, w2*2] = [B, 16, 28, 28]
        
        # up_sampleç›®æ ‡å°ºå¯¸åŒ¹é…pxShuffleè¾“å‡º  
        self.target_size = (h2 * 2, w2 * 2)  # (28, 28)
        
        # èåˆä¸¤å±‚ç‰¹å¾: 32 + 16 = 48 channels
        self.down_sample = nn.Conv2d(48, 12, 3, padding=1)
        
        # decoderè¾“å…¥ç»´åº¦å¤§å¹…å‡å°‘: 12 * 28 * 28 = 9408 (vs 37632)
        decoder_input_dim = 12 * self.target_size[0] * self.target_size[1]
        self.decoder = spectral_norm(nn.Linear(decoder_input_dim, 512))
        
        print(f"   ğŸ“ å¤§Strideç‰ˆæœ¬å°ºå¯¸è®¡ç®—:")
        print(f"      Layer1 output: {h1}x{w1} (32 channels, stride=8)")
        print(f"      Layer2 output: {h2}x{w2} (64 channels, stride=2)")  
        print(f"      Fusion target: {self.target_size}")
        print(f"      Decoder input: {decoder_input_dim:,} (å‡å°‘ {((37632-decoder_input_dim)/37632)*100:.1f}%)")
        print(f"      Decoder params: {decoder_input_dim * 512:,} (vs åŸå§‹19.3M)")
        print(f"   ğŸ¯ ä¿æŒä¸¤å±‚ç»“æ„ï¼Œä»…é€šè¿‡å¤§strideå®ç°å‚æ•°ä¼˜åŒ–")

    @staticmethod
    def _calc_conv_output_size(input_size, kernel_size, stride, padding):
        """è®¡ç®—å·ç§¯å±‚è¾“å‡ºå°ºå¯¸ - éµå¾ªNavRL utilsæ¨¡å¼"""
        h, w = input_size
        h_out = (h + 2 * padding - kernel_size) // stride + 1
        w_out = (w + 2 * padding - kernel_size) // stride + 1
        return h_out, w_out

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ - å¤§strideç‰ˆæœ¬ï¼Œå‚æ•°ä¼˜åŒ–
        Args:
            x: Tensor [B, 1, H, W] (ç°åº¦æ·±åº¦å›¾)
        Returns:
            out: Tensor [B, 512] (ç‰¹å¾å‘é‡)
        """
        # è¾“å…¥éªŒè¯éµå¾ªNavRLæ¨¡å¼
        assert x.dim() == 4, f"Expected 4D input [B,C,H,W], got {x.shape}"
        assert x.shape[1] == 1, f"Expected 1 channel (grayscale), got {x.shape[1]} channels"
        
        # å¦‚æœè¾“å…¥å°ºå¯¸ä¸æœŸæœ›ä¸ç¬¦ï¼Œæ’å€¼è°ƒæ•´
        if x.shape[-2:] != self.input_size:
            x = F.interpolate(x, size=self.input_size, mode='bilinear', align_corners=True)

        # ä¸¤å±‚ViTç¼–ç  - å¤§strideå¿«é€Ÿä¸‹é‡‡æ ·
        embeds = [x]
        for block in self.encoder_blocks:
            embeds.append(block(embeds[-1]))
        
        out1, out2 = embeds[1], embeds[2]  # [B,32,28,28], [B,64,14,14]
        
        # æ¸…ç†embedsåˆ—è¡¨
        del embeds
        
        # ä¸¤å±‚ç‰¹å¾èåˆ
        # Layer2 pxShuffle: [B,64,14,14] -> [B,16,28,28]
        pxshuf_out = self.pxShuffle(out2)  
        
        # Layer1 ç›´æ¥ä½¿ç”¨: [B,32,28,28] - å·²ç»æ˜¯ç›®æ ‡å°ºå¯¸
        upsampled_out1 = F.interpolate(out1, size=self.target_size, mode='bilinear', align_corners=True)
        
        # æ‹¼æ¥ä¸¤å±‚ç‰¹å¾: 32 + 16 = 48 channels
        out = torch.cat([upsampled_out1, pxshuf_out], dim=1)  # [B,48,28,28]
        
        # æ¸…ç†ä¸­é—´å˜é‡
        del out1, out2, pxshuf_out, upsampled_out1
        
        # é™ç»´åˆ°12é€šé“
        out = self.down_sample(out)  # [B,12,28,28]
        
        # å±•å¹³å¹¶é€šè¿‡decoder - å‚æ•°å¤§å¹…å‡å°‘
        out = self.decoder(out.flatten(1))  # [B, 9408] -> [B, 512]
        
        return out
    
class LSTMNetVIT(nn.Module):
    """
    ViT+LSTM Network 
    Num Params: 3,563,663   
    """
    def __init__(self):
        super().__init__()
        self.encoder_blocks = nn.ModuleList([
            MixTransformerEncoderLayer(1, 32, patch_size=7, stride=4, padding=3, n_layers=2, reduction_ratio=8, num_heads=1, expansion_factor=8),
            MixTransformerEncoderLayer(32, 64, patch_size=3, stride=2, padding=1, n_layers=2, reduction_ratio=4, num_heads=2, expansion_factor=8)
        ])

        self.decoder = spectral_norm(nn.Linear(4608, 512))
        self.lstm = (nn.LSTM(input_size=517, hidden_size=128,
                         num_layers=3, dropout=0.1))
        self.nn_fc2 = spectral_norm(nn.Linear(128, 3))

        self.up_sample = nn.Upsample(size=(16,24), mode='bilinear', align_corners=True)
        self.pxShuffle = nn.PixelShuffle(upscale_factor=2)
        self.down_sample = nn.Conv2d(48,12,3, padding = 1)

    def forward(self, X):

        # X = refine_inputs(X)

        x = X[0]
        embeds = [x]
        for block in self.encoder_blocks:
            embeds.append(block(embeds[-1]))        
        out = embeds[1:]
        out = torch.cat([self.pxShuffle(out[1]),self.up_sample(out[0])],dim=1) 
        out = self.down_sample(out)
        out = self.decoder(out.flatten(1))
        out = torch.cat([out, X[1]/10, X[2]], dim=1).float()
        if len(X)>3:
            out,h = self.lstm(out, X[3])
        else:
            out,h = self.lstm(out)
        out = self.nn_fc2(out)
        return out, h