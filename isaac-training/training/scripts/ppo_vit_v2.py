'''
Author: zdytim zdytim@foxmail.com
Date: 2025-12-29 21:00:25
LastEditors: zdytim zdytim@foxmail.com
LastEditTime: 2026-01-07 13:14:07
FilePath: /u20/NavRL/isaac-training/training/scripts/ppo_vit_v1 copy.py
Description: è¿™æ˜¯é»˜è®¤è®¾ç½®,è¯·è®¾ç½®`customMade`, æ‰“å¼€koroFileHeaderæŸ¥çœ‹é…ç½® è¿›è¡Œè®¾ç½®: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE
'''
'''
Author: zdytim zdytim@foxmail.com
Date: 2025-12-23
Description: PPO with ViT Backbone (SAC-style dimension handling & Shared Features)
'''
import torch
import torch.nn as nn
import torch.nn.functional as F
from tensordict.tensordict import TensorDict
from tensordict.nn import TensorDictModuleBase, TensorDictSequential, TensorDictModule
from einops.layers.torch import Rearrange
from torchrl.modules import ProbabilisticActor
from torchrl.envs.transforms import CatTensors
from utils import ValueNorm, make_mlp, IndependentNormal, Actor, GAE, make_batch, IndependentBeta, BetaActor, vec_to_world
from VIT import VIT
from torch.cuda.amp import autocast, GradScaler  # âœ… æ··åˆç²¾åº¦è®­ç»ƒ


# ==========================================
# 2. å…±äº«ç‰¹å¾æå–å™¨ (çº¯å‡€æ¨¡å‹ï¼Œä¸å«ç»´åº¦å¤„ç†)
# ==========================================
class SharedFeatureExtractor(nn.Module):
    def __init__(self, device):
        super().__init__()
        self.device = device
        
        # --- A. ViT Backbone (DINO Pretrained & Frozen) ---
        print("Loading ViT backbone")
        self.vit = VIT().to(device)
        
        # åŠ è½½å®Œæ•´æ¨¡å‹æƒé‡
        full_model_checkpoint = torch.load("/home/u20/NavRL/isaac-training/training/scripts/ViTLSTM_model.pth")
        
        # åªåŠ è½½ ViT ç›¸å…³çš„æƒé‡ï¼ˆè¿‡æ»¤æ‰ LSTM å’Œ fc2ï¼‰
        vit_state_dict = {}
        ignored_keys = []
        for k, v in full_model_checkpoint.items():
            # ä¿ç•™ encoder_blocksã€decoder ä»¥åŠè¾…åŠ©å±‚çš„æƒé‡
            if k.startswith('encoder_blocks') or k.startswith('decoder') or \
               k.startswith('up_sample') or k.startswith('pxShuffle') or k.startswith('down_sample'):
                vit_state_dict[k] = v
            else:
                ignored_keys.append(k)
        
        # åŠ è½½æƒé‡å¹¶è®°å½•ç»“æœ
        missing_keys, unexpected_keys = self.vit.load_state_dict(vit_state_dict, strict=False)
        
        print(f"âœ… ViT weights loaded successfully!")
        print(f"   - Loaded keys: {len(vit_state_dict)}")
        print(f"   - Ignored keys (LSTM/FC2): {ignored_keys}")
        if missing_keys:
            print(f"   âš ï¸  Missing keys (will use random init): {missing_keys}")
        if unexpected_keys:
            print(f"   âš ï¸  Unexpected keys: {unexpected_keys}")
        
        # å¼ºåˆ¶å†»ç»“å‚æ•°
        for param in self.vit.parameters():
            param.requires_grad = False
        self.vit.eval() # å§‹ç»ˆä¿æŒ Eval æ¨¡å¼ (å…³é—­ Dropout/BatchNormæ›´æ–°)
        

        # --- B. åŠ¨æ€éšœç¢ç‰©æå–å™¨ ---
        self.dyn_ext = nn.Sequential(
            Rearrange("n c w h -> n (c w h)"),
            nn.Linear(50, 128), nn.LeakyReLU(), nn.LayerNorm(128),
            nn.Linear(128, 64), nn.LeakyReLU(), nn.LayerNorm(64),
        ).to(device)

        # --- C. State ç‰¹å¾æå–å™¨ï¼ˆæ–°å¢ï¼‰---
        # å°† 8 ç»´åŸå§‹ state ç¼–ç ä¸º 64 ç»´ç‰¹å¾ï¼Œä¸ Dyn Obs ç»´åº¦å¯¹é½
        self.state_ext = nn.Sequential(
            nn.Linear(8, 64), nn.LeakyReLU(), nn.LayerNorm(64),
            nn.Linear(64, 64), nn.LeakyReLU(), nn.LayerNorm(64),
        ).to(device)

        # --- D. èåˆ MLP (æ¼æ–—ç»“æ„) ---
        # Input: 512(ViT) + 64(Dyn) + 64(State) = 640
        self.fusion_mlp = nn.Sequential(
            nn.Linear(640, 512), nn.LeakyReLU(), nn.LayerNorm(512),
            nn.Linear(512, 256), nn.LeakyReLU(), nn.LayerNorm(256),
            nn.Linear(256, 128), nn.LeakyReLU(), nn.LayerNorm(128)
        ).to(device)

    def forward(self, camera, dynamic_obstacle, state):
        """
        Input shapes are assumed to be flattened: [Batch, ...] 
        No dimension checks inside the model.
        """
        camera = torch.nan_to_num(camera, nan=10.0, posinf=10.0, neginf=0.0)
        camera = camera.clamp(0.0, 10.0)
        
        # éªŒè¯è¾“å…¥æ˜¯å•é€šé“ç°åº¦å›¾ [Batch, 1, H, W]
        assert camera.dim() == 4, f"Camera input should be 4D [B,C,H,W], got {camera.shape}"
        assert camera.shape[1] == 1, f"Camera input should be grayscale (1 channel), got {camera.shape[1]} channels"
        
        # 1. Image Processing
        x = (camera / 10 if camera.max() > 1.1 else camera)
        
        # [FIX] è½¬æ¢ RGB ä¸ºå•é€šé“ (Gray)ï¼Œå› ä¸º VIT æ¨¡å‹å®šä¹‰ä¸º 1 é€šé“è¾“å…¥
        # x = (x - self.mean) / self.std
        
        with torch.no_grad():
            # ViT forward
            v_feat = self.vit(x) # [Batch, 512]
        
        # 2. Dynamic Obstacle Processing
        d_feat = self.dyn_ext(dynamic_obstacle) # [Batch, 64]
        
        # 3. State Processing (NEW: encode state for dimension balance)
        s_feat = self.state_ext(state) # [Batch, 8] -> [Batch, 64]
        
        # 4. Concatenation & Fusion (640 = 512+64+64)
        combined = torch.cat([v_feat, d_feat, s_feat], dim=-1) # [Batch, 640]
        latent = self.fusion_mlp(combined) # [Batch, 128]
        
        return latent

# ==========================================
# 3. PPO ä¸»ç±»
# ==========================================
class PPOVIT(TensorDictModuleBase):
    def __init__(self, cfg, observation_spec, action_spec, device):
        super().__init__()
        self.cfg = cfg
        self.device = device
        
        # å¤„ç† action_spec å…¼å®¹æ€§
        if hasattr(action_spec, "shape"):
            shape = tuple(action_spec.shape)
            self.action_dim = int(shape[-1]) if len(shape) > 0 else int(shape[0])
        else:
            self.action_dim = int(action_spec)

        # --- åˆå§‹åŒ–ç½‘ç»œç»„ä»¶ ---
        self.shared_features = SharedFeatureExtractor(device)
        
        # Actor Head (Input: _latent -> Output: alpha, beta)
        self.actor_head = ProbabilisticActor(
            TensorDictModule(
                BetaActor(self.action_dim), 
                in_keys=["_latent"], 
                out_keys=["alpha", "beta"]
            ),
            in_keys=["alpha", "beta"],
            out_keys=[("agents", "action_normalized")],
            distribution_class=IndependentBeta,
            return_log_prob=True
        ).to(device)

        # Critic Head (Input: _latent -> Output: state_value)
        self.critic_head = nn.Linear(128, 1).to(device)


        # 1. æ„é€ ä¸€ä¸ª Dummy Input (å‡æ•°æ®)
        # ä» observation_spec ä¸­ç”Ÿæˆå…¨0æ•°æ®ï¼Œå¹¶æ·»åŠ  Batch ç»´åº¦ [1, ...]
        dummy_tensordict = observation_spec.zero().unsqueeze(0).to(device).reshape(-1)
    
        with torch.no_grad():
            latent = self.shared_features(
                dummy_tensordict["agents", "observation", "camera"],
                dummy_tensordict["agents", "observation", "dynamic_obstacle"],
                dummy_tensordict["agents", "observation", "state"]
            )
            dummy_tensordict.set("_latent", latent)
            # è¿è¡Œä¸€æ¬¡ actor_headï¼Œè§¦å‘ LazyLinear åˆå§‹åŒ–
            self.actor_head(dummy_tensordict)

        # ----------------------------------------------------------------
        # [åŸæœ‰é€»è¾‘] ç°åœ¨çš„æƒé‡å·²ç»å®ä¾‹åŒ–äº†ï¼Œå¯ä»¥å®‰å…¨åˆå§‹åŒ–äº†
        # ----------------------------------------------------------------
        # --- ä¼˜åŒ–å™¨ä¸å·¥å…· ---
        # ç»Ÿä¸€ä¼˜åŒ–å™¨ï¼šåªåŒ…å«å¯è®­ç»ƒçš„å‚æ•°
        # æ³¨æ„ï¼šViT å‚æ•°å·²è¢«å†»ç»“ï¼ˆrequires_grad=Falseï¼‰ï¼Œä¸ä¼šåŒ…å«åœ¨ä¼˜åŒ–å™¨ä¸­
        all_params = []
        for module in [self.shared_features, self.actor_head, self.critic_head]:
            all_params.extend([p for p in module.parameters() if p.requires_grad])
        
        print(f"ğŸ“Š Trainable parameters: {sum(p.numel() for p in all_params):,}")
        print(f"   - Total parameters: {sum(p.numel() for p in self.parameters()):,}")
        
        self.optimizer = torch.optim.Adam(all_params, lr=cfg.actor.learning_rate)
        self.gae = GAE(0.99, 0.95)
        self.value_norm = ValueNorm(1).to(device)
        self.critic_loss_fn = nn.HuberLoss(delta=10)
        
        # æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16ï¼‰- å¯èŠ‚çœ 30-50% æ˜¾å­˜
        self.use_amp = getattr(cfg, 'use_amp', True)  # é»˜è®¤å¯ç”¨
        self.scaler = GradScaler() if self.use_amp else None
        if self.use_amp:
            print("âœ… Mixed Precision (AMP) enabled - saving 30-50% GPU memory")

        # ç°åœ¨è°ƒç”¨åˆå§‹åŒ–ä¸ä¼šæŠ¥é”™äº†
        self._init_weights()

    def _init_weights(self):
        def init_(m):
            if isinstance(m, nn.Linear):
                weight = getattr(m, "weight", None)
                if isinstance(weight, torch.nn.parameter.UninitializedParameter):
                    return
                nn.init.orthogonal_(weight, 0.01)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.)
        self.actor_head.apply(init_)
        self.critic_head.apply(init_)

    def __call__(self, tensordict):
        """æ¨ç†æ¨¡å¼ï¼šé€šå¸¸ç”¨äºç¯å¢ƒäº¤äº’"""
        # 1. æå–ç‰¹å¾
        latent = self.shared_features(
            tensordict["agents", "observation", "camera"],
            tensordict["agents", "observation", "dynamic_obstacle"],
            tensordict["agents", "observation", "state"]
        )
        tensordict.set("_latent", latent)
        
        # 2. ç­–ç•¥é‡‡æ ·
        self.actor_head(tensordict)
        
        # 3. ä»·å€¼ä¼°è®¡
        value = self.critic_head(latent)
        tensordict.set("state_value", value)

        # 4. åæ ‡è½¬æ¢ (Local -> World)
        actions = (2 * tensordict["agents", "action_normalized"] * self.cfg.actor.action_limit) - self.cfg.actor.action_limit
        actions_world = vec_to_world(actions, tensordict["agents", "observation", "direction"])
        tensordict["agents", "action"] = actions_world
        
        # âœ… æ¸…ç†ä¸­é—´å˜é‡ï¼ˆå¯é€‰ï¼Œæå‡ collector æ€§èƒ½ï¼‰
        # æ³¨é‡Šæ‰è¿™è¡Œå¦‚æœéœ€è¦è°ƒè¯•ä¸­é—´å˜é‡
        # for key in ["_latent", "alpha", "beta"]:
        #     if key in tensordict.keys():
        #         tensordict.exclude(key, inplace=True)
        
        return tensordict

    def train(self, tensordict):
        """
        è®­ç»ƒå‡½æ•°ï¼š
        åœ¨å¤–éƒ¨å¤„ç†æ‰€æœ‰ç»´åº¦é—®é¢˜ï¼Œæ¨¡æ‹Ÿ SAC çš„ buffer.sample() æ•ˆæœã€‚
        """
        # 1. è·å–ç»´åº¦ä¿¡æ¯ [Batch, Time]
        # å³ä½¿ T=1ï¼ŒTensordict ä¾ç„¶ä¿ç•™è¿™ä¸ªç»“æ„
        B, T = tensordict.shape 
        
        # step 1: å±•å¹³ Batch ç»´åº¦ä¾›ç½‘ç»œæ¨ç†
        td_flat = tensordict.reshape(-1)
        next_td_flat = tensordict["next"].reshape(-1)

        with torch.no_grad():
            # -----------------------------------------------------------------
            # [æ–¹æ¡ˆ1] ä½¿ç”¨é‡‡æ ·æ—¶å·²ä¿å­˜çš„valuesï¼ˆæ ‡å‡†PPOåšæ³•ï¼‰
            # -----------------------------------------------------------------
            # 1. ä»tensordictè¯»å–é‡‡æ ·æ—¶ä¿å­˜çš„values
            values_flat = tensordict["state_value"].reshape(-1, 1)  # [B*T, 1]
            
            # 2. åªéœ€è®¡ç®—next_values
            next_latent = self.shared_features(
                next_td_flat["agents", "observation", "camera"],
                next_td_flat["agents", "observation", "dynamic_obstacle"],
                next_td_flat["agents", "observation", "state"]
            )
            next_values_flat = self.critic_head(next_latent)  # [B*T, 1]
            
            # 3. åå½’ä¸€åŒ–
            values_flat = self.value_norm.denormalize(values_flat)
            next_values_flat = self.value_norm.denormalize(next_values_flat)

            # 4. è¿˜åŸä¸º [B, T] ä¾› GAE ä½¿ç”¨
            values = values_flat.squeeze(-1).view(B, T)
            next_values = next_values_flat.squeeze(-1).view(B, T)
            # -----------------------------------------------------------------

        # step 3: å‡†å¤‡ GAE æ‰€éœ€æ•°æ®
        # åŸå§‹æ•°æ®æ˜¯ [B, T, 1]ï¼Œsqueeze æ‰æœ€åä¸€ç»´å˜æˆ [B, T]
        rewards = tensordict["next", "agents", "reward"].squeeze(-1) 
        dones = tensordict["next", "terminated"].float().squeeze(-1)

        # step 4: è®¡ç®— GAE
        # è¾“å…¥å…¨éƒ¨ä¸º [B, T]ï¼Œvalues å’Œ next_values æ˜¯åå½’ä¸€åŒ–åçš„çœŸå®å€¼
        # GAE è¿”å›çš„ adv å’Œ ret ä¹Ÿéƒ½æ˜¯çœŸå®å°ºåº¦
        adv, ret = self.gae(rewards, dones, values, next_values)
        
        # step 5: ValueNorm æ›´æ–°å’Œå½’ä¸€åŒ–
        # 5.1 ç”¨çœŸå® Return æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        ret_flat = ret.reshape(-1, 1)  # [B*T, 1]
        self.value_norm.update(ret_flat)
        
        # 5.2 å½’ä¸€åŒ– Return ç”¨äº Critic Loss Target
        ret_normalized_flat = self.value_norm.normalize(ret_flat)  # [B*T, 1]
        
        # 5.3 è¿˜åŸå½¢çŠ¶
        ret_normalized = ret_normalized_flat.view(B, T, 1)
        
        # step 6: å°†è®¡ç®—ç»“æœå­˜å› tensordict
        # æ ‡å‡†åŒ– Advantageï¼ˆå…¨å±€å½’ä¸€åŒ–ï¼‰
        adv_normalized = (adv - adv.mean()) / adv.std().clip(1e-7)
        
        # å½’ä¸€åŒ–é‡‡æ ·æ—¶çš„ valueï¼ˆç”¨äº critic loss clippingï¼‰
        values_normalized = self.value_norm.normalize(values.unsqueeze(-1).reshape(-1, 1))
        values_normalized = values_normalized.view(B, T, 1)
        
        tensordict.set("adv", adv_normalized.unsqueeze(-1))  # [B, T, 1] - å½’ä¸€åŒ–åçš„
        tensordict.set("ret", ret_normalized)     # [B, T, 1] - å½’ä¸€åŒ–åçš„
        tensordict.set("state_value", values_normalized)  # [B, T, 1] - å½’ä¸€åŒ–åçš„ï¼ˆç”¨äº clippingï¼‰
        
        # step 5: æœ€ç»ˆå±•å¹³ï¼Œå‡†å¤‡è®­ç»ƒ
        # æ­¤æ—¶æ•°æ®å®Œå…¨æ‰“æ•£ï¼Œä¸å†æœ‰æ—¶åºæ¦‚å¿µï¼Œç­‰åŒäº SAC Buffer
        td_flat = tensordict.reshape(-1)

        infos = []
        for epoch in range(self.cfg.training_epoch_num):
            # å…¨é‡æ›´æ–°ï¼ˆä¸ä½¿ç”¨ Minibatchï¼‰
            update_result = self._update(td_flat)
            infos.append(update_result)
        
        if len(infos) == 0: return {}
        infos = torch.stack(infos).to_tensordict()
        infos = infos.apply(torch.mean, batch_size=[])
        return {k: v.item() for k, v in infos.items()}

    def _update(self, batch):
        """
        æ›´æ–°å‡½æ•°ï¼š
        è¾“å…¥ batch æ˜¯ä¸€ç»´çš„ [Minibatch_Size]ï¼Œå®Œå…¨å¯¹ç…§ SAC é£æ ¼ã€‚
        """
        # 1. é‡æ–°æå–ç‰¹å¾ (å¸¦æ¢¯åº¦)
        # è¾“å…¥æ•°æ®å·²ç»æ˜¯ squeeze è¿‡çš„æ‰å¹³æ•°æ®
        latent = self.shared_features(
            batch["agents", "observation", "camera"],
            batch["agents", "observation", "dynamic_obstacle"],
            batch["agents", "observation", "state"]
        )
        # å°†ç‰¹å¾æ³¨å…¥ batchï¼Œä¾› actor_head ä½¿ç”¨
        batch.set("_latent", latent)

        # 2. Actor Update
        # è·å–åŠ¨ä½œåˆ†å¸ƒ
        action_dist = self.actor_head.get_dist(batch)
        log_probs = action_dist.log_prob(batch["agents", "action_normalized"])
        entropy_per_dim = action_dist.base_dist.entropy()  # [B, 3] æ¯ä¸ªåŠ¨ä½œç»´åº¦çš„ç†µ
        action_entropy = entropy_per_dim.mean()  # å¹³å‡è€Œéæ±‚å’Œ

        # PPO Loss Calculation
        # adv å·²ç»æ˜¯ [B, 1]ï¼Œsqueeze æˆ [B] è¿›è¡Œè®¡ç®—
        # æ³¨æ„ï¼šadvå·²ç»åœ¨train()ä¸­å…¨å±€å½’ä¸€åŒ–ï¼Œè¿™é‡Œç›´æ¥ä½¿ç”¨
        advantage = batch["adv"].squeeze(-1)
        
        ratio = torch.exp(log_probs - batch["sample_log_prob"])
        surr1 = advantage * ratio
        surr2 = advantage * ratio.clamp(1.-self.cfg.actor.clip_ratio, 1.+self.cfg.actor.clip_ratio)
        actor_loss = -torch.min(surr1, surr2).mean() - self.cfg.entropy_loss_coefficient * action_entropy

        # 3. Critic Update (with Value Clipping)
        # è·å–é‡‡æ ·æ—¶çš„æ—§ valueï¼ˆå½’ä¸€åŒ–åçš„ï¼‰
        b_value = batch["state_value"].squeeze(-1)  # [B]
        
        # è®¡ç®—å½“å‰ç­–ç•¥çš„æ–° value
        value = self.critic_head(latent).squeeze(-1)  # [B]
        
        # Value Clipping: é™åˆ¶ value æ›´æ–°å¹…åº¦
        value_clipped = b_value + (value - b_value).clamp(
            -self.cfg.critic.clip_ratio, 
            self.cfg.critic.clip_ratio
        )
        
        # Target: å½’ä¸€åŒ–åçš„ return
        target = batch["ret"].squeeze(-1)  # [B]
        
        # è®¡ç®—ä¸¤ç§ Critic Lossï¼Œå–æœ€å¤§å€¼ï¼ˆæ›´ä¿å®ˆçš„æ›´æ–°ï¼‰
        critic_loss_clipped = self.critic_loss_fn(value_clipped, target)
        critic_loss_original = self.critic_loss_fn(value, target)
        critic_loss = torch.max(critic_loss_clipped, critic_loss_original)

        # 4. Total Loss & Optimization
        total_loss = actor_loss + critic_loss

        self.optimizer.zero_grad()
        total_loss.backward()
        
        # æ¢¯åº¦è£å‰ªï¼šåˆ†åˆ«è£å‰ªå’Œç›‘æ§ actor å’Œ critic
        actor_params = [p for p in self.actor_head.parameters() if p.requires_grad]
        critic_params = [p for p in self.critic_head.parameters() if p.requires_grad]
        shared_params = [p for p in self.shared_features.parameters() if p.requires_grad]
        
        actor_grad_norm = torch.nn.utils.clip_grad_norm_(actor_params, max_norm=5.0)
        critic_grad_norm = torch.nn.utils.clip_grad_norm_(critic_params, max_norm=5.0)
        shared_grad_norm = torch.nn.utils.clip_grad_norm_(shared_params, max_norm=5.0)
        
        self.optimizer.step()

        # è®¡ç®— Explained Varianceï¼ˆè¡¡é‡ Critic é¢„æµ‹è´¨é‡ï¼‰
        explained_var = 1 - F.mse_loss(value, target) / target.var()

        return TensorDict({
            "actor_loss": actor_loss.detach(),
            "critic_loss": critic_loss.detach(),
            "entropy": action_entropy.detach(),
            "total_loss": total_loss.detach(),
            "actor_grad_norm": actor_grad_norm.detach(),
            "critic_grad_norm": critic_grad_norm.detach(),
            "shared_grad_norm": shared_grad_norm.detach(),
            "explained_var": explained_var.detach(),
        }, [])