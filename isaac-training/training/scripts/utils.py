import torch
import torch.nn as nn
import wandb
import numpy as np
import pandas as pd
from typing import Iterable, Union
from tensordict.tensordict import TensorDict
from omni_drones.utils.torchrl import RenderCallback
from torchrl.envs.utils import ExplorationType, set_exploration_type

class ValueNorm(nn.Module):
    def __init__(
        self,
        input_shape: Union[int, Iterable],
        beta=0.995,
        epsilon=1e-5,
    ) -> None:
        super().__init__()

        self.input_shape = (
            torch.Size(input_shape)
            if isinstance(input_shape, Iterable)
            else torch.Size((input_shape,))
        )
        self.epsilon = epsilon
        self.beta = beta

        self.running_mean: torch.Tensor
        self.running_mean_sq: torch.Tensor
        self.debiasing_term: torch.Tensor
        self.register_buffer("running_mean", torch.zeros(input_shape))
        self.register_buffer("running_mean_sq", torch.zeros(input_shape))
        self.register_buffer("debiasing_term", torch.tensor(0.0))

        self.reset_parameters()

    def reset_parameters(self):
        self.running_mean.zero_()
        self.running_mean_sq.zero_()
        self.debiasing_term.zero_()

    def running_mean_var(self):
        debiased_mean = self.running_mean / self.debiasing_term.clamp(min=self.epsilon)
        debiased_mean_sq = self.running_mean_sq / self.debiasing_term.clamp(
            min=self.epsilon
        )
        debiased_var = (debiased_mean_sq - debiased_mean**2).clamp(min=1e-2)
        return debiased_mean, debiased_var

    @torch.no_grad()
    def update(self, input_vector: torch.Tensor):
        assert input_vector.shape[-len(self.input_shape) :] == self.input_shape
        dim = tuple(range(input_vector.dim() - len(self.input_shape)))
        batch_mean = input_vector.mean(dim=dim)
        batch_sq_mean = (input_vector**2).mean(dim=dim)

        weight = self.beta

        self.running_mean.mul_(weight).add_(batch_mean * (1.0 - weight))
        self.running_mean_sq.mul_(weight).add_(batch_sq_mean * (1.0 - weight))
        self.debiasing_term.mul_(weight).add_(1.0 * (1.0 - weight))

    def normalize(self, input_vector: torch.Tensor):
        assert input_vector.shape[-len(self.input_shape) :] == self.input_shape
        mean, var = self.running_mean_var()
        out = (input_vector - mean) / torch.sqrt(var)
        return out

    def denormalize(self, input_vector: torch.Tensor):
        assert input_vector.shape[-len(self.input_shape) :] == self.input_shape
        mean, var = self.running_mean_var()
        out = input_vector * torch.sqrt(var) + mean
        return out

def make_mlp(num_units):
    layers = []
    for n in num_units:
        layers.append(nn.LazyLinear(n))
        layers.append(nn.LeakyReLU())
        layers.append(nn.LayerNorm(n))
    return nn.Sequential(*layers)

class IndependentNormal(torch.distributions.Independent):
    arg_constraints = {"loc": torch.distributions.constraints.real, "scale": torch.distributions.constraints.positive} 
    def __init__(self, loc, scale, validate_args=None):
        scale = torch.clamp_min(scale, 1e-6)
        base_dist = torch.distributions.Normal(loc, scale)
        super().__init__(base_dist, 1, validate_args=validate_args)

class IndependentBeta(torch.distributions.Independent):
    arg_constraints = {"alpha": torch.distributions.constraints.positive, "beta": torch.distributions.constraints.positive}

    def __init__(self, alpha, beta, validate_args=None):
        beta_dist = torch.distributions.Beta(alpha, beta)
        super().__init__(beta_dist, 1, validate_args=validate_args)

class Actor(nn.Module):
    def __init__(self, action_dim: int) -> None:
        super().__init__()
        self.actor_mean = nn.LazyLinear(action_dim)
        self.actor_std = nn.Parameter(torch.zeros(action_dim)) 
    
    def forward(self, features: torch.Tensor):
        loc = self.actor_mean(features)
        scale = torch.exp(self.actor_std).expand_as(loc)
        return loc, scale

class BetaActor(nn.Module):
    def __init__(self, action_dim: int) -> None:
        super().__init__()
        self.alpha_layer = nn.LazyLinear(action_dim)
        self.beta_layer = nn.LazyLinear(action_dim)
        self.alpha_softplus = nn.Softplus()
        self.beta_softplus = nn.Softplus()
    
    def forward(self, features: torch.Tensor):
        alpha = 1. + self.alpha_softplus(self.alpha_layer(features)) + 1e-6
        beta = 1. + self.beta_softplus(self.beta_layer(features)) + 1e-6
        MAX_BETA_PARAM = 20.0  # æˆ–è€…10.0ï¼Œæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
        alpha = torch.clamp(alpha, min=1.0001, max=MAX_BETA_PARAM)
        beta = torch.clamp(beta, min=1.0001, max=MAX_BETA_PARAM)
        # print("alpha: ", alpha)
        # print("beta: ", beta)
        return alpha, beta


class GaussianActor(nn.Module):
    def __init__(self, action_dim: int) -> None:
        super().__init__()
        self.mean_layer = nn.Linear(256, action_dim)
        # ä½¿ç”¨å½¢çŠ¶ä¸º (1, action_dim) çš„å‚æ•°ï¼Œä¾¿äºåœ¨ forward æ—¶æŒ‰ batch æ‰©å±•
        self.log_std_param = nn.Parameter(torch.zeros(1, action_dim))

    def forward(self, features: torch.Tensor):
        # features: [B, 256]ï¼ˆæˆ–å…¼å®¹å½¢çŠ¶ï¼‰
        loc = self.mean_layer(features)  # [B, action_dim]
        # clamp é˜²æ­¢ log_std æ¼‚ç§»åˆ°æç«¯å€¼
        log_std = self.log_std_param.clamp(-20.0, 2.0)
        # æŒ‰ batch æ‰©å±•åˆ°ä¸ loc åŒ¹é…çš„ shape
        log_std = log_std.expand_as(loc)  # [B, action_dim]
        return loc, log_std

class GAE(nn.Module):
    def __init__(self, gamma, lmbda):
        super().__init__()
        self.register_buffer("gamma", torch.tensor(gamma))
        self.register_buffer("lmbda", torch.tensor(lmbda))
        self.gamma: torch.Tensor
        self.lmbda: torch.Tensor
    
    def forward(
        self, 
        reward: torch.Tensor, 
        terminated: torch.Tensor, 
        value: torch.Tensor, 
        next_value: torch.Tensor
    ):
        num_steps = terminated.shape[1]
        advantages = torch.zeros_like(reward)
        not_done = 1 - terminated.float()
        gae = 0
        for step in reversed(range(num_steps)):
            delta = (
                reward[:, step] 
                + self.gamma * next_value[:, step] * not_done[:, step] 
                - value[:, step]
            )
            advantages[:, step] = gae = delta + (self.gamma * self.lmbda * not_done[:, step] * gae) 
        returns = advantages + value
        return advantages, returns

def make_batch(tensordict: TensorDict, num_minibatches: int):
    tensordict = tensordict.reshape(-1)
    total_samples = tensordict.shape[0]
    
    # ç¡®ä¿è‡³å°‘æœ‰è¶³å¤Ÿçš„æ ·æœ¬æ¥åˆ›å»ºminibatches
    if total_samples < num_minibatches:
        print(f"Warning: total_samples ({total_samples}) < num_minibatches ({num_minibatches}), adjusting to {total_samples}")
        num_minibatches = max(1, total_samples)
    
    # è°ƒæ•´æ ·æœ¬æ•°é‡ä»¥ç¡®ä¿å¯ä»¥æ•´é™¤
    samples_per_batch = total_samples // num_minibatches
    if samples_per_batch == 0:
        # å¦‚æœæ¯ä¸ªæ‰¹æ¬¡éƒ½æ²¡æœ‰æ ·æœ¬ï¼Œè¿”å›æ•´ä¸ªæ•°æ®é›†ä½œä¸ºå•ä¸ªæ‰¹æ¬¡
        yield tensordict
        return
        
    usable_samples = samples_per_batch * num_minibatches
    
    perm = torch.randperm(usable_samples, device=tensordict.device).reshape(num_minibatches, -1)
    for indices in perm:
        yield tensordict[indices]

@torch.no_grad()
def evaluate(
    env,
    policy,
    cfg,
    seed: int=0, 
    exploration_type: ExplorationType=ExplorationType.MEAN
):

    # ç¦ç”¨æ¸²æŸ“ä»¥èŠ‚çœæ˜¾å­˜ï¼Œåªæ”¶é›†æ•°æ®
    env.enable_render(True)
    env.eval()
    env.set_seed(seed)

    render_callback = RenderCallback(interval=2)
    
    with set_exploration_type(exploration_type):
        trajs = env.rollout(
            max_steps=env.max_episode_length,
            policy=policy,
            callback=render_callback, 
            # callback=None,# ç¦ç”¨è§†é¢‘å½•åˆ¶
            auto_reset=True,
            break_when_any_done=False,
            return_contiguous=False,
        )
    # base_env.enable_render(not cfg.headless)
    env.enable_render(not cfg.headless)  # ä¿æŒæ¸²æŸ“å…³é—­çŠ¶æ€
    env.reset()
    
    done = trajs.get(("next", "done")) 
    first_done = torch.argmax(done.long(), dim=1).cpu() # idx of first done will be return for each trajs

    def take_first_episode(tensor: torch.Tensor):
        indices = first_done.reshape(first_done.shape+(1,)*(tensor.ndim-2))
        return torch.take_along_dim(tensor, indices, dim=1).reshape(-1)

    traj_stats = {
        k: take_first_episode(v)
        for k, v in trajs[("next", "stats")].cpu().items()
    }

    info = {
        "eval/stats." + k: torch.mean(v.float()).item() 
        for k, v in traj_stats.items()
    }

    # ç¦ç”¨è§†é¢‘å½•åˆ¶ä»¥èŠ‚çœæ˜¾å­˜
    info["recording"] = wandb.Video(
        render_callback.get_video_array(axes="t c h w"), 
        fps=0.5 / (cfg.sim.dt * cfg.sim.substeps), 
        format="mp4"
    )
    
    env.train()
    # env.reset()

    return info
# @torch.no_grad()
# def evaluate(
#     env,
#     policy,
#     cfg,
#     seed: int=0, 
#     exploration_type: ExplorationType=ExplorationType.MEAN
# ):
#     print(f"\n[NavRL Eval]: ğŸŸ¢ Starting Memory-Efficient Evaluation (Seed {seed})...")
    
#     # 1. å¼ºåˆ¶ Train æ¨¡å¼ (å¼€å¯å¹¶è¡Œç‰©ç†)
#     env.enable_render(False) # å½»åº•å…³é—­æ¸²æŸ“æ¥å£
#     env.train()  
    
#     # 2. ç­–ç•¥è®¾ä¸º Eval (ç¡®å®šæ€§)
#     if hasattr(policy, "eval"):
#         policy.eval()

#     env.set_seed(seed)
    
#     # 3. é‡ç½®ç¯å¢ƒï¼Œè·å–åˆå§‹è§‚æµ‹
#     print("[NavRL Eval]: Resetting environment...")
#     tensordict = env.reset()
    
#     # 4. åˆå§‹åŒ–ç»Ÿè®¡å®¹å™¨
#     # æˆ‘ä»¬åªè®°å½•æ¯ä¸ªç¯å¢ƒ"ç¬¬ä¸€æ¬¡"å®Œæˆä»»åŠ¡æ—¶çš„æ•°æ®ï¼Œé¿å…é‡å¤ç»Ÿè®¡
#     finished_mask = torch.zeros(env.num_envs, dtype=torch.bool, device=env.device)
#     final_stats = {} 
    
#     # 5. æ‰‹åŠ¨å¾ªç¯ (æ›¿ä»£ env.rollout)
#     # è¿™æ ·æˆ‘ä»¬å¯ä»¥æ§åˆ¶æ¯ä¸€æ­¥éƒ½ä¸ä¿å­˜å†å²å›¾åƒï¼Œåªä¿å­˜ç»Ÿè®¡æ•°æ®
#     max_steps = 12000 # åªè¦æ—¶é—´å¤Ÿé•¿ï¼Œå°±èƒ½é£å®Œ
#     print(f"[NavRL Eval]: Running loop for {max_steps} steps (Discarding history)...")
    
#     import time
#     start_time = time.time()
    
#     for step in range(max_steps):
#         # A. ç­–ç•¥æ¨ç† (ä¸ä¿å­˜æ¢¯åº¦)
#         with set_exploration_type(exploration_type):
#             tensordict = policy(tensordict)
        
#         # B. ç¯å¢ƒæ­¥è¿›
#         tensordict = env.step(tensordict)
        
#         # C. æå– Next State
#         tensordict = tensordict["next"]
        
#         # D. å®æ—¶ç»Ÿè®¡ (å…³é”®æ­¥éª¤)
#         # è·å– done ä¿¡å· (terminated æˆ– truncated)
#         done = tensordict["done"].squeeze(-1) # [Num_Envs]
        
#         # å¦‚æœæœ‰ç¯å¢ƒåˆšåˆšå®Œæˆ (done=True) ä¸”ä¹‹å‰æ²¡å®Œæˆè¿‡
#         newly_finished = done & (~finished_mask)
        
#         if newly_finished.any():
#             # æå–è¿™äº›ç¯å¢ƒçš„ç»Ÿè®¡æ•°æ® (stats å­˜åœ¨äº tensordict ä¸­)
#             # æ³¨æ„ï¼šenv.py åœ¨ reset æ—¶ä¼šæ¸…ç©º statsï¼Œæ‰€ä»¥è¦åœ¨ done çš„è¿™ä¸€å¸§æŠ“å–
#             current_stats = tensordict["stats"] # [Num_Envs, Stats_Dim]æˆ–å…¶ä»–ç»“æ„
            
#             # åˆå§‹åŒ– final_stats (å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡)
#             if not final_stats:
#                 for k in current_stats.keys():
#                     # é¢„åˆ†é…ç©ºé—´ï¼Œé¿å…ç¢ç‰‡
#                     final_stats[k] = torch.zeros(env.num_envs, device=env.device)
            
#             # è®°å½•æ•°æ®
#             indices = newly_finished.nonzero().squeeze(-1)
#             for k, v in current_stats.items():
#                 # v å¯èƒ½æ˜¯ [Num_Envs, 1] æˆ– [Num_Envs]
#                 val = v[indices]
#                 if val.dim() > 1: val = val.squeeze(-1)
#                 final_stats[k][indices] = val
            
#             # æ›´æ–°æ©ç 
#             finished_mask = finished_mask | newly_finished
            
#             # æ‰“å°è¿›åº¦ (æ¯å®Œæˆ 10% æ‰“å°ä¸€æ¬¡)
#             completed_count = finished_mask.sum().item()
#             if step % 100 == 0:
#                  print(f"\r[Eval Progress]: Step {step}/{max_steps} | Completed: {completed_count}/{env.num_envs}", end="")

#         # E. æå…¶é‡è¦ï¼šå¤„ç† Auto-Reset
#         # IsaacEnv é€šå¸¸ä¼šè‡ªåŠ¨ resetï¼Œä½†æˆ‘ä»¬éœ€è¦ç¡®ä¿ tensordict é‡Œçš„ observation æ˜¯æœ€æ–°çš„
#         # å¦‚æœ env.step å†…éƒ¨å¤„ç†äº† resetï¼Œtensordict["next"] å·²ç»æ˜¯ reset åçš„çŠ¶æ€äº†
#         # æˆ‘ä»¬ä¸éœ€è¦æ‰‹åŠ¨ resetï¼Œåªéœ€è¦æŠŠ done çš„ç¯å¢ƒæ ‡è®°ä¸€ä¸‹å³å¯
        
#         # F. æå‰é€€å‡ºæœºåˆ¶
#         if finished_mask.all():
#             print(f"\n[NavRL Eval]: All {env.num_envs} environments finished at step {step}!")
#             break
            
#     print(f"\n[NavRL Eval]: Loop finished. Duration: {time.time() - start_time:.2f}s")
    
#     # 6. è®¡ç®—æœ€ç»ˆå¹³å‡å€¼
#     # æ³¨æ„ï¼šåªç»Ÿè®¡é‚£äº›å®é™…å®Œæˆäº†çš„ç¯å¢ƒ (finished_mask)
#     # å¦‚æœæ²¡è·‘å®Œ (ä¾‹å¦‚ crash äº†æˆ–è€…æ—¶é—´ä¸å¤Ÿ)ï¼Œå°±åªç®—è·‘å®Œçš„
#     num_finished = finished_mask.sum().item()
#     if num_finished == 0:
#         print("[NavRL Eval]: âš ï¸ WARNING: No environments finished! Check max_steps or difficulty.")
#         return {}

#     info = {}
#     for k, v in final_stats.items():
#         # åªå– finished çš„éƒ¨åˆ†æ±‚å¹³å‡
#         valid_values = v[finished_mask]
#         info["eval/stats." + k] = torch.mean(valid_values.float()).item()

#     # æ¢å¤ Policy çŠ¶æ€
#     if hasattr(policy, "train"):
#         policy.train()

#     print(f"[NavRL Eval]: Stats collected: {info}")
#     return info

def vec_to_new_frame(vec, goal_direction):
    if (len(vec.size()) == 1):
        vec = vec.unsqueeze(0)
    # print("vec: ", vec.shape)

    # goal direction x
    goal_direction_x = goal_direction / goal_direction.norm(dim=-1, keepdim=True)
    z_direction = torch.tensor([0, 0, 1.], device=vec.device)
    
    # goal direction y
    goal_direction_y = torch.cross(z_direction.expand_as(goal_direction_x), goal_direction_x)
    goal_direction_y /= goal_direction_y.norm(dim=-1, keepdim=True)
    
    # goal direction z
    goal_direction_z = torch.cross(goal_direction_x, goal_direction_y)
    goal_direction_z /= goal_direction_z.norm(dim=-1, keepdim=True)

    n = vec.size(0)
    if len(vec.size()) == 3:
        vec_x_new = torch.bmm(vec.view(n, vec.shape[1], 3), goal_direction_x.view(n, 3, 1)) 
        vec_y_new = torch.bmm(vec.view(n, vec.shape[1], 3), goal_direction_y.view(n, 3, 1))
        vec_z_new = torch.bmm(vec.view(n, vec.shape[1], 3), goal_direction_z.view(n, 3, 1))
    else:
        vec_x_new = torch.bmm(vec.view(n, 1, 3), goal_direction_x.view(n, 3, 1))
        vec_y_new = torch.bmm(vec.view(n, 1, 3), goal_direction_y.view(n, 3, 1))
        vec_z_new = torch.bmm(vec.view(n, 1, 3), goal_direction_z.view(n, 3, 1))

    vec_new = torch.cat((vec_x_new, vec_y_new, vec_z_new), dim=-1)

    return vec_new


def vec_to_world(vec, goal_direction):
    world_dir = torch.tensor([1., 0, 0], device=vec.device).expand_as(goal_direction)
    
    # directional vector of world coordinate expressed in the local frame
    world_frame_new = vec_to_new_frame(world_dir, goal_direction)

    # convert the velocity in the local target coordinate to the world coodirnate
    world_frame_vel = vec_to_new_frame(vec, world_frame_new)
    return world_frame_vel


def construct_input(start, end):
    input = []
    for n in range(start, end):
        input.append(f"{n}")
    return "(" + "|".join(input) + ")"

