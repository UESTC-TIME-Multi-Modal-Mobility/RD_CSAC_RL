defaults:
  - _self_
  - drone
  - ppo
  - sim

headless: True
device: "cuda:0"
seed: 0

# Total Training Length
max_frame_num: 7e8 # max frame 
eval_interval: 1000 # evaluate the policy for every N training steps
save_interval: 1000
# collector
collector:
  total_frames: 7e8
  init_random_frames: 25000
  frames_per_batch: 1000
  init_env_steps: 1000
  env_per_collector: 8
  reset_at_each_iter: False
# Training Environment
env:
  num_envs: 1536 # Number of drones for training
  max_episode_length: 2200
  env_spacing: 8.0 
  num_obstacles: 350
  max_episode_steps: 1000

env_dyn:
  num_obstacles: 80 # set to zero if static env is needed
  vel_range: [0.5, 1.5]
  local_range: [5.0, 5.0, 4.5]

viewer:
  eye: [0., 40., 40.]
  lookat: [0., 2.5, 0.]
  resolution: [960, 720]

wandb:
  project: NavRL
  name: zdytim-uestc
  # entity: CERLAB-UAV-RL-Navigation
  # mode: offline # online
  mode: online
  run: True
  id: 2
  key: #836b25ad1ef4b0832c55eb637d418e6d
  run_id: #kw9xf9k9

# network
network:
  hidden_sizes: [256, 256]
  activation: relu
  default_policy_scale: 1.0
  scale_lb: 0.1
  device: cuda:0

# optim
optim:
  utd_ratio: 1.0
  gamma: 0.99
  loss_function: l2
  lr: 3.0e-4
  weight_decay: 0.0
  batch_size: 256
  target_update_polyak: 0.995
  alpha_init: 1.0
  adam_eps: 1.0e-8

# replay buffer
# replay_buffer:
#   size: 1000000
#   prb: 0 # use prioritized experience replay
#   scratch_dir:

compile:
  compile: False
  compile_mode:
  cudagraphs: False

entropy_loss_coefficient: 1e-3 # encourage exploration
training_frame_num: 32   #每环境32帧
training_epoch_num: 4
num_minibatches: 16 # split into N minibatches
alpha_learning_rate: 1e-7 # coefficient for learning alpha